2025-12-10 14:46:45,138 [INFO] HTTP Request: GET https://huggingface.co/api/datasets/livecodebench/code_generation_lite "HTTP/1.1 200 OK"
2025-12-10 14:46:52,861 [INFO] Indexed livecodebench rows: 1055
2025-12-10 14:46:57,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:46:58,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:46:59,907 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:00,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:00,641 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:03,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:05,889 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:07,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:07,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:07,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:09,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:09,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:10,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:11,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:12,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:15,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:15,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:16,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:18,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:18,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:19,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:20,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:20,312 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:20,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:21,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:21,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:21,875 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:22,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:23,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:23,437 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:23,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:24,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:28,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:28,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:29,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:29,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:30,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:30,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:32,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:32,643 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:35,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:36,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:36,016 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:36,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:36,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:38,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:40,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:40,389 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:40,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:41,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:41,619 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:42,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:43,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:44,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:44,377 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:45,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:45,608 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:48,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:49,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:49,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:49,097 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:49,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:50,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:50,421 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 790057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:50,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:52,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:53,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:53,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:53,668 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:53,677 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:54,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:47:54,430 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:47:58,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:47:59,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:00,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:01,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:02,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:02,173 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817916 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:02,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:03,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:03,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:03,111 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:06,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:06,038 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817916 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:06,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:06,965 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:10,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:10,449 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817916 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:11,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:11,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:11,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:11,836 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:12,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:12,493 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:13,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:13,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:13,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:13,839 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:14,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:14,513 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:15,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:15,181 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:15,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:17,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:18,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:18,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:18,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:18,488 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:20,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:21,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:21,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:21,373 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:22,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:22,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:23,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:24,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:24,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:24,609 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:27,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:28,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:29,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:30,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:30,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:31,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:32,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:32,232 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:33,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:33,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:33,920 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 425922 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:33,933 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 901191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:35,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:36,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:36,965 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:38,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:38,515 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 425922 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:38,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:38,529 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 901191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:41,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:41,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:41,713 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:43,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:43,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:43,333 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 425922 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:43,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 901191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:44,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:46,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:46,646 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:47,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:47,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:47,800 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 466012 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:47,813 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:47,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:51,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:51,230 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:52,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:52,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:52,412 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 466012 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:52,425 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:54,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:48:55,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:55,777 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:56,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:56,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:48:56,975 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 466012 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:56,979 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:48:57,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:00,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:02,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:03,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:03,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:03,233 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:03,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:04,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:04,485 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:05,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:07,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:07,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:08,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:08,012 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:09,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:09,360 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:11,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:12,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:12,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:13,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:13,965 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:16,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:16,363 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:18,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:18,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:20,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:20,155 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:21,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:22,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:22,147 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:23,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:25,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:25,631 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:27,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:27,622 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:29,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:29,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:29,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:49:31,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:31,917 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:32,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:32,394 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 392592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:36,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:36,006 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:36,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:36,816 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 392592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:49:37,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:49:37,322 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 392592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:50:54,547 [INFO] HTTP Request: GET https://huggingface.co/api/datasets/livecodebench/code_generation_lite "HTTP/1.1 200 OK"
2025-12-10 14:51:04,158 [INFO] Indexed livecodebench rows: 1055
2025-12-10 14:51:06,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:08,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:09,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:09,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:11,065 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:11,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:12,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:12,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:13,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:13,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:14,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:16,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:16,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:17,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:17,559 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:17,561 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:19,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:19,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:20,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:20,449 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:20,450 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:23,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:23,517 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:23,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:23,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:23,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:24,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:26,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:26,223 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1514000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:28,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:28,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:28,966 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1514000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:28,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:29,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:29,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:31,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:32,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:32,276 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1514000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:34,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:35,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:35,558 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:35,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978971 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:38,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:38,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:38,976 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978971 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:39,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:39,505 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:41,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:41,663 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978971 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:42,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:42,902 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:43,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:43,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:44,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:44,273 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:45,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:45,701 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:45,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:46,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:48,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:48,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:49,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:50,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:50,556 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:52,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:52,557 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:52,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:53,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:53,075 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:53,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:53,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:53,486 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:53,866 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:53,868 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:54,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:55,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:55,832 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525170 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:56,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:57,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:51:57,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:51:57,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525170 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:51:59,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:00,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:00,695 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525170 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:01,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:01,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:02,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:02,847 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:02,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:03,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:03,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:06,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:06,227 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:06,229 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:06,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:08,177 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:08,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:08,509 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:08,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:08,629 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:08,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:10,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:11,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:11,072 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:11,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:11,213 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:13,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:13,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:13,313 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:13,332 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:13,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:13,578 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:15,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:15,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:15,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:15,658 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:15,677 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:15,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:15,855 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:17,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:17,617 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:17,618 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:17,647 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:17,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:17,885 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:19,747 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:20,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:20,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:20,621 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:20,648 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:20,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:20,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:20,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:22,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:22,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:22,959 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:22,987 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:23,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:23,262 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:25,216 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:25,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:25,219 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551763 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:25,235 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:25,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:25,477 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:25,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:27,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:27,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:27,430 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:27,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:27,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:27,647 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:29,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:29,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:29,042 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:29,057 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:29,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:29,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:30,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:30,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:30,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:30,656 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:30,670 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:30,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:30,841 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:31,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:31,574 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:31,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:31,782 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:32,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:32,676 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:33,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:33,579 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:34,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:35,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:35,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:35,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:38,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:38,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:39,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:39,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:41,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:42,106 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:42,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:43,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:43,884 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:45,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:45,531 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:46,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:46,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:47,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:49,370 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:49,372 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:51,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:51,458 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:53,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:54,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:55,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:55,073 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:55,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:52:56,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:52:56,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:52:59,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:00,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:00,013 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:02,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:03,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:03,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:03,257 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:04,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:04,091 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:04,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:04,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:04,819 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:05,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:05,458 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:05,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:06,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:06,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:07,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:07,548 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:09,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:09,844 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:09,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:10,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:12,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:12,459 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:13,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:14,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:15,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:15,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:16,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:16,794 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:17,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:17,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:17,815 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:18,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:18,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:18,790 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:18,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:19,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:21,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:22,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:22,983 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:24,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:25,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:26,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:27,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:27,136 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:29,679 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:29,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:30,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:30,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:30,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:30,884 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:32,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:32,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:35,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:35,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:35,973 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:36,003 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:39,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:39,743 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:41,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:41,260 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:43,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:53:43,902 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:53:46,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:47,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:50,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:51,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:53,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:54,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:56,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:57,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:53:59,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:00,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:01,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:01,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:02,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:03,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:05,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:05,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:06,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:06,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:06,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:07,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:07,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:08,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:08,432 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:10,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:10,259 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:11,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:11,201 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:11,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:12,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:12,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:12,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:13,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:14,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:14,090 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:17,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:17,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:17,412 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:18,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:19,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:20,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:21,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:21,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:24,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:24,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:24,839 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:27,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:27,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:28,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:30,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:30,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:31,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:31,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:32,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:32,535 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:33,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:33,412 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:34,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:34,037 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:34,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:34,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:35,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:35,667 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:36,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:38,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:39,456 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:39,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:40,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:41,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:41,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:41,994 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:42,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:43,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:44,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:44,166 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:45,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:46,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:54:46,322 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:54:46,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:48,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:50,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:51,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:53,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:54,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:54,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:56,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:58,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:59,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:54:59,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:01,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:04,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:04,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:04,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:05,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:08,283 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:10,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:10,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:11,119 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:13,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:14,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:15,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:15,204 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:15,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:15,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:16,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:16,708 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:18,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:18,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:18,330 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:20,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:20,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:21,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:22,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:24,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:25,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:26,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:26,653 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:28,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:28,076 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:28,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:29,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:30,047 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:30,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:30,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:30,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:30,634 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:31,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:31,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:32,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:33,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:33,297 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:34,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:34,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:34,464 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:35,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:35,567 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:36,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:36,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:36,740 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:39,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:41,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:41,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:41,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:44,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:44,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:44,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:44,885 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:45,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:45,115 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:45,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:45,384 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:45,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:49,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:49,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:51,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:51,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:51,702 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:51,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:51,904 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:52,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:52,105 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:52,304 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:52,306 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:52,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:52,504 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:52,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:55:52,703 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:55:52,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:56,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:56,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:57,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:57,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:55:59,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:01,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:02,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:02,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:03,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:03,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:03,481 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:04,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:04,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:04,544 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:05,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:05,574 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:06,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:07,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:07,007 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:07,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:08,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:08,369 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:08,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:09,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:09,971 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:10,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:12,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:13,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:15,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:15,321 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:18,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:20,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:21,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:21,390 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:21,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:24,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:25,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:27,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:27,100 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:29,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:31,157 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:33,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:33,168 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:33,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:37,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:37,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:38,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:38,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:38,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:38,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:41,898 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:43,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:44,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:44,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:45,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:45,021 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:48,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:49,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:50,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:50,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:56:50,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:51,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:53,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:53,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:56:56,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:56:56,844 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:03,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:03,490 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:10,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:10,625 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:12,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:13,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:13,519 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:13,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:17,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:17,909 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:22,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:22,389 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:25,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:26,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:26,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:29,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:29,686 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:33,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:33,967 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:41,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:41,416 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:46,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:46,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:51,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:51,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:53,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:57:54,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:54,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:57:58,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:57:58,052 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:05,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:05,446 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:10,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:10,259 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:15,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:15,322 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:18,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:18,238 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:18,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:22,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:22,016 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:24,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:28,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:28,861 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:31,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:31,952 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:38,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:38,233 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:42,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:42,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:45,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:46,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:46,326 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:50,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:50,728 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:53,779 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:58:53,780 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:58:57,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:58,106 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:58:59,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:02,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:02,320 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:08,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:08,238 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3710051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:12,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:13,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:14,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:16,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:16,898 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:22,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:22,911 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3710051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:25,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:26,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:30,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:30,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:34,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:36,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:36,879 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3710051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:38,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:40,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:44,357 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:44,358 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:47,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 14:59:48,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:48,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:48,991 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90734 input tokens (60000 > 129024 - 90734). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:48,997 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 14:59:56,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 14:59:56,680 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:00,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:01,193 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:01,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:01,195 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90734 input tokens (60000 > 129024 - 90734). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:01,201 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:04,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:08,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:08,308 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:12,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:12,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:12,612 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:12,618 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90734 input tokens (60000 > 129024 - 90734). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:12,621 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:20,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:20,023 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:24,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:27,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:27,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:27,452 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:27,490 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:28,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:28,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:31,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:31,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:31,819 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:38,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:38,702 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:43,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:44,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:45,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:46,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:46,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:46,855 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:46,892 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:50,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:51,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:51,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:51,825 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:00:55,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:00:59,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:00:59,456 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:02,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:03,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:03,375 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:06,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:11,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:11,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:12,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:15,283 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:15,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:17,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:23,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:23,636 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:26,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:28,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:28,171 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:32,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:36,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:36,242 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:39,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:40,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:40,639 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:44,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:47,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:47,744 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:52,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:52,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:01:52,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:54,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:01:59,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:01:59,566 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:02,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:02,825 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:02,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:06,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:07,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:11,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:11,142 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:14,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:14,183 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:17,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:17,239 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:19,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:19,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:19,981 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:20,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:22,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:22,425 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:24,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:24,783 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:25,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:25,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:26,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:26,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:28,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:28,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:28,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:28,356 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88270 input tokens (60000 > 129024 - 88270). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:31,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:31,109 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:31,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:31,362 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88270 input tokens (60000 > 129024 - 88270). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:31,671 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:32,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:33,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:34,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:34,117 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:34,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:34,321 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88270 input tokens (60000 > 129024 - 88270). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:35,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:35,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:39,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:41,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:41,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:41,181 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712134 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:42,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:42,258 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:44,981 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:46,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:48,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:48,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712134 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:49,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:49,514 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:53,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:55,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:55,317 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712134 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:55,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:02:56,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:02:56,785 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:02:58,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:00,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:00,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:02,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:02,255 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:03,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:04,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:05,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:06,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:06,719 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:07,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:07,727 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:11,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:11,911 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:13,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:16,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:16,228 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:16,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:17,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:18,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:19,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:19,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:19,435 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:24,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:24,083 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580457 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:25,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:26,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:27,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:27,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:27,731 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:31,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:31,952 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580457 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:35,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:36,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:36,417 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:36,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:36,456 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:39,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:39,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580457 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:40,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:40,349 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98996 input tokens (60000 > 129024 - 98996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:42,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:43,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:44,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:44,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:44,046 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:44,083 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 347046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:44,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:44,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98996 input tokens (60000 > 129024 - 98996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:45,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:45,063 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 347046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:46,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:48,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:48,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:48,699 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:48,732 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98996 input tokens (60000 > 129024 - 98996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:49,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:49,415 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 347046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:52,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:52,158 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:52,501 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:52,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:03:56,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:56,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580737 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:03:59,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:03:59,144 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:01,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:03,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:03,299 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580737 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:06,068 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:06,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:06,412 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:09,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:10,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:10,313 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580737 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:12,624 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:14,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:15,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:15,349 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:17,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:19,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:19,929 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:23,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:24,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:24,683 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:28,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:28,098 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:28,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:30,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:31,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:33,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:33,626 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:36,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:36,864 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:39,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:39,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:39,851 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:40,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:42,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:43,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:43,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:43,647 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:44,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:45,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:45,469 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:47,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:47,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:48,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:48,767 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:51,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:51,190 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:51,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:54,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:55,026 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:04:56,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:56,033 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:04:58,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:04:58,011 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:02,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:02,193 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:03,660 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:03,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:04,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:04,351 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:05,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:08,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:08,430 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:09,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:09,975 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:10,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:11,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:13,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:13,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:15,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:15,246 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:18,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:18,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:19,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:20,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:20,085 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:23,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:23,747 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:28,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:28,107 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:28,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:28,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:28,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:31,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:31,784 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:35,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:35,160 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:38,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:39,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:39,110 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:40,161 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:43,047 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:43,049 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:46,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:46,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:47,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:47,513 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:50,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:51,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:51,915 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:55,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:56,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:05:56,084 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:05:58,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:05:59,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:00,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:00,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:00,578 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:00,623 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94420 input tokens (60000 > 129024 - 94420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:05,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:05,213 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:05,505 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:05,506 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94420 input tokens (60000 > 129024 - 94420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:07,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:08,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:08,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:08,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:09,001 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:09,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:09,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:09,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:09,951 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94420 input tokens (60000 > 129024 - 94420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:09,955 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:13,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:13,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:13,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:13,428 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:14,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:14,382 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:17,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:17,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:17,068 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:17,069 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:17,102 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:17,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:18,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:18,921 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:23,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:23,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:23,620 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:23,655 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:27,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:27,351 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:28,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:28,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:29,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:31,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:31,571 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:36,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:36,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:36,958 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:36,995 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95849 input tokens (60000 > 129024 - 95849). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:40,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:41,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:41,288 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:45,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:45,465 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:45,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:45,503 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95849 input tokens (60000 > 129024 - 95849). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:47,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:47,483 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:49,530 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:52,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:52,132 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:52,133 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800894 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:52,160 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95849 input tokens (60000 > 129024 - 95849). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:54,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:54,184 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:54,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:56,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:06:57,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:57,039 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:58,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:58,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:06:58,875 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:06:58,916 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789867 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:00,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:02,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:02,190 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:04,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:05,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:05,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:05,972 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:06,013 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789867 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:09,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:09,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:10,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:10,446 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789867 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:12,803 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:14,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:14,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:14,622 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:14,673 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:15,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:17,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:18,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:18,906 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:18,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:18,957 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:21,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:21,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:21,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:23,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:25,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:25,959 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:27,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:27,849 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:28,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:31,788 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:32,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:32,231 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:33,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:33,711 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:36,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:38,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:39,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:39,132 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:41,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:43,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:43,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:43,040 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:43,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:45,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:46,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:47,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:48,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:48,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:48,476 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714387 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:50,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:50,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:50,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:51,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:54,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:07:54,462 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714387 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:07:56,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:56,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:58,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:07:59,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:00,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:00,765 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714387 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:02,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:02,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:03,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:04,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:04,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:04,785 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:06,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:07,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:08,269 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:08,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:09,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:09,009 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:11,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:11,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:12,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:13,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:13,476 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:16,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:16,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:17,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:17,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756187 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:18,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:19,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:21,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:21,412 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756187 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:23,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:24,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:25,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:25,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:25,800 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:25,802 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756187 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:28,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:29,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:29,996 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:30,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:30,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:33,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:33,226 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:34,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:36,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:36,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:36,853 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:39,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:40,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:40,105 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:40,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:43,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:43,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:43,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:43,885 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:45,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:45,996 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:46,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:46,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:46,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:48,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:48,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:48,885 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:50,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:50,797 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:51,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:52,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:08:54,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:54,158 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:55,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:55,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:08:59,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:08:59,497 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:00,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:01,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:01,863 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:04,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:04,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:06,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:07,304 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:07,305 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:09,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:09,585 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:11,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:11,555 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:14,356 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:14,357 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:16,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:16,287 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:19,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:19,343 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737755 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:21,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:21,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:21,732 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:24,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:24,676 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737755 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:26,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:26,600 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:29,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:29,816 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737755 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:31,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:32,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:32,031 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:35,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:35,546 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153120 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:35,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:38,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:38,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:38,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:38,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153120 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:42,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:42,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:42,459 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153120 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:45,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:46,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:46,724 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:47,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:48,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:50,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:51,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:51,023 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:52,788 [WARNING] gen_code timeout (attempt 1)
2025-12-10 15:09:55,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:09:55,345 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200530 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:09:57,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:59,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:09:59,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:02,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:02,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:04,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:05,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:07,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:07,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:08,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:08,304 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:11,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:11,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:11,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:12,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:14,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:15,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:18,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:18,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:19,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:19,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:23,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:24,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:24,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:24,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:27,543 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:27,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:28,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:30,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:31,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:31,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:32,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:32,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:35,517 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:37,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:39,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:43,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:43,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:44,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:46,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:47,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:50,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:52,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:53,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:55,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:10:56,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:01,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:03,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:05,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:07,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:08,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:10,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:12,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:13,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:13,016 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:13,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:13,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:13,656 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:14,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:14,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:14,240 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:14,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:15,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:15,085 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:15,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:15,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:16,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:16,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:16,861 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:17,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:17,866 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:18,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:18,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:18,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:18,964 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:20,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:20,168 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:21,068 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:21,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:21,465 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:22,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:22,573 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:23,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:23,744 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:24,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:24,898 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:25,530 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:25,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:26,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:26,082 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:26,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:27,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:27,289 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:28,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:28,390 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:29,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:29,532 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:29,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:30,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:30,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:30,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:30,687 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:31,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:31,896 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:32,124 [WARNING] gen_code timeout (attempt 1)
2025-12-10 15:11:33,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:33,091 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:33,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:34,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:34,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:35,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:36,322 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:36,324 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:37,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:38,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:38,394 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:39,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:39,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:40,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:41,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:41,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:41,874 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:43,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:43,926 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:44,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:46,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:46,206 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:46,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:48,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:48,237 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:48,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:50,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:50,201 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:52,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:52,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:52,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:52,219 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:53,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:54,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:54,410 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:55,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:56,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:56,416 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:56,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:57,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:11:58,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:11:58,384 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:11:58,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:01,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:01,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:02,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:03,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:04,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:06,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:06,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:08,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:09,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:11,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:12,243 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:13,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:14,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:15,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:16,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:17,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:19,023 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:19,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:21,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:24,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:24,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:25,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:26,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:26,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:29,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:34,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:34,976 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:36,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:38,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:39,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:43,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:46,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:46,949 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:48,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:50,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:01,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:01,248 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:03,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:05,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:17,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:17,050 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:18,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:28,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:32,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:32,125 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:34,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:35,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:36,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:39,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:47,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:47,032 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:49,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:50,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:50,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:55,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:03,760 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:03,761 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:06,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:07,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:15,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:15,416 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:17,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:17,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:18,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:19,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:30,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:30,425 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:32,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:33,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:33,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:34,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:34,171 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:34,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:36,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:37,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:37,443 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:37,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:38,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:38,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:38,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:39,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:39,320 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:40,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:42,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:43,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:43,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:44,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:45,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:47,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:48,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:48,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:48,991 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:50,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:51,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:52,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:52,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:54,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:54,455 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:55,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:56,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:57,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:01,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:07,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:07,950 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:10,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:10,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:11,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:11,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:21,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:21,203 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:23,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:24,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:24,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:24,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:36,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:36,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:39,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:39,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:41,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:43,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:47,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:47,432 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:50,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:50,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:51,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:52,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:57,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:57,337 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:59,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:00,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:01,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:02,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:07,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:07,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:10,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:10,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:11,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:12,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:18,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:18,546 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:21,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:22,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:23,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:28,147 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:28,148 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:31,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:32,425 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:35,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:35,894 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:36,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:36,008 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:38,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:38,230 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:39,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:40,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:54,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:54,257 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:55,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:55,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:58,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:10,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:10,555 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:10,645 [WARNING] gen_code timeout (attempt 1)
2025-12-10 15:17:11,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:11,775 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:14,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:14,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:26,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:26,866 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:28,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:28,278 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:30,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:31,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:34,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:34,106 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:35,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:35,235 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:36,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:39,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:39,741 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:40,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:40,693 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:43,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:43,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:46,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:46,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:47,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:47,315 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:49,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:01,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:01,863 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:03,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:03,109 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:04,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:05,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:10,651 [WARNING] gen_code timeout (attempt 2)
2025-12-10 15:18:19,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:19,176 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:21,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:22,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:23,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:23,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:35,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:35,570 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:38,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:39,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:40,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:40,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:52,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:52,568 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:55,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:56,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:03,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:03,939 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:07,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:07,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:08,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:19,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:19,778 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:22,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:23,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:23,530 [WARNING] gen_code timeout (attempt 1)
2025-12-10 15:19:23,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:26,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:26,782 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:28,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:30,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:30,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:32,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:32,414 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:35,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:36,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:37,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:37,776 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:40,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:40,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:41,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:44,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:44,970 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:47,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:48,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:51,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:51,839 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:55,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:59,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:59,136 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:59,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:03,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:03,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:13,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:13,898 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:16,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:17,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:21,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:23,533 [WARNING] gen_code timeout (attempt 2)
2025-12-10 15:20:28,624 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:28,626 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:31,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:31,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:32,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:32,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:44,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:44,625 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:47,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:48,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:48,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:48,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:53,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:53,346 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:56,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:58,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:59,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:59,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:02,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:02,865 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:05,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:06,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:06,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:09,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:11,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:11,884 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:15,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:16,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:17,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:17,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:18,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:18,795 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:21,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:22,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:23,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:23,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:25,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:25,449 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:28,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:29,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:29,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:32,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:32,323 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:45,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:45,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:46,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:46,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:50,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:50,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:02,470 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:02,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:02,750 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:04,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:17,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:17,827 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:18,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:18,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:21,328 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:21,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:22,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:30,412 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:30,412 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:30,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:30,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:33,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:33,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:40,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:40,579 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:41,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:41,102 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:42,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:42,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:49,543 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:49,544 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:50,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:50,529 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:51,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,120 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:51,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,328 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:51,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,540 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:51,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,764 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:52,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:53,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:53,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:55,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:56,968 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:59,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:01,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:01,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:02,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:02,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:03,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:03,994 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:05,292 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:05,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:05,520 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:05,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:05,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:07,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:07,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:07,958 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:08,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:10,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:10,451 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:10,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:10,473 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:10,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:11,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:12,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:13,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:13,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:13,114 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:13,136 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:14,384 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:14,385 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:15,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:16,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:17,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:17,635 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:18,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:18,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:18,622 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:20,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:22,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:22,406 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:23,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:23,525 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:24,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:25,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:27,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:27,480 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:27,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:28,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:28,466 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:30,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:32,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:32,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:34,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:35,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:35,718 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:36,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:39,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:39,983 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:42,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:42,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:43,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:43,362 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:46,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:46,762 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:50,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:51,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:51,126 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:51,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:54,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:54,265 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:57,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:58,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:58,485 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:00,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:02,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:02,303 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:02,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:03,023 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:07,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:07,399 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:11,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:11,239 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:12,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:12,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:13,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:17,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:17,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:20,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:20,682 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:22,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:22,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:25,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:25,646 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:27,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:28,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:28,764 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:29,624 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:33,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:33,241 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:34,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:37,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:37,146 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:39,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:39,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:41,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:41,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:44,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:45,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:45,818 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:47,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:50,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:50,950 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:54,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:54,575 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:54,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:55,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:56,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:02,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:02,250 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:06,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:06,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:08,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:09,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:11,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:13,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:13,274 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:17,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:17,803 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:18,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:19,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:20,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:24,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:24,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:24,363 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:24,444 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:25,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:28,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:29,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:32,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:32,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:32,005 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:32,093 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:34,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:35,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:37,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:37,809 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:40,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:44,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:44,282 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:46,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:47,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:49,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:49,575 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:53,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:57,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:57,554 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:59,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:00,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:03,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:03,935 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:09,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:11,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:11,962 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:13,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:17,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:17,456 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:22,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:24,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:24,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:26,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:30,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:30,636 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:31,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:37,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:37,140 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:39,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:42,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:42,640 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:44,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:49,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:49,415 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:50,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:56,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:56,611 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:57,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:03,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:03,685 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:08,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:09,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:09,731 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:11,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:18,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:18,856 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:19,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:24,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:24,779 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:29,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:33,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:33,559 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:34,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:36,551 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:39,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:39,637 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:48,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:48,116 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:52,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:54,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:54,863 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:57,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:02,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:05,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:05,540 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:06,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:12,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:12,932 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:15,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:24,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:24,561 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:25,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:26,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:27,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:30,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:34,953 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:34,958 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:36,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:36,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:37,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:37,800 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:45,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:45,656 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:46,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:48,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:49,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:56,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:56,647 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:58,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:58,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:58,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:00,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:06,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:06,333 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:08,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:08,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:08,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:09,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:09,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:10,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:10,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:11,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:11,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:12,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:13,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:13,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:14,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:14,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:15,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:15,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:16,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:16,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:17,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:18,520 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:18,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:19,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:19,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:20,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:20,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:21,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:21,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:21,732 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:21,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:22,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:22,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:23,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:23,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:24,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:24,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:25,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:25,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:25,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:26,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:26,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:27,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:27,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:27,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:34,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:35,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:36,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:36,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:36,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:37,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:37,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208870 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:37,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:37,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:37,712 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208870 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:37,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:37,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:37,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:38,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:38,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208870 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:38,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:38,597 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:38,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:38,988 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:39,425 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:39,426 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:43,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:44,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:44,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:45,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:46,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:46,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:47,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:47,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:47,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:48,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:48,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:49,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:50,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:50,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:50,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:51,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:51,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:51,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:52,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:00,732 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:00,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:02,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:02,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:03,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:03,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:04,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:04,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:04,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:05,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:14,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:14,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:15,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:15,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:17,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:17,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:17,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:18,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:18,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:19,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:19,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:19,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:20,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:30,382 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:32,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:34,243 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:34,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:34,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:34,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:35,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:35,870 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:36,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:36,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:37,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:37,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:38,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:39,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:39,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:40,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:40,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:40,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:41,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:42,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:48,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:48,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:49,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:49,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:50,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:50,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:50,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:51,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:51,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:01,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:03,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:04,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:04,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:05,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:06,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:07,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:08,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:08,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:10,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:10,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:12,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:13,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:14,035 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:14,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:14,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:16,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:16,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:18,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:20,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:21,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:22,260 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:24,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:26,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:27,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:27,042 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:27,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:27,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:27,653 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:27,978 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:28,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:28,223 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:28,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:31,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:33,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:34,357 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:34,359 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:34,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:34,840 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:35,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:35,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:35,345 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:36,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:37,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:38,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:40,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:41,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:44,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:44,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:45,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:46,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:46,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:50,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:50,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:51,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:51,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:53,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:54,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:54,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:54,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:55,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:56,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:59,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:59,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:00,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:01,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:04,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:05,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:05,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:07,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:08,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:09,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:09,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:09,479 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:09,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:09,999 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:10,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:10,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:10,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:11,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:13,641 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:15,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:17,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:17,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:18,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:19,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:19,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:21,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:21,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:22,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:24,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:26,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:26,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:27,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:30,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:30,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:31,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:31,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:35,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:35,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:36,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:37,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:38,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:39,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:40,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:41,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:42,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:43,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:43,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:45,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:46,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:47,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:49,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:52,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:52,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:53,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:54,177 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:55,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:55,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:57,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:59,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:00,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:02,297 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:03,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:04,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:04,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:05,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:07,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:07,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:09,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:11,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:11,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,640 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,713 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:13,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:15,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:15,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:16,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:16,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:18,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:19,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:19,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:20,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:20,006 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:20,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:20,552 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:21,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:21,168 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:21,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:21,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:21,692 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:22,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:22,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:22,134 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:22,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:22,571 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:22,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:22,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:24,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,106 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:32,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:32,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:33,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:36,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:36,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:38,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:38,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:39,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:40,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:42,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:44,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:46,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:47,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:48,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:48,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:50,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:52,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:52,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:53,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:55,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:57,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:57,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:57,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:58,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:59,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:00,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:01,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:02,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:03,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:05,559 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:06,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:09,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:11,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:12,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:14,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:14,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:14,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:15,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:16,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:18,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:18,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:19,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:20,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:22,328 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:22,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:22,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:24,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:25,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:28,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:28,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:29,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:29,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:31,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:33,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:34,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:34,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:36,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:37,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:38,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:39,425 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:43,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:44,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:44,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:45,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:46,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:47,328 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:50,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:51,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:56,193 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,456 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:01,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:02,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:04,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:05,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:10,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:12,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:13,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:17,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:17,728 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:18,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:19,096 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:19,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:20,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:21,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:23,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:23,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:24,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:25,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:25,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:26,520 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:27,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:28,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:29,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:29,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:30,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:31,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:31,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:33,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:33,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:34,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:37,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:37,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:38,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,746 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:40,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:42,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:47,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:48,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:49,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:52,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:54,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:54,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:56,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:57,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:57,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:58,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:58,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:01,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:01,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:02,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:03,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:04,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:05,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:05,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:06,757 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:10,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:10,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:12,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:12,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:15,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:15,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:17,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:19,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:19,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:28,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:31,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:33,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:35,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:35,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:37,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:45,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:47,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:48,505 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:53,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:54,216 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:58,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:59,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:00,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:00,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,304 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:06,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:06,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:07,269 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:15,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:15,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:17,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:19,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:27,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:28,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:29,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:29,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:31,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:32,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:32,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:34,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:35,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:36,039 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:36,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:37,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:38,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:40,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:41,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:41,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:41,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:42,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:44,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:44,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:44,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:45,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:46,937 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:47,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:47,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:52,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:53,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:53,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:00,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:00,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:01,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:02,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:03,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:06,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:07,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:08,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:10,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:11,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:12,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:13,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:15,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:20,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:22,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:25,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:30,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:31,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:32,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,065 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:40,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:43,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:44,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:44,161 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:44,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:44,477 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:44,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:44,804 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:47,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,791 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:00,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:01,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:03,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:04,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:05,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:05,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:09,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:11,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:14,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:15,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:20,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:22,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:25,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:27,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:27,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:28,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:29,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:30,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:31,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:32,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:33,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:33,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:34,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:34,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:35,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:36,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:36,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:37,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:38,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:38,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:40,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:40,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:40,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:41,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:41,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:42,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:43,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:43,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:44,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:45,154 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:45,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:46,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:46,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:47,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:48,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:49,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:49,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:49,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:49,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:51,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:52,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:52,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:53,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:54,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:54,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:55,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:56,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:58,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:00,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:00,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:02,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:02,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:02,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:03,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:04,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:04,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:06,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:06,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:08,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:09,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:09,326 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:18,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:20,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:20,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:22,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:23,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:25,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:28,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:28,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:31,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:31,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:31,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:33,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:33,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:36,866 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:37,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:38,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:39,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:40,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:41,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:41,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:41,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:44,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:44,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:45,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:46,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:01,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:01,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:01,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:01,675 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:01,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:02,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:02,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:02,297 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:02,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:02,956 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:06,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:06,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:09,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:09,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:14,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:14,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:17,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:18,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:18,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:19,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:20,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:20,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:20,906 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:21,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:21,135 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:21,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:21,378 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:22,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:24,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:26,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:26,060 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:27,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:27,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:29,856 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:29,857 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:30,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:30,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:30,771 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:30,786 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:32,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:34,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:34,352 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:35,471 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:35,472 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:36,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:36,744 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:37,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:37,959 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:39,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:39,230 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:41,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:43,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:43,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:43,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:43,419 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:43,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:45,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:45,475 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:45,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:45,621 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:46,807 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:47,597 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:47,598 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:47,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:47,742 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:49,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:49,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:49,746 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:51,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:52,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:52,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:53,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:56,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:57,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:59,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:00,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:01,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:01,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:02,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:02,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:02,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:02,812 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:03,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:04,398 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:04,399 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:04,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:04,644 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:05,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:06,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:06,333 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:06,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:06,574 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:06,885 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:06,886 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:07,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:07,123 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:07,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:07,396 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:08,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:11,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:12,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:13,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:14,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:17,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:17,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:18,384 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:18,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:18,898 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:20,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:20,566 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:22,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:22,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:24,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:24,114 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:25,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:25,632 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:27,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:27,191 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:28,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:28,916 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:30,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:30,668 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:31,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:32,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:32,501 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:37,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:41,640 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:43,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:54,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:57,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:58,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:01,132 [WARNING] gen_code timeout (attempt 1)
2025-12-10 15:43:02,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:03,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:04,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:05,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:05,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:05,694 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:06,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:06,791 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:09,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:09,493 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:09,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:10,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:10,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:10,536 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:13,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:13,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:13,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:15,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:15,181 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:18,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:19,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:20,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:20,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:21,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:23,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:24,591 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:26,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:26,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:27,671 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:27,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:27,821 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:29,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:29,239 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:30,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:30,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:30,750 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:32,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:34,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:35,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:37,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:50,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:56,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:57,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:00,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:02,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:02,267 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:02,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:03,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:03,593 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:03,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:04,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:04,890 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:05,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:07,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:07,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:09,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:10,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:11,912 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:12,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:12,076 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:12,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:12,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:12,404 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:14,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:14,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:14,159 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:14,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:14,166 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:14,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:14,523 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:15,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:16,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:16,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:16,210 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:16,214 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:18,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:18,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:18,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:19,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:20,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:20,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:20,433 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:20,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:20,688 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:22,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:22,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:22,963 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:23,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:23,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:24,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:25,617 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:25,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:25,882 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:28,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:28,659 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:29,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:29,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:29,889 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:30,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:30,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:32,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:32,657 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:33,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:33,647 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:34,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:36,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:36,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:36,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:37,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:37,210 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:38,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:38,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:41,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:41,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:41,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:41,407 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:42,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:42,658 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:45,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:46,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:46,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:46,053 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:46,089 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:47,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:47,299 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:47,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:49,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:51,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:51,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:51,075 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:51,079 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:52,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:52,111 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:53,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:55,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:55,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:55,315 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:55,331 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:55,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:56,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:56,830 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:57,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:57,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:00,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:00,418 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:01,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:01,446 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:03,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:05,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:05,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:05,456 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:05,463 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:05,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:05,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:05,931 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:08,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:09,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:10,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:11,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:11,090 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:12,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:12,186 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:12,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:12,196 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:13,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:14,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:17,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:17,103 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:18,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:18,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:18,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:18,098 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:21,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:21,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:23,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:23,271 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:24,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:24,537 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:24,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:24,547 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:26,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:29,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:30,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:30,637 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:30,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:30,948 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:33,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:33,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:36,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:36,101 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:36,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:36,571 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 198115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:38,615 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:39,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:41,787 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:41,789 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:42,243 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:42,243 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 198115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:44,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:45,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:34,525 [INFO] HTTP Request: GET https://huggingface.co/api/datasets/livecodebench/code_generation_lite "HTTP/1.1 200 OK"
2025-12-10 15:59:43,865 [INFO] Indexed livecodebench rows: 1055
2025-12-10 15:59:48,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:48,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:48,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:50,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:50,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:52,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:52,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:53,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:59:53,930 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:59:54,679 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:55,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:59:55,795 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:59:56,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:56,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:57,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:59:57,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:59:57,677 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1027032 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:59:59,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:00,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:00,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:01,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:01,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:02,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:04,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:04,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:05,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:05,617 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:06,425 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:07,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:08,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:08,800 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:12,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:12,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:12,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:14,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:15,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:16,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:17,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:18,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:18,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:22,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:22,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:22,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:23,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:25,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:26,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:27,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:27,819 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:28,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:29,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:29,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:29,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:29,654 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:31,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:31,291 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:34,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,866 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:38,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:41,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:42,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:43,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:43,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:44,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:45,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:46,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:47,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:48,855 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:48,856 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:49,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:50,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:50,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:50,598 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:50,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:50,755 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:51,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:52,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:52,619 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1313784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:52,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:52,757 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:52,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,000 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81599 input tokens (60000 > 129024 - 81599). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:53,161 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,162 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:53,322 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,324 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:53,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81687 input tokens (60000 > 129024 - 81687). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:53,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,667 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:53,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:53,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:54,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:54,035 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108399 input tokens (60000 > 129024 - 108399). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:54,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:54,218 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:54,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:54,405 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:54,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:54,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:54,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:54,606 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108374 input tokens (60000 > 129024 - 108374). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:55,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:55,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:56,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:56,531 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:58,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:58,027 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:59,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:59,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:59,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:59,708 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:59,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:00,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:01,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:01,282 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:02,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:02,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:02,776 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:04,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:04,264 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:05,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:05,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:05,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:05,922 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:07,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:07,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:07,713 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:08,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:09,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:09,248 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:10,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:11,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:12,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:12,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:12,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:12,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:13,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:13,978 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:14,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:14,304 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:15,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:15,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:15,973 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:29,786 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:32,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:32,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:34,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:35,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:35,262 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:37,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:37,249 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:37,937 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:38,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:38,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:39,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:39,517 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1541572 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:40,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:43,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:43,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:44,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:46,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:47,713 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:49,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:49,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:49,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:50,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:50,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:51,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:51,397 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:52,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:52,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:52,548 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:54,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:55,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:57,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:58,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:59,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:59,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:00,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:03,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:04,119 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:04,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:04,679 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:05,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:06,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:06,211 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:07,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:07,783 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:07,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:08,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:08,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:10,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:11,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:14,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:14,004 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:17,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:18,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:18,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:19,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:20,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:20,309 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:21,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:21,421 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:25,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:25,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:28,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:28,621 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3824110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:29,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:29,770 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:32,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:35,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:35,417 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:36,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:36,481 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:39,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:39,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:43,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:43,666 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:44,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:44,957 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:48,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:50,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:50,726 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3842664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:51,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:51,939 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:56,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:56,746 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:01,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:01,489 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:02,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:02,895 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:05,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:08,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:08,600 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:11,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:12,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:12,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:16,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:16,731 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:18,865 [WARNING] gen_code timeout (attempt 1)
2025-12-10 16:03:19,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:19,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:21,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:22,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:22,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:23,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:23,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:24,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:29,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:29,105 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:29,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:29,427 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:32,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:34,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:35,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:35,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3901083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:35,786 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:35,788 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90056 input tokens (60000 > 129024 - 90056). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:38,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:41,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:41,238 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:41,968 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:42,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:42,854 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:42,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:45,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:48,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:48,827 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:50,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:50,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:50,105 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:50,122 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:50,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:51,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:57,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:57,493 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3916002 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:59,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:59,083 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:59,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:59,088 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:01,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:03,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:06,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:06,690 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:07,065 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:07,066 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72238 input tokens (60000 > 129024 - 72238). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:10,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:11,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:11,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:14,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:14,709 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:15,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:15,089 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:19,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:20,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:20,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:23,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:23,035 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4378191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:23,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:23,461 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:26,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:27,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:27,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:31,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:31,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:31,485 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:31,547 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71695 input tokens (60000 > 129024 - 71695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:35,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:36,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:36,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:37,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:41,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:41,745 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:44,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:45,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:46,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:47,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:50,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:50,849 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4964686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:53,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:53,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:56,304 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:58,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:58,905 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:01,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:03,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:03,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:06,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:06,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:06,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:06,630 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:10,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:11,356 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:14,732 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:14,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:14,735 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5027994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:14,798 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:16,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:16,232 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589623 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:17,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:17,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:20,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:23,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:23,753 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:25,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:25,084 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:28,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:29,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:32,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:32,569 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:34,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:34,040 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:35,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:37,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:41,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:41,703 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5115244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:43,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:43,211 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:44,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:44,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:51,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:51,263 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:54,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:55,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:56,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:59,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:59,249 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:01,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:02,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:04,023 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:07,054 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:07,055 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116815 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:10,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:10,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:10,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:15,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:15,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:18,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:18,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:19,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:19,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:23,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:23,580 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:28,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:28,889 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:29,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:30,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:31,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:31,337 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5155981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:34,807 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:35,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:36,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:38,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:39,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:39,919 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:43,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:43,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:44,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:48,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:48,314 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:56,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:56,296 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5267086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:06,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:09,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:09,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:11,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:12,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:14,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:24,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:25,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:28,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:28,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:30,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:31,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:32,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:32,973 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:34,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:38,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:38,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:42,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:42,464 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:44,356 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:49,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:49,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:49,422 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4697400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:52,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:55,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:56,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:56,508 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4697400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:56,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:57,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:59,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:59,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:01,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:03,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:03,601 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4697400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:08,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:08,376 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2932840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:12,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:12,933 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2932840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:17,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:17,895 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2932840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:18,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:18,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:20,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:23,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:24,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:24,019 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:28,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:28,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:29,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:30,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:30,279 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:32,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:33,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:34,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:34,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:35,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:35,486 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:36,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:37,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:39,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:40,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:41,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:41,154 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:42,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:43,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:45,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:46,786 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:46,788 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:49,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:49,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:50,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:50,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:52,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:52,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:54,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:57,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:57,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:58,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:58,277 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:59,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:02,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:03,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:03,398 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:04,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:04,222 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:05,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:09,335 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:09,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:10,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:10,222 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:10,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:11,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:13,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:16,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:16,496 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841820 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:19,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:21,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:22,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:22,279 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841820 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:23,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:25,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:26,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:28,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:29,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:29,062 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841820 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:31,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:33,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:34,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:35,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:35,231 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841598 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:36,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:36,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:39,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:40,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:40,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:40,824 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841598 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:42,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:45,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:45,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:46,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:46,484 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841598 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:48,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:49,292 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:49,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:51,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:53,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:53,254 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4388144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:54,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:55,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:57,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:57,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:59,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:59,562 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4388144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:00,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:00,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:02,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:06,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:06,164 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4388144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:07,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:08,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:08,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:10,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:10,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:10,525 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3009677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:12,787 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:12,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:14,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:14,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:14,998 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3009677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:17,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:17,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:17,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:19,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:19,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:19,171 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3009677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:19,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:23,267 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:25,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:25,483 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:25,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:27,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:28,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:28,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:31,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:31,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:35,760 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:36,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:38,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:38,832 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:39,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:40,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:40,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:42,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:45,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:45,572 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:48,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:48,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:48,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:50,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:52,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:52,407 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:53,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:58,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:58,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3788125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:59,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:01,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:03,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:03,961 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:05,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:05,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:09,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:09,509 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:12,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:12,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:13,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:15,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:15,290 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:16,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:18,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:19,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:19,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:21,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:21,247 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:23,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:24,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:25,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:27,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:27,433 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:28,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:30,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:31,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:32,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:32,965 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:34,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:35,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:38,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:38,641 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:38,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:40,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:40,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:41,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:44,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:44,629 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:46,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:47,757 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:50,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:50,262 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3841857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:53,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:54,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:57,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:57,459 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4778363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:58,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:00,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:01,471 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:04,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:04,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:04,848 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4778363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:05,868 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:07,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:08,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:10,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:12,382 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:12,384 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4778363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:13,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:14,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:14,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:17,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:19,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:19,596 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4109812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:21,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:21,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:21,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:25,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:27,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:27,016 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4109812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:28,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:29,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:33,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:33,907 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4109812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:36,227 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:36,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:36,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:40,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:40,333 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3091561 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:41,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:43,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:43,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:46,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:46,224 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3091561 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:49,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:49,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:51,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:51,191 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3091561 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:53,779 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:57,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:57,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:57,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:57,813 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:00,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:01,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:04,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:04,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:05,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:05,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:07,227 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:09,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:11,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:11,015 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3536074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:13,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:15,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:19,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:19,627 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4799702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:21,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:21,823 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:24,412 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:24,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:25,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:29,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:29,934 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4799702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:32,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:32,024 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:38,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:38,547 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4799702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:40,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:40,829 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:40,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:41,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:45,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:47,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:47,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:49,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:49,276 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:51,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:53,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:54,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:54,850 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:56,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:56,862 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:58,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:02,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:02,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:03,839 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:03,841 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3818174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:05,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:05,836 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089395 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:09,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:12,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:12,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:12,773 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4800009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:12,829 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:14,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:15,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:15,394 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:16,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:16,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:21,679 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:21,680 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4800009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:23,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:23,603 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:26,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:27,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:30,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:30,775 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4800009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:33,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:38,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:38,589 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4889350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:39,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:40,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:43,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:45,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:45,632 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4889350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:48,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:49,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:52,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:52,587 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4889350 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:54,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:55,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:55,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:57,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:58,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:58,392 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3796462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:01,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:01,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:02,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:02,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:04,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:04,478 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3796462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:06,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:07,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:07,615 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:08,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:10,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:10,600 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3796462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:12,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:12,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:13,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:16,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:18,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:18,414 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4882519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:22,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:23,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:26,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:26,338 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4882519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:26,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:29,746 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:30,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:30,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:33,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:33,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:33,635 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4882519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:37,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:37,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:38,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:39,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:40,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:41,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:42,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:44,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:45,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:47,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:48,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:49,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:50,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:51,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:54,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:54,154 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:54,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:54,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:54,754 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:55,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:55,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 329959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:55,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:56,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:56,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:57,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:57,140 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:57,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:58,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:58,088 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 474980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:58,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:59,119 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:59,120 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:59,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:00,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:00,153 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:01,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:01,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:01,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:01,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:02,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:02,325 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:02,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:03,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:03,492 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:04,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:04,631 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:05,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:05,802 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:06,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:06,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:06,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:06,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:06,978 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:08,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:08,124 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:08,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:09,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:09,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:09,335 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:09,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:10,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:10,444 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:10,588 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:11,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:11,761 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:12,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:13,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:13,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:13,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:14,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:14,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:15,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:15,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:16,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:17,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:17,653 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:19,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:19,306 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:19,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:20,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:21,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:21,138 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1196236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:21,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:23,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:23,280 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:23,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:25,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:25,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:25,206 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:27,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:27,035 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1305283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:27,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:27,154 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:27,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:29,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:29,779 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:30,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:32,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:32,071 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:32,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:32,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:33,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:34,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:34,061 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1501509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:35,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:36,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:36,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:36,562 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:37,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:37,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:38,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:38,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:38,835 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:40,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:41,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:41,354 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1505597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:41,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:41,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:43,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:44,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:45,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:46,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:46,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:49,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:50,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:50,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:50,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:52,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:53,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:54,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:54,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:54,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:57,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:57,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:57,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:57,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:00,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:00,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:01,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:02,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:02,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:04,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:06,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:07,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:09,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:09,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:10,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:11,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:12,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:13,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:14,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:15,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:17,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:19,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:20,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:21,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:28,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:28,856 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:32,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:34,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:35,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:36,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:38,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:38,791 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:41,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:41,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:42,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:48,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:48,539 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6626226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:49,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:51,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:51,606 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1966887 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:53,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:02,510 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:02,511 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:04,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:06,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:06,113 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1966887 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:06,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:08,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:16,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:16,894 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:18,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:20,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:20,371 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1966887 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:22,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:31,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:31,058 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7327766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:32,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:32,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 281857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:36,308 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:36,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:38,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:44,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:44,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:45,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:45,254 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 281857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:47,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:48,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:58,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:58,160 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:59,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:59,018 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 281857 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:01,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:02,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:10,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:10,362 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7316913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:14,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:16,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:16,221 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3607158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:18,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:18,221 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:19,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:21,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:23,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:26,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:26,585 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3607158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:28,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:28,818 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:31,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:31,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:32,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:37,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:37,051 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3607158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:39,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:39,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1288983 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:40,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:40,365 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 149260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:41,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:42,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:42,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:44,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:44,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:45,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:45,007 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 149260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:47,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:48,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:49,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:50,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:50,029 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:50,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:50,292 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 149260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:52,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:54,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:54,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:55,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:55,914 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3246964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:58,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:58,117 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1380041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:01,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:01,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:09,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:10,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:10,169 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:12,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:12,459 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1380041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:15,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:15,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:16,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:24,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:24,801 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:27,035 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:27,037 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1380041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:29,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:32,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:35,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:37,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:37,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:37,479 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7376909 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:37,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:38,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:38,225 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:41,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:41,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:43,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:45,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:45,864 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:46,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:46,482 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:50,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:52,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:54,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:54,825 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:57,328 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:59,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:59,047 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2887198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:01,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:05,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:05,082 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3781305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:07,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:09,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:09,602 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2887198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:11,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:12,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:16,370 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:16,372 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:20,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:20,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:20,726 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2887198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:22,887 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:27,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:27,942 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:29,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:29,078 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:29,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:31,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:36,953 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:38,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:38,820 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4653382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:40,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:40,129 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:43,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:44,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:45,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:56,047 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:56,049 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:57,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:57,698 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 746179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:01,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:02,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:07,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:14,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:14,924 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:15,517 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:15,519 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115920 input tokens (60000 > 129024 - 115920). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:19,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:20,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:23,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:34,640 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:34,642 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9997600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:35,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:35,456 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115920 input tokens (60000 > 129024 - 115920). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:37,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:42,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:42,939 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:45,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:47,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:47,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:47,655 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:47,697 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115920 input tokens (60000 > 129024 - 115920). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:54,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:54,740 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:57,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:59,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:59,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:59,437 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:59,494 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 340122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:01,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:06,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:06,661 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:08,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:11,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:11,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:11,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:11,583 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 340122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:14,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:14,659 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178346 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:14,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:15,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:29,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:29,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:29,513 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:29,615 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 340122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:33,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:33,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178346 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:34,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:34,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110112 input tokens (60000 > 129024 - 110112). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:34,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:38,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:48,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:48,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:48,815 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:48,915 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178346 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:49,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:49,609 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110112 input tokens (60000 > 129024 - 110112). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:50,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:50,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:50,844 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 626472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:52,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:04,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:04,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:04,946 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9994846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:04,951 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110112 input tokens (60000 > 129024 - 110112). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:06,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:06,364 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 626472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:08,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:08,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:08,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:08,607 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:08,635 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 626472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:10,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:10,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:11,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:18,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:18,029 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:18,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:18,067 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844929 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:29,510 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:29,511 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:31,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:32,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:32,169 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:33,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:43,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:43,283 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:43,285 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:43,364 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844929 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:45,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:45,279 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 875405 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:47,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:56,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:56,100 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7314597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:57,181 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:03,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:03,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:03,496 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 875405 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:03,518 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844929 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:06,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:08,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:08,487 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:09,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:16,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:16,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:16,006 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 875405 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:16,048 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4867030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:21,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:21,334 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:25,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:25,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:25,936 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2876509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:27,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:33,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:33,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:33,115 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3178418 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:33,184 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4867030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:38,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:38,927 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2876509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:42,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:47,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:47,038 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:47,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:54,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:54,760 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2876509 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:54,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:54,794 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4867030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:03,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:03,212 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:04,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:04,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:04,844 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:05,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:12,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:12,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:12,850 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4650866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:12,951 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:14,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:14,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:15,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:17,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:17,553 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:18,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:32,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:32,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:32,829 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:32,929 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:35,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:35,577 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:36,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:51,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:51,430 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:51,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:51,555 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4355872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:52,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:52,749 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 220960 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:54,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:59,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:59,267 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4355872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:00,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:13,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:13,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:13,478 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9993579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:13,546 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 220960 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:14,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:20,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:20,266 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4355872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:23,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:27,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:27,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:27,665 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:27,719 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 220960 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:35,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:35,075 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:36,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:38,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:42,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:42,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:42,138 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:42,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 420195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:49,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:49,580 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:50,755 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:50,757 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 420195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:50,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:52,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:58,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:58,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:58,351 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4504978 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:58,396 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:59,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:59,503 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 420195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:01,091 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:06,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:06,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:06,813 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:06,859 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:07,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:07,418 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177008 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:08,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:12,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:12,340 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:19,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:19,444 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177008 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:19,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:19,455 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:20,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:23,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:24,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:24,786 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3131796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:31,953 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:31,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:31,956 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177008 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:31,966 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:33,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:33,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:43,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:43,675 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:51,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:51,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:51,136 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128643 input tokens (60000 > 129024 - 128643). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:51,144 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844669 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:52,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:53,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:02,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:02,629 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:02,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:02,719 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128643 input tokens (60000 > 129024 - 128643). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:10,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:10,265 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844669 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:10,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:22,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:22,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7514849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:22,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:22,137 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128643 input tokens (60000 > 129024 - 128643). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:24,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:29,614 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844669 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:31,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:37,597 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:37,599 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:37,656 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 711901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:41,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:45,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:45,433 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:49,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:53,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:53,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:53,211 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:53,271 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 711901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:00,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:00,919 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:07,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:08,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:08,695 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4977910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:08,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:08,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:08,697 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2469726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:08,724 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 711901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:16,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:16,010 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:18,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:19,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:19,978 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:19,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:19,982 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2469726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:27,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:27,318 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:29,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:29,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:31,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:31,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:31,354 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:31,363 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2469726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:38,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:38,647 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:39,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:44,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:44,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:44,301 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:44,308 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:46,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:51,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:51,137 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:52,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:56,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:56,588 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85695 input tokens (60000 > 129024 - 85695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:56,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:56,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:02,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:02,104 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3585314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:03,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:07,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:07,617 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:07,619 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85695 input tokens (60000 > 129024 - 85695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:07,625 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:12,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:13,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:13,073 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3585314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:16,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:18,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:18,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:18,226 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85695 input tokens (60000 > 129024 - 85695). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:18,234 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:23,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:23,827 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3585314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:24,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:28,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:28,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:28,939 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 131734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:28,947 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:31,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:35,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:35,610 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:38,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:41,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:41,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:41,129 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 131734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:41,137 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:48,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:48,279 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:48,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:48,853 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 131734 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:52,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:52,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:55,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:55,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:55,963 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556429 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:55,998 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:56,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:56,601 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111799 input tokens (60000 > 129024 - 111799). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:58,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:01,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:01,769 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556429 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:03,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:08,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:08,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:08,256 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111799 input tokens (60000 > 129024 - 111799). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:08,263 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4270718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:14,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:14,324 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556429 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:15,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:21,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:21,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:21,253 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111799 input tokens (60000 > 129024 - 111799). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:21,260 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4270718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:25,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:27,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:27,095 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:28,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:33,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:33,786 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:33,787 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102026 input tokens (60000 > 129024 - 102026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:33,795 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4270718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:39,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:39,395 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:40,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:40,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:43,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:43,903 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102026 input tokens (60000 > 129024 - 102026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:43,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:43,906 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2714074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:49,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:49,008 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:49,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:53,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:53,728 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:53,729 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102026 input tokens (60000 > 129024 - 102026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:53,730 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2714074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:56,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:58,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:58,893 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:59,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:02,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:02,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2714074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:05,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:08,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:08,666 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:12,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:12,258 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2255106 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:12,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:13,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:17,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:17,763 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:19,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:21,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:21,382 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2255106 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:23,375 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:27,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:27,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:28,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:30,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:30,738 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2255106 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:32,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:36,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:36,007 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:38,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:43,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:43,230 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:45,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:49,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:49,318 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:51,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:57,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:57,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:00,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:04,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:04,369 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:06,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:13,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:13,117 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:15,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:19,521 [WARNING] gen_code timeout (attempt 1)
2025-12-10 16:34:20,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:20,254 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:21,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:27,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:27,837 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3878326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:29,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:33,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:33,602 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:39,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:39,733 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3878326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:41,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:43,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:43,986 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2534527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:46,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:50,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:50,029 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3878326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:53,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:53,989 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2534527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:54,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:58,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:03,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:03,210 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:07,398 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:07,400 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2534527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:10,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:14,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:14,896 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:17,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:18,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:18,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2535727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:19,524 [WARNING] gen_code timeout (attempt 2)
2025-12-10 16:35:21,408 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:26,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:26,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:26,181 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:26,248 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:30,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:30,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2535727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:31,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:31,629 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:36,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:39,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:39,490 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845102 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:39,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:39,543 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2535727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:40,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:40,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:44,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:48,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:48,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:48,404 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845102 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:48,459 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:49,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:49,574 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:51,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:57,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:57,733 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845102 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:57,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:57,792 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:58,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:58,776 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:00,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:07,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:07,260 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:07,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:07,317 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555845 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:08,586 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:08,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:12,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:17,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:17,137 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:17,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:17,208 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:18,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:18,458 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:19,526 [WARNING] gen_code timeout (attempt 3)
2025-12-10 16:36:20,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:26,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:26,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:26,611 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:26,675 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:28,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:28,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:29,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:30,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:35,788 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:35,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:35,791 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:35,857 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:37,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:37,182 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:38,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:46,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:46,078 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:46,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:46,141 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:50,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:50,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:51,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:54,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:54,610 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4845025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:54,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:54,677 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:58,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:59,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:03,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:03,072 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:04,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:09,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:09,180 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:11,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:17,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:17,136 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:18,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:19,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:23,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:23,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555826 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:25,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:31,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:31,493 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:34,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:37,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:37,709 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555826 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:41,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:45,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:45,439 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:46,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:47,558 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:50,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:50,995 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555826 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:53,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:58,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:58,556 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:01,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:01,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:04,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:04,718 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:09,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:13,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:13,841 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4844704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:16,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:18,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:20,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:20,784 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:27,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:27,978 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:33,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:33,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:36,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:36,263 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:36,308 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:37,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:37,203 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:39,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:40,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:42,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:42,465 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:42,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:42,637 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:43,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:45,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:48,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:48,573 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:48,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:48,891 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80558 input tokens (60000 > 129024 - 80558). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:50,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:50,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:54,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:54,291 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555720 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:54,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:54,629 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:57,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:59,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:01,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:01,656 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555720 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:01,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:01,952 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:04,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:04,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:07,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:07,712 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3555720 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:08,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:08,008 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80832 input tokens (60000 > 129024 - 80832). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:10,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:11,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:14,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:14,533 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:14,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:14,816 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:16,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:17,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:20,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:20,829 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:21,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:21,121 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:25,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:25,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:27,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:27,100 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:27,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:27,372 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80924 input tokens (60000 > 129024 - 80924). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:30,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:30,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:31,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:33,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:35,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:36,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:36,584 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556065 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:40,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:40,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:40,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:42,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:42,951 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556065 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:44,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:47,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:48,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:48,584 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556065 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:49,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:49,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:49,421 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:50,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:50,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:50,709 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:51,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:51,648 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:52,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:52,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:52,679 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:52,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:53,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:53,687 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:53,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:54,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:54,653 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 613219 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:57,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:58,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:58,234 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:58,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:59,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:00,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:00,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:01,519 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:01,520 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:02,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:02,736 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:05,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:06,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:06,096 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:06,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:06,580 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:06,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:06,597 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1915153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:09,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:10,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:11,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:11,978 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:11,979 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 637564 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:12,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:12,004 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:14,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:14,354 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:14,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:15,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:16,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:19,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:19,042 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:21,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:21,221 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:23,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:24,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:26,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:26,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:26,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:26,462 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:26,491 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2500411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:30,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:30,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:30,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:31,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:34,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:34,801 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:37,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:38,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:38,427 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:43,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:43,791 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:43,792 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:45,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:46,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:47,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:47,913 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2017437 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:53,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:53,370 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2646718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:53,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:55,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:57,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:57,654 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:59,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:03,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:03,359 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:06,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:07,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:07,276 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:10,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:11,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:12,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:12,550 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:14,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:16,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:16,410 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2277416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:18,603 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:22,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:22,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3044586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:24,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:26,501 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:26,503 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:28,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:33,379 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:33,381 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:35,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:37,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:37,282 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:41,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:43,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:44,887 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:44,889 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:47,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:49,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:49,017 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2279097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:50,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:56,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:56,339 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3893354 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:58,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:59,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:59,934 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:01,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:07,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:07,017 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:09,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:11,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:11,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:11,968 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:15,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:20,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:20,243 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:23,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:25,591 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:25,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2350574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:26,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:28,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:34,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:34,337 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3922398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:36,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:39,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:39,877 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:42,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:47,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:47,930 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:50,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:50,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:52,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:52,266 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:57,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:59,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:59,167 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:01,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:03,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:03,680 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2575658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:06,243 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:10,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:10,622 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3924739 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:13,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:13,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:15,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:15,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:17,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:24,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:24,915 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:28,147 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:29,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:29,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:29,972 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:31,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:40,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:40,009 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:42,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:45,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:45,130 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3068192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:46,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:50,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:53,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:53,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546327 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:55,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:59,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:59,755 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:01,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:09,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:09,539 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:12,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:15,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:15,544 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:17,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:24,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:24,820 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:28,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:29,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:30,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:30,177 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3352462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:32,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:38,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:38,836 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:42,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:44,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:44,640 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:46,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:49,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:55,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:55,039 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:57,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:01,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:01,403 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:04,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:07,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:12,488 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:12,489 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:14,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:20,460 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3413058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:22,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:22,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:31,978 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:31,979 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5546338 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:34,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:40,469 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:42,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:44,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:45,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:48,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:48,203 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:50,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:51,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:54,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:54,260 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3761072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:56,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:58,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:59,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:00,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:00,959 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:02,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:04,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:04,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:04,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:08,072 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:09,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:10,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:14,716 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3889597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:16,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:18,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:18,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:18,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:18,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:22,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:23,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:23,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:24,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:26,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:27,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:27,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:27,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:29,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:30,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:31,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:32,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:32,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:33,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:33,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:34,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:37,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:37,131 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:37,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:38,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:38,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:40,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:41,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:51,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:52,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:52,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:54,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:55,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:55,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:55,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:57,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:57,543 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:59,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:00,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:01,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:02,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:02,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:03,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:03,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:05,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:07,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:09,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:09,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:10,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:11,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:11,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:13,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:14,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:22,517 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:22,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:22,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:23,203 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:25,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:25,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:26,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:30,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:30,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:31,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:31,559 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:43,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:44,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:44,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:47,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:49,669 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:49,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:52,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:52,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:53,551 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:55,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:55,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:55,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:57,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:57,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:58,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:58,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:00,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:00,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:01,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:02,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:03,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:03,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:04,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:06,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:06,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:06,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:08,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:08,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:09,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:10,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:12,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:12,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:12,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:16,728 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:16,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:17,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:19,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:21,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:22,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:23,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:24,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:25,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:26,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:27,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:28,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:29,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:31,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:33,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:34,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:35,387 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:37,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:37,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:40,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:43,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:44,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:44,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:46,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:49,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:49,322 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:49,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:53,558 [WARNING] gen_code timeout (attempt 1)
2025-12-10 16:48:54,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:56,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:58,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:59,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:01,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:02,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:02,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:02,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:05,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:08,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:09,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:10,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:12,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:12,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:19,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:24,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:27,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:28,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:28,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:29,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:36,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:36,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:37,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:40,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:42,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:44,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:48,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:49,624 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:52,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:53,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:54,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:55,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:57,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:58,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:58,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:59,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:00,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:01,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:02,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:03,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:04,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:04,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:04,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:08,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:09,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:10,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:12,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:12,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:12,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:13,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:13,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:15,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:15,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:16,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:16,907 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:17,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:17,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:18,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:19,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:20,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:21,558 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:21,760 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:22,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:22,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:25,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:25,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:26,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:27,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:27,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:28,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:28,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:28,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:31,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:33,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:35,131 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:35,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:35,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:36,827 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:37,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:41,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:43,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:44,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:45,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:47,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:47,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:47,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:49,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:50,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:50,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:53,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:53,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:53,882 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:54,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:54,158 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:54,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:54,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:54,440 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109596 input tokens (60000 > 129024 - 109596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:54,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:57,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:57,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:58,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:59,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:00,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:00,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:01,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:02,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:02,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:04,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:04,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:04,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:05,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:07,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:07,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:08,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:08,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:09,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:10,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:12,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:15,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:17,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:17,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:19,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:21,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:22,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:24,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:27,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:28,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:28,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:28,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:28,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:30,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:31,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:32,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:33,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:33,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:35,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:35,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:35,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:37,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:38,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:38,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:38,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:39,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:42,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:42,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:42,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:45,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:46,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:46,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:47,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:50,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:53,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:53,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:55,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:55,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:56,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:57,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:05,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:05,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:06,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:06,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:07,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:08,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:10,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:12,617 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:12,833 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:13,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:13,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:15,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:17,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:17,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:17,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:18,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:18,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:19,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:22,188 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:22,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:24,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:24,593 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:31,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:35,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:35,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:40,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:50,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:58,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:00,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:02,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:04,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:06,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:07,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:08,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:09,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:10,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:10,937 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:11,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:12,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:12,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:13,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:14,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:14,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:15,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:15,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:22,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:23,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:23,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:30,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:31,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:32,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:32,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:32,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:42,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:46,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:55,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:55,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:00,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:00,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:01,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:02,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:04,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:06,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:09,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:10,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:13,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:13,660 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:14,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:14,788 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:17,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:17,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:17,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:18,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:18,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:19,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:20,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:21,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:22,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:22,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:23,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:23,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:24,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:24,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:25,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:25,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:30,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:31,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:32,328 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:32,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:33,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:33,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:34,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:34,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:35,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:37,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:37,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:37,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:38,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:38,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:38,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:38,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:39,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:39,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:42,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:43,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:44,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:44,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:46,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:46,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:47,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:48,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:49,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:49,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:56,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:57,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:59,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:59,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:01,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:01,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:02,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:04,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:05,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:06,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:06,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:06,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:07,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:09,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:10,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:10,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:10,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:12,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:13,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:14,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:14,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:16,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:16,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:17,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:17,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:18,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:19,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:20,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:20,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:22,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:24,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:24,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:25,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:25,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:26,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:27,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:28,356 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:28,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:29,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:30,297 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:31,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:33,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:33,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:34,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:34,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:35,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:36,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:36,297 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:36,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:37,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:38,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:38,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:39,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:41,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:42,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:44,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:44,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:46,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:46,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:46,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:48,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:49,885 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:51,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:55,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:56,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:56,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:57,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:59,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:59,489 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978233 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:59,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:02,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:02,267 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978233 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:03,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:04,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:05,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:05,023 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978233 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:05,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:05,773 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 407911 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:06,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:06,424 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 407911 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:07,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:07,123 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 407911 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:07,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:07,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:09,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:09,921 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978143 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:10,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:12,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:12,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:13,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:13,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978143 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:13,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:15,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:15,873 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978143 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:16,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:16,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:18,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:18,619 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:20,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:21,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:21,678 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:21,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:22,159 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:23,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:24,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:24,492 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:25,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:25,534 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 622113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:26,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:26,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 622113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:27,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:27,644 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 622113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:27,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:28,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:28,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:30,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:30,645 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:33,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:33,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:33,436 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:34,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:35,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:36,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:36,182 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:36,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:38,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:39,449 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:39,450 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:41,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:42,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:42,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:42,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:42,634 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:43,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:46,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:46,164 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:46,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:46,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:47,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:49,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:49,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:51,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:51,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:51,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:51,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:51,902 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:52,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:54,586 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:54,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:54,715 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:55,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:57,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:57,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:57,853 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978228 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:57,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:00,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:00,815 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978228 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:01,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:01,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:02,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:03,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:03,931 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978228 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:05,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:07,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:07,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:07,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:08,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:09,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:10,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:10,373 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:11,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:12,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:13,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:13,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:13,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:13,585 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:16,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:16,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:16,686 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:16,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:16,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:17,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:18,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:20,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:20,128 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:21,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:23,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:23,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:23,505 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:23,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:26,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:26,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:26,351 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:27,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:28,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:29,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:29,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:30,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:31,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:32,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:32,722 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:32,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:35,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:36,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:36,233 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:36,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:39,131 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:39,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:40,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:40,397 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:43,335 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:43,474 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:43,476 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:44,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:44,042 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:44,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:44,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:46,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:46,836 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:47,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:47,418 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:50,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:50,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:50,686 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:51,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:51,328 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:54,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:55,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:55,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:55,470 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:55,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:58,408 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:58,410 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:58,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:59,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:59,326 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:00,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:02,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:02,100 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:04,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:05,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:05,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:05,834 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:06,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:06,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:07,760 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:07,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:09,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:09,068 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:12,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:12,524 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:12,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:14,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:14,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:15,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:16,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:16,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978398 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:17,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:17,037 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:17,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:17,653 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:17,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:17,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79324 input tokens (60000 > 129024 - 79324). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:19,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:19,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:21,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:21,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:22,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:22,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:22,995 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:23,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:23,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:26,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:26,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:27,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:27,755 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:28,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:28,810 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:29,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:31,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:31,733 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:32,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:32,773 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 560229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:33,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:36,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:36,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:36,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:38,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:38,014 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:38,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:41,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:41,391 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:42,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:42,466 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:45,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:46,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:46,441 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:47,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:47,456 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:51,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:52,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:52,198 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:54,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:55,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:56,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:56,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:56,635 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:56,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:56,887 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:00,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:01,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:01,085 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:01,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:01,460 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:01,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:02,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:02,016 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79279 input tokens (60000 > 129024 - 79279). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:05,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:06,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:10,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:11,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:12,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:13,779 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:15,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:15,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:17,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:17,249 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:18,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:18,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:18,925 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:19,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:20,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:20,517 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789229 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:20,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:20,819 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:21,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:21,086 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:21,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:21,350 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108671 input tokens (60000 > 129024 - 108671). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:24,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:24,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:25,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:28,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:28,342 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:28,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:28,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:29,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:29,661 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:32,449 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:32,450 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:34,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:34,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:36,181 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:37,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:37,803 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:37,805 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1887688 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:38,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:38,925 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:40,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:40,538 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:41,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:41,903 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:43,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:43,345 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:44,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:44,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:46,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:46,769 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:47,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:48,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:48,634 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:58,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:04,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:05,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:06,870 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:08,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:08,223 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:10,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:10,250 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:12,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:12,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1371683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:15,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:16,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:18,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:20,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:21,054 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:24,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:28,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:28,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:28,620 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:28,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:28,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:28,958 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:29,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:29,268 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143861 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:32,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:34,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:35,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:37,254 [WARNING] gen_code timeout (attempt 1)
2025-12-10 17:00:39,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:42,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:46,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:50,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:51,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:57,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:01,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:03,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:05,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:07,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:11,679 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:13,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:14,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:20,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:21,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:34,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:34,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:36,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:38,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:39,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:40,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:42,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:42,129 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:42,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:43,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:45,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:45,019 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:45,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:45,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:45,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:45,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:47,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:48,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:48,604 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1886667 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:49,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:49,945 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:50,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:51,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:51,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:51,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:53,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:54,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:54,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:54,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:55,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:57,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:59,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:00,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:01,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:01,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:02,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:02,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:02,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:04,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:04,074 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:05,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:05,640 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:05,642 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:06,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:06,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:08,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:11,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:17,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:24,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:27,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:28,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:29,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:29,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:31,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:31,106 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:31,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:32,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:32,559 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:32,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:33,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:33,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:33,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:33,966 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:38,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:38,520 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:38,521 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:38,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:38,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:38,961 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:39,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:39,355 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 188703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:41,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:44,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:44,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:44,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:44,318 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:44,569 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:44,570 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:44,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:44,842 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79281 input tokens (60000 > 129024 - 79281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:47,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:47,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:47,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:50,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:51,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:51,226 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:51,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:51,597 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:51,598 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:51,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:51,920 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:53,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:53,298 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:53,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:53,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:54,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:54,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:54,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:54,741 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:56,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:56,215 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:57,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:57,502 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:58,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:58,725 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:59,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:59,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:59,941 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 620645 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:59,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:01,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:01,422 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:02,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:02,953 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:04,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:04,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:04,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:05,674 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:07,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:09,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:09,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:10,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:11,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:11,122 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:11,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:12,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:12,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:12,242 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:13,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:14,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:15,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:15,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:15,934 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:15,938 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:16,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:17,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:19,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:19,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:20,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:20,600 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:21,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:23,196 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:24,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:24,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:24,835 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2352086 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:24,839 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:26,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:26,518 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:28,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:28,352 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:28,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:29,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:30,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:30,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:30,527 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:30,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:32,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:32,313 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:32,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:32,608 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:34,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:34,420 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:34,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:34,720 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:35,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:35,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:36,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:36,434 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1133264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:36,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:36,711 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:38,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:39,054 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:39,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:39,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:39,470 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:41,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:42,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:42,296 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:42,325 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:42,615 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:42,617 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:44,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:44,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:44,996 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1624464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:45,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:45,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185588 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:45,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:47,267 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:48,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:48,071 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:48,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:48,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:48,432 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 198115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:49,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:50,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:50,836 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:51,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:51,234 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 198115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:53,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:53,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:53,695 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1700853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:54,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:54,090 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 198115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:55,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:55,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:56,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:59,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:59,286 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:04,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:04,383 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:04,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:06,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:09,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:09,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2346998 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:09,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:12,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:12,460 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1985195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:14,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:14,387 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:17,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:17,394 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:18,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:20,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:20,554 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1985195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:22,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:25,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:25,892 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:27,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:28,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:29,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:29,531 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1985195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:31,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:34,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:34,952 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3162704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:37,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:40,679 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:41,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:42,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:42,292 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:42,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:44,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:44,703 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502371 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:45,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:49,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:49,646 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:52,476 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:52,478 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502371 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:57,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:57,180 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3420552 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:57,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:59,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:59,404 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502371 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:59,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:59,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:06,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:06,378 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:08,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:08,848 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 915362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:08,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:11,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:16,181 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:16,182 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:19,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:19,348 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 915362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:26,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:26,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:26,377 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:26,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2011299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:28,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:28,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:29,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:29,545 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 915362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:37,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:37,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:37,682 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:37,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2011299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:41,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:41,191 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 868612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:41,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:44,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:48,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:48,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:48,171 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:48,229 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2011299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:50,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:50,634 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 868612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:51,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:57,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:57,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:57,151 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:57,210 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:59,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:59,394 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 868612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:01,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:02,757 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:05,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:05,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:05,874 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:05,934 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:09,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:09,020 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1271650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:09,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:09,368 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140703 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:11,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:12,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:16,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:16,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:16,342 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1271650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:16,386 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:16,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:16,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122813 input tokens (60000 > 129024 - 122813). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:18,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:20,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:20,018 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1271650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:23,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:26,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:26,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:26,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:26,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122813 input tokens (60000 > 129024 - 122813). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:29,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:29,288 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1754158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:31,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:32,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:36,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:36,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:36,787 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:36,845 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122813 input tokens (60000 > 129024 - 122813). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:39,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:39,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1754158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:41,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:46,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:46,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:46,340 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:46,400 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:49,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:49,234 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1754158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:51,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:55,828 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:55,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:55,830 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4578243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:55,831 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:58,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:58,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1614423 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:00,688 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:00,981 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:00,983 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:02,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:03,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:03,445 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1614423 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:03,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:06,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:06,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:06,021 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1774839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:08,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:08,484 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1614423 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:11,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:11,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1774839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:12,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:14,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:14,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:14,795 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1800542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:17,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:17,628 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1774839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:20,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:20,190 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1800542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:20,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:20,614 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:21,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:22,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:22,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:22,941 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1800542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:23,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:23,186 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:25,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:25,153 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:25,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:25,466 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:25,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:25,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:26,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:27,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:27,353 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:29,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:29,385 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 502384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:29,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:29,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:29,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:31,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:31,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:31,632 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:33,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:33,888 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:34,686 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:34,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:36,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:36,153 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:39,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:40,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:42,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:44,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:46,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:46,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:48,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:48,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:50,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:50,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:53,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:54,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:10,603 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:14,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:15,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:21,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:25,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:29,736 [WARNING] gen_code timeout (attempt 1)
2025-12-10 17:08:30,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:34,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:35,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:36,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:46,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:48,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:51,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,914 [WARNING] gen_code timeout (attempt 1)
2025-12-10 17:08:56,177 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:56,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:56,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:56,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 139329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:56,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:56,777 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 139329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:57,106 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:57,107 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 139329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:58,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:58,547 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:59,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:00,000 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:01,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:01,369 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:01,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:05,412 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:06,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:10,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:10,004 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5591023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:14,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:15,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:16,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:18,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5591023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:22,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:24,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:27,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:27,250 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5591023 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:31,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:38,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:39,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:39,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:43,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:43,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:47,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:48,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:53,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:53,901 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:54,919 [WARNING] gen_code timeout (attempt 2)
2025-12-10 17:09:57,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:06,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:08,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:12,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:22,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:23,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:31,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:37,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:37,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:39,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:42,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:47,399 [WARNING] gen_code timeout (attempt 1)
2025-12-10 17:10:47,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:51,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:52,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:54,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:54,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:54,921 [WARNING] gen_code timeout (attempt 3)
2025-12-10 17:10:55,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:58,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:58,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:00,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:01,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:03,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:05,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:05,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:06,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:06,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:06,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,680 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:12,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:18,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:18,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:19,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:21,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:21,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:25,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:27,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:29,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:29,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:30,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:31,642 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:31,868 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:31,870 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:32,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:32,122 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:32,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:32,367 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:32,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:33,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:34,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:35,047 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:35,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:35,495 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:35,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:35,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:35,978 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:36,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:36,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:36,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:36,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:36,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:36,941 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235534 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:37,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:37,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:37,525 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235534 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:37,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:37,997 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235534 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:38,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:38,429 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:38,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:38,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:38,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:38,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:39,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:39,257 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195173 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:39,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:39,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:39,779 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 248018 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:40,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:40,234 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 248018 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:40,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:40,781 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 248018 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:40,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:41,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:41,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:41,314 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247786 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:41,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:41,850 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247786 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:42,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:42,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:42,394 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247786 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:42,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:42,640 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:42,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:42,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:42,978 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:43,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:43,152 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:43,203 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:43,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:43,503 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:43,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:43,680 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:44,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:44,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 195828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:44,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:44,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:46,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:47,464 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:49,026 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:49,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:50,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:56,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:57,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:57,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:03,173 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:03,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:03,402 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:03,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:03,656 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:03,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:03,906 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:03,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:04,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:07,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:09,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:10,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:12,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:14,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:15,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:18,819 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:19,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:19,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:19,525 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:19,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:19,773 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:20,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:20,013 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:20,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:21,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:22,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:23,117 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:24,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:24,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:24,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:26,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:27,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:27,392 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489176 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:27,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:27,650 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:31,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:31,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:31,761 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489176 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:32,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:32,031 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:34,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:36,065 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:37,096 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:37,097 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489176 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:37,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:37,391 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:39,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:41,779 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:41,780 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2725585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:43,858 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:44,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:45,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:46,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:46,886 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2725585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:49,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:50,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:51,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:51,014 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2725585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:54,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:55,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:55,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:56,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:56,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2836854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:56,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:59,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:00,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:01,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:01,693 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2836854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:04,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:05,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:05,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:06,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:07,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:07,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2836854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:09,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:10,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:11,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:11,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:15,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:15,383 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3845433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:16,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:18,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:19,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:19,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:22,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:22,836 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3845433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:26,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:26,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:26,981 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:26,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:30,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:30,509 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3845433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:32,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:34,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:34,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:36,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:38,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:38,565 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3994152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:40,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:42,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:42,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:45,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:46,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:46,715 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3994152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:48,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:49,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:50,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:54,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:54,615 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3994152 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:55,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:55,062 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:57,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:57,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:02,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:02,574 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:02,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:02,860 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:06,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:06,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:09,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:09,859 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:10,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:10,200 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100563 input tokens (60000 > 129024 - 100563). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:12,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:14,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:17,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:17,178 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4289268 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:17,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:17,544 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:21,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:23,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:25,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:25,601 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:26,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:26,034 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:28,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:29,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:34,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:34,679 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:35,269 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:35,270 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:36,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:37,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:43,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:43,331 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:46,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:47,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:49,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:53,292 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:53,294 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:55,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:56,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:57,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:02,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:02,951 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:06,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:07,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:07,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:11,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:11,702 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5867054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:11,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:11,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578092 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:15,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:15,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:16,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:21,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:21,732 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6067053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:21,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:21,815 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578092 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:24,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:26,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:26,602 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578092 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:26,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:28,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:35,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:35,244 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6067053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:36,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:38,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:39,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:44,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:44,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6067053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:47,889 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:48,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:48,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:48,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:55,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:55,386 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6089302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:55,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:55,655 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:58,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:05,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:05,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6089302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:06,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:06,050 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:07,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:09,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:10,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:16,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:16,125 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6089302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:16,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:16,302 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100561 input tokens (60000 > 129024 - 100561). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:16,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:16,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69268 input tokens (60000 > 129024 - 69268). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:17,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:17,053 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69268 input tokens (60000 > 129024 - 69268). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:17,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:17,236 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69268 input tokens (60000 > 129024 - 69268). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:17,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:18,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:18,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:20,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:20,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:20,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:35,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:38,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:47,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:48,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:49,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:02,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:03,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:05,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:06,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:16,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:18,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:33,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:34,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:34,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:35,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:39,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:40,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:54,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:54,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:57,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:00,119 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:00,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:00,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:00,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:00,501 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121522 input tokens (60000 > 129024 - 121522). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:00,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:00,818 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121522 input tokens (60000 > 129024 - 121522). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:01,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:01,134 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121522 input tokens (60000 > 129024 - 121522). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:03,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:17,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:20,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:33,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:38,557 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:46,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:50,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:51,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:51,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:56,503 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:56,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:57,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:06,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:09,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:10,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69077 input tokens (60000 > 129024 - 69077). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:10,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:10,237 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69077 input tokens (60000 > 129024 - 69077). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:10,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:10,441 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69077 input tokens (60000 > 129024 - 69077). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:10,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:23,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:27,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:29,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:29,641 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:39,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:40,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:42,325 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:52,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:54,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:56,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:58,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:14,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:17,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:26,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:27,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:42,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:43,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:46,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:49,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:58,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:58,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:58,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:00,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:15,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:17,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:17,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:23,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:27,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:42,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:42,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:48,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:49,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:50,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:50,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:52,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:55,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:55,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:58,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:00,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:01,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:03,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:03,024 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 863627 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:03,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:04,860 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 863627 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:06,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:06,188 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 863627 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:09,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:11,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:13,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:14,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:15,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:17,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:19,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:20,154 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:20,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:25,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:25,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:29,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:30,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:31,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:31,240 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 772899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:31,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:32,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:32,706 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 772899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:33,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:34,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:34,151 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 772899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:37,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:39,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:40,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:40,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:40,858 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:40,859 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 682198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:41,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:42,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:42,320 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 682198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:43,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:43,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:43,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 682198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:46,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:46,950 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1852936 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:47,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:47,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:49,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:50,019 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1852936 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:50,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:50,572 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 332726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:50,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:53,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:53,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:53,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:53,812 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1852936 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:54,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:54,365 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 332726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:55,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:55,075 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 332726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:58,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:58,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:02,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:02,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:02,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:06,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:11,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:12,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:13,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:15,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:17,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:20,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:22,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:23,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:23,007 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 430754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:23,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:23,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:23,894 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 430754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:24,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:24,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:24,719 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 430754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:26,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:27,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:27,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:27,184 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 257935 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:27,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:27,659 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 257935 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:28,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:28,199 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 257935 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:28,234 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:29,023 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:29,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:32,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:33,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:33,978 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:34,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:34,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:37,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:37,988 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2321411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:39,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:40,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:40,755 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:41,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:41,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:41,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2321411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:42,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:42,666 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:46,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:46,971 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2321411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:48,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:48,363 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:49,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:49,858 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:49,860 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:50,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:51,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:51,226 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:51,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:52,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:52,529 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:52,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:53,335 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:53,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:53,823 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:54,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:56,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:56,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:57,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:57,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:58,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:58,216 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:58,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:58,722 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:59,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:59,267 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:59,268 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:01,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:03,841 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:04,747 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:04,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:19,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:20,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:22,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:23,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:25,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:26,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:28,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:29,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:30,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:30,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:31,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:32,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:34,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:34,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:35,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:37,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:37,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:38,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:38,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:38,411 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 136404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:38,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:38,713 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 136404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:39,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:39,023 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 136404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:39,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:39,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:40,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:40,869 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:40,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:42,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:42,786 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:43,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:44,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:44,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:44,867 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:45,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:45,222 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100464 input tokens (60000 > 129024 - 100464). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:45,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:45,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:45,479 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100464 input tokens (60000 > 129024 - 100464). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:45,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:45,700 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100464 input tokens (60000 > 129024 - 100464). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:46,023 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:47,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:48,968 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:51,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:51,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:54,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:59,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:01,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:03,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:05,556 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:06,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:06,009 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:06,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:06,484 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:06,600 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:06,723 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:06,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:06,963 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:07,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:07,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:07,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:07,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:07,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:07,736 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:08,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:08,114 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:10,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:11,603 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:11,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:12,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:12,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:12,704 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 357015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:13,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:13,347 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 357015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:13,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:14,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:14,058 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 357015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:14,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:17,260 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:17,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:17,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:18,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:18,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:18,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:18,745 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:19,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:19,325 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:19,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:19,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:19,929 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:21,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:21,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:21,772 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:21,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:22,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:23,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:23,683 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:24,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:25,510 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:25,511 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:25,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:26,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:26,310 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:26,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:26,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:26,669 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:26,670 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:27,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:27,065 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:28,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:30,379 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:30,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:33,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:33,898 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:33,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:36,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:36,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:37,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:38,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:39,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:39,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:40,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:41,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:41,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:41,539 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:42,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:42,041 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:42,268 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:42,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:42,573 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:44,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:44,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:44,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:45,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:45,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:45,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:46,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:46,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:46,519 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:47,140 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:47,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:47,631 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:47,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:47,942 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 141500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:48,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:48,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:48,249 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 141500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:48,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:48,528 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 141500 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:48,755 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:51,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:52,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:53,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:54,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:54,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:56,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:58,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:00,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:00,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:01,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:01,501 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:03,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:03,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:05,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:05,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:05,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:06,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:07,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:08,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:08,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:08,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:08,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:11,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:11,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:12,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:15,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:15,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:16,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:17,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:18,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:18,968 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:21,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:21,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:22,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:22,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:25,177 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:25,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:26,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:28,474 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:28,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:29,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:29,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:30,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:31,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:32,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:34,588 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:36,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:37,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:37,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:38,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:39,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:39,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:40,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:41,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:42,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:42,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:42,953 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:43,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:48,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:48,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:49,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:51,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:54,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:54,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:54,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:54,759 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:55,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:58,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:58,103 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089007 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:00,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:03,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:03,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:05,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:05,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:06,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:06,946 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089007 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:08,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:19,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:19,221 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735510 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:19,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:22,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:22,720 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089007 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:23,713 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:26,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:33,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:33,837 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735510 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:34,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:35,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:35,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:35,732 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 451272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:35,743 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:47,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:47,653 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:47,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:47,699 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735510 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:49,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:49,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:49,639 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 451272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:49,651 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:49,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:01,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:01,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:01,946 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:02,009 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:03,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:03,205 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 451272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:09,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:09,054 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:09,055 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:09,109 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:09,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:22,600 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:22,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:22,603 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1350242 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:22,628 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:30,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:30,493 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4400216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:32,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:36,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:43,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:43,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:43,349 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1350242 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:43,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:51,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:51,416 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4400216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:54,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:54,176 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1350242 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:55,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:01,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:07,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:07,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:07,819 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4400216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:07,865 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:11,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:11,044 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1398125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:17,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:17,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:17,233 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:17,279 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:23,928 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:29,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:29,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:29,445 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1398125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:29,475 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:36,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:36,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:36,016 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:36,063 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:38,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:38,617 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1398125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:40,173 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:50,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:50,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:50,041 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:50,068 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7735677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:56,282 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:56,283 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:56,284 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:56,318 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:07,239 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:07,241 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1187481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:07,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:07,265 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7202579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:09,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:11,639 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:12,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:21,754 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1187481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:21,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:21,778 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7202579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:24,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:28,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:28,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:28,154 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:28,189 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:38,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:38,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:38,951 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1187481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:38,976 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7202579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:40,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:45,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:45,658 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:45,660 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:45,693 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:58,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:58,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:58,247 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069811 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:58,270 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7867946 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:00,161 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:05,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:05,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:05,547 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:05,591 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:21,268 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:21,270 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069811 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:21,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:21,294 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7867946 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:26,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:26,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:33,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:33,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:33,008 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:33,054 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069811 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:36,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:45,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:45,318 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:45,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:45,351 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7867946 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:52,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:52,397 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:56,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:56,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:56,176 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:58,481 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:08,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:08,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:08,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:08,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6802602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:12,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:12,834 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:13,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:19,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:19,573 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:28,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:31,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:31,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:31,075 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1763294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:31,105 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6802602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:38,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:38,498 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:40,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:41,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:41,727 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1763294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:44,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:52,988 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:53,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:53,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6802602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:57,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:57,293 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1763294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:03,662 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:03,663 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:06,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:11,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:15,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:15,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:15,581 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:15,613 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:22,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:22,308 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:28,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:33,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:33,927 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:33,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:33,961 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:36,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:40,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:40,212 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:41,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:51,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:51,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:51,991 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:52,027 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:58,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:58,771 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:00,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:00,728 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:11,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:11,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:11,128 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:11,161 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:17,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:17,911 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:20,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:29,870 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:29,871 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:29,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:29,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:29,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097477 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:29,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:37,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:37,610 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955036 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:41,631 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:49,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:49,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:49,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:49,948 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2089066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:49,982 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097477 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:49,993 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:58,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:58,104 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955309 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:00,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:01,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:01,510 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2088957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:01,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:01,549 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097477 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:14,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:14,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:14,631 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955309 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:14,677 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:17,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:18,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:18,940 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2088957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:20,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:31,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:31,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:31,313 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955309 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:31,366 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:35,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:35,147 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2088957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:41,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:47,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:47,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:47,346 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:47,395 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:51,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:51,079 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2132165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:53,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:56,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:03,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:03,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:03,213 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:03,261 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7724166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:07,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:07,529 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2132165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:09,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:13,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:13,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:14,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:26,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:26,379 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:26,380 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2132165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:26,414 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7724166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:33,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:33,130 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955832 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:35,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:38,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:46,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:46,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:46,759 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2119280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:46,793 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7724166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:53,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:53,356 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955832 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:54,791 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:57,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:57,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2119280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:59,760 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:08,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:08,632 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955832 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:08,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:08,679 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:12,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:12,518 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2119280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:18,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:18,474 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:20,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:22,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:30,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:30,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:30,965 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:31,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:37,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:37,762 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:40,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:40,611 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:42,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:42,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:51,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:51,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:51,905 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:51,950 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:55,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:55,304 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:03,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:06,593 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:06,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:06,596 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:06,640 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7777021 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:08,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:10,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:10,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:13,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:22,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:22,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:22,250 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:22,294 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7777021 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:26,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:26,391 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:29,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:38,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:38,203 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:38,204 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:38,259 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7777021 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:39,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:43,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:43,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:47,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:57,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:57,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:57,610 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:57,665 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7697261 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:00,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:00,196 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:04,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:04,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:12,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:12,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:12,015 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:12,064 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7697261 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:15,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:15,339 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:21,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:21,724 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:24,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:31,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:34,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:34,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:34,032 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:34,070 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7697261 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:36,525 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:36,527 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1369274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:39,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:39,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:47,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:47,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:47,959 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:47,992 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7727142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:51,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:51,151 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1369274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:54,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:54,736 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:56,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:57,518 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:05,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:05,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:05,782 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1369274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:05,842 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7727142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:09,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:09,362 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:11,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:12,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:21,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:21,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:21,185 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:21,260 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7727142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:24,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:24,849 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:27,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:28,196 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:37,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:37,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:37,062 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:37,113 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7756098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:40,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:40,482 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:43,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:46,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:46,497 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:48,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:58,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:58,382 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:58,383 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:58,402 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7756098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:04,612 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:04,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:04,615 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955709 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:04,661 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1039965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:10,803 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:16,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:16,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:16,515 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 733842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:16,533 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7756098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:22,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:22,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:22,863 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955709 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:22,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1039965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:24,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:30,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:30,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:30,721 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 733842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:30,738 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4979358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:38,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:38,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:38,081 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955709 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:38,132 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1039965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:39,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:39,872 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 733842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:47,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:47,461 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4979358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:48,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:49,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:53,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:53,958 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:56,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:01,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:01,544 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4979358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:03,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:07,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:07,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:07,977 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:08,023 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:11,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:19,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:19,988 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:23,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:26,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:26,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:26,889 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:26,940 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:28,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:38,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:38,148 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:40,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:45,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:45,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:45,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:45,089 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:45,135 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955239 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:45,165 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:56,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:56,743 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:59,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:03,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:03,104 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:05,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:14,755 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:14,757 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:14,758 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955239 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:14,801 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:21,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:21,568 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:24,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:25,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:33,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:33,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:33,698 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955239 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:33,746 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:40,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:40,583 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3864844 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:46,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:46,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:46,883 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:50,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:58,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:58,180 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3864844 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:58,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:58,229 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754177 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:05,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:05,771 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:12,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:13,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:13,058 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3864844 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:15,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:24,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:24,051 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:24,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:24,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:24,084 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1018954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:24,093 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:30,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:30,657 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:32,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:33,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:33,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:33,948 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1528302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:33,975 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1018954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:45,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:45,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:45,796 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:45,849 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:48,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:48,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:48,765 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1528302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:48,791 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1018954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:48,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:00,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:00,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:00,539 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:00,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7754456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:04,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:04,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:04,015 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1528302 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:11,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:14,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:14,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:14,865 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3745396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:14,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7696256 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:21,510 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:21,512 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:27,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:27,369 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3745396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:28,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:31,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:38,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:38,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:38,704 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:38,749 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7696256 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:45,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:45,286 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3745396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:46,480 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:51,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:51,545 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955917 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:53,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:02,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:02,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:02,495 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3861610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:02,541 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7696256 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:03,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:09,546 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:11,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:20,586 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:20,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:20,588 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3861610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:20,636 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7033546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:27,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:27,248 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:28,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:33,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:33,223 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3861610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:35,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:44,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:44,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:44,086 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:44,087 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7033546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:45,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:49,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:49,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3341020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:52,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:00,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:00,322 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7033546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:01,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:05,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:05,646 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3341020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:05,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:09,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:17,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:17,482 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7723585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:18,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:22,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:22,831 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3341020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:24,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:34,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:34,457 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7723585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:39,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:41,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:41,223 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:42,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:54,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:54,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:54,304 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7723585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:54,391 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:03,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:03,443 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:08,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:12,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:12,977 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:14,160 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:26,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:26,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:26,878 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:26,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:35,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:35,486 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:41,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:41,409 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3679015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:42,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:46,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:53,409 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:53,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:53,412 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:53,496 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1504544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:59,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:59,297 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3679015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:00,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:10,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:10,530 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:10,531 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7736646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:10,624 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1504544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:16,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:17,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:17,080 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3679015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:18,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:28,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:28,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:28,356 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:28,449 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1504544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:34,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:34,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:41,188 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:46,908 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:46,909 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:48,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:53,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:53,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:53,798 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:53,847 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:07,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:07,333 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7734234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:11,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:14,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:14,022 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:15,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:25,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:25,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:25,901 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:25,905 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:29,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:31,700 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:31,701 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3556953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:32,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:43,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:43,444 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:44,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:44,191 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:47,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:48,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:55,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:55,500 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7755041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:56,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:01,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:01,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:01,639 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3739852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:01,681 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:03,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:06,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:07,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:07,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:07,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:07,964 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3739852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:08,009 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:09,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:11,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:12,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:14,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:14,206 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3739852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:16,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:17,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:19,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:19,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:21,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:21,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:23,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:25,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:25,927 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:31,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:31,753 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:32,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:35,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:36,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:39,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:39,419 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:44,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:44,556 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:46,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:48,214 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:51,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:51,964 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:57,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:57,247 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3557103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:58,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:03,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:03,597 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:07,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:07,468 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2401420 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:07,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:08,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:15,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:15,715 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:19,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:19,404 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2401420 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:22,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:23,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:25,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:25,649 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:26,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:29,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:29,877 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2401420 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:31,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:36,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:36,104 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3866692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:38,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:39,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:39,919 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2454546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:40,871 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:42,196 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:45,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:45,839 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3866692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:47,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:49,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:49,468 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2454546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:50,255 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:55,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:55,525 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3866692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:57,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:58,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:59,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:59,003 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2454546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:01,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:04,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:04,743 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:06,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:07,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:10,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:10,364 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3601402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:12,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:17,102 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:17,103 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:21,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:22,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:22,561 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3601402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:25,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:28,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:28,603 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:30,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:34,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:34,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:34,454 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:34,476 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3601402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:40,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:40,147 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2206747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:41,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:42,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:42,230 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:42,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:45,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:45,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2206747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:47,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:47,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:47,704 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:47,727 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:47,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:53,520 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:53,522 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2206747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:53,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:55,588 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:55,590 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:59,308 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:02,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:02,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:02,647 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3847834 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:02,700 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:04,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:04,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:06,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:06,416 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:06,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:07,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:12,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:12,441 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3847834 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:12,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:12,497 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1096999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:15,326 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:15,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:16,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:16,230 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:17,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:21,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:21,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:21,776 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3847834 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:21,807 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1050700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:23,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:23,593 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:24,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:26,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:26,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:26,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:26,261 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1823560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:26,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:26,287 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1050700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:28,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:28,479 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:29,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:30,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:30,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:30,499 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:30,521 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1050700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:34,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:34,378 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1823560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:36,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:36,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:36,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1066653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:36,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:39,169 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:39,533 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:39,535 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1823560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:41,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:42,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:44,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:44,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:44,743 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3650749 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:46,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:47,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:47,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:50,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:50,020 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3650749 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:53,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:54,005 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:54,612 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:55,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:55,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:55,135 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3650749 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:55,184 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1061871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:56,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:56,968 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1061871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:57,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:57,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:59,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:03,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:03,884 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:05,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:05,795 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1061871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:12,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:12,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:12,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:12,989 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:17,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:18,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:19,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:19,126 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:19,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:19,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:19,724 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 351034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:21,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:21,616 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:26,063 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:26,065 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:26,758 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:26,760 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 351034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:28,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:29,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:32,709 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:32,710 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:33,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:33,450 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 351034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:34,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:35,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:36,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:40,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:40,539 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:42,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:42,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:42,978 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:43,001 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:45,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:47,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:50,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:50,547 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:52,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:52,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:52,446 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:52,472 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:58,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:58,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:58,934 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:58,983 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150115 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:01,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:02,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:02,965 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:03,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:08,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:08,816 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3867154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:13,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:14,160 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:14,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:16,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:16,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:16,287 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844768 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:23,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:23,213 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844768 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:30,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:30,030 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3844768 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:31,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:34,571 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:35,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:35,102 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2566015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:35,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:35,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:38,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:39,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:39,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:39,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:39,950 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2566015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:43,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:43,974 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2566015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:48,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:50,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:50,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:53,748 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:54,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:55,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:56,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:56,110 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:56,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:56,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:58,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:58,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:58,769 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:58,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:01,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:01,858 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:03,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:03,234 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 724236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:04,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:04,281 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 724236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:05,378 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:05,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 724236 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:08,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:08,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:10,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:12,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:13,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:14,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:16,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:16,615 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:16,616 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1578407 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:16,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:19,042 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:19,043 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1578407 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:20,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:21,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:21,224 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1578407 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:27,746 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:36,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:40,800 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:41,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:41,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 417544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:41,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:42,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:42,035 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 417544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:42,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:42,645 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 417544 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:42,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:43,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:43,807 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 597804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:45,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:45,002 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 597804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:45,216 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:45,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:45,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:46,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:46,480 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 597804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:47,520 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:47,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:47,916 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 749341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:48,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:49,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:49,303 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 749341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:50,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:50,679 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 749341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:50,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:51,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:52,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:55,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:55,569 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:59,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:59,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:59,813 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:01,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:03,887 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:04,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:04,136 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:04,137 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:04,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:04,454 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:06,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:11,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:11,952 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5085341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:12,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:12,219 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:14,546 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:19,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:19,852 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5085341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:20,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:20,256 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:22,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:24,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:28,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:28,474 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5085341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:30,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:30,856 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:32,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:34,847 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:36,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:36,202 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5290937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:39,651 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:39,653 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:40,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:41,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:42,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:47,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:47,405 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5290937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:51,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:51,005 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:54,157 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:54,292 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:54,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:59,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:59,324 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5290937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:02,603 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:02,605 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:04,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:10,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:10,703 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5402042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:11,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:11,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:12,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:12,477 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1117231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:14,260 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:21,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:21,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5402042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:24,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:24,058 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1117231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:25,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:29,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:32,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:32,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:32,059 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5402042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:32,060 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:34,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:34,067 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1117231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:35,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:36,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:37,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:37,523 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:39,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:39,025 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 961937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:39,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:41,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:42,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:42,875 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:44,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:44,267 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 961937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:45,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:45,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:47,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:47,434 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2022046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:48,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:48,798 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 961937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:49,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:49,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:52,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:52,057 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2022046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:54,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:54,990 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2022046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:58,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:59,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:59,637 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2060084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:01,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:02,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:02,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:02,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:02,932 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2060084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:04,188 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:06,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:06,029 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2060084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:07,096 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:07,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:08,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:08,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:08,928 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2044428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:10,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:11,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:11,765 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2044428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:11,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:12,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:13,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:13,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:15,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:15,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2044428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:17,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:19,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:20,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:20,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:20,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:20,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:20,717 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:21,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:21,018 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:22,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:23,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:26,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:26,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:27,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:27,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:28,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:29,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:30,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:30,927 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:31,981 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:31,982 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:31,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:31,996 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 780918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:33,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:35,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:35,396 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:36,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:36,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:36,753 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:36,765 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 780918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:40,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:40,119 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:41,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:41,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:41,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:41,419 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:41,423 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 780918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:45,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:45,112 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2006044 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:46,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:48,413 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:48,414 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2006044 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:48,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:50,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:50,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:51,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:51,251 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2006044 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:53,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:54,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:54,853 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:56,398 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:56,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:56,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:57,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:57,981 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:02,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:02,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:02,153 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:02,160 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178547 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:02,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:02,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:05,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:05,942 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2127607 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:06,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:06,359 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:09,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:09,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:09,289 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2127607 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:09,641 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:09,642 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:10,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:11,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:12,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:12,968 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2127607 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:13,292 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:13,293 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:13,723 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:01:16,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:17,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:17,021 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1991040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:17,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:17,486 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:17,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:20,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:20,273 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1991040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,164 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:22,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:23,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:25,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:26,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:26,173 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1991040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:27,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:27,890 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:28,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:31,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:31,309 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2031676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:33,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:33,106 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:33,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:34,384 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:36,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:36,797 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2031676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:37,112 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:37,113 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189777 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:39,898 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:40,157 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:40,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:40,668 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2031676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:40,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:40,988 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189777 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:42,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:44,293 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:44,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019557 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:44,729 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:44,731 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189777 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:47,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:47,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:48,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:48,187 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019557 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:49,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:49,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:51,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:53,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:53,556 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2019557 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:55,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:55,429 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:56,673 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:57,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:00,173 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:00,174 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:01,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:01,889 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:04,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:05,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:05,307 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:06,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:06,720 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:07,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:09,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:11,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:11,539 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2055283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:12,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:12,816 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:13,726 [WARNING] gen_code timeout (attempt 2)
2025-12-10 18:02:13,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:16,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:16,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1987651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:18,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:18,384 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:18,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:22,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:22,436 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1987651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:24,192 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:27,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:27,220 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1987651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:27,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:27,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:30,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:30,340 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2084772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:30,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:31,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:31,853 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:32,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:33,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:33,579 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2084772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:33,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:33,892 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:35,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:37,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:37,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:37,980 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2084772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:38,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:38,300 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:38,488 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:39,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:41,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:41,668 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2030941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:42,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:42,075 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:43,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:45,787 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:45,789 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2030941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:48,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:50,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:50,549 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2030941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:50,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:52,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:53,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:53,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:53,846 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:55,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:56,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:57,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:57,203 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:59,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:00,813 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:00,814 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:01,129 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:03,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:03,921 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2149997 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:07,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:07,062 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2149997 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:10,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:10,386 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2149997 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:12,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:16,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:19,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:22,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:23,126 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:23,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:27,257 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:27,517 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:27,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:27,640 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:27,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:28,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:28,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:28,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:28,404 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:30,816 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:32,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:32,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:32,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:32,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:32,906 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185545 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:33,321 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:33,322 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185545 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:33,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:33,750 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185545 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:34,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:35,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:38,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:38,493 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:03:39,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:40,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:40,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:41,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:41,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:41,362 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:42,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:42,146 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:42,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:43,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:43,112 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:43,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:44,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:44,034 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:44,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:44,946 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:45,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:45,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:45,350 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:45,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:45,691 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:45,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:46,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:46,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:47,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:50,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:51,447 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:51,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:55,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:56,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:58,551 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:59,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:01,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:01,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:03,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:04,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:05,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:08,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:09,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:09,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:10,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:10,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:12,719 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:14,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:15,624 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:16,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:16,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:22,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:22,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:22,870 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72917 input tokens (60000 > 129024 - 72917). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:23,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:23,031 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72917 input tokens (60000 > 129024 - 72917). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:23,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:23,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:23,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72917 input tokens (60000 > 129024 - 72917). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:33,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:33,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:33,312 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107080 input tokens (60000 > 129024 - 107080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:33,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:33,628 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107080 input tokens (60000 > 129024 - 107080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:33,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:33,928 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107080 input tokens (60000 > 129024 - 107080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:33,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:34,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:34,206 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106194 input tokens (60000 > 129024 - 106194). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:34,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:34,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:34,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106194 input tokens (60000 > 129024 - 106194). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:34,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:34,699 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106194 input tokens (60000 > 129024 - 106194). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:34,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:34,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:34,992 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114675 input tokens (60000 > 129024 - 114675). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:35,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:35,249 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114675 input tokens (60000 > 129024 - 114675). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:35,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:35,566 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114675 input tokens (60000 > 129024 - 114675). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:35,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:35,878 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:36,195 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:36,197 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:36,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:36,497 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:36,934 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:36,935 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:37,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:37,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:37,384 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:37,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:37,753 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:37,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:38,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:38,051 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 163812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:38,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:38,298 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 163812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:38,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:38,579 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 163812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:38,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:38,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78503 input tokens (60000 > 129024 - 78503). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:39,137 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:39,139 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78503 input tokens (60000 > 129024 - 78503). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:39,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:39,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78503 input tokens (60000 > 129024 - 78503). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:39,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:39,806 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:40,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:40,248 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:40,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:40,696 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:41,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:41,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:45,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:45,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:46,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:46,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:47,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:47,537 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1016516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:49,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:49,079 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1016516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:50,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:50,763 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1016516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:51,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:51,743 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:04:52,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:52,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:52,632 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:52,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:54,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:54,173 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:55,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:55,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:55,677 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:56,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:57,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:57,501 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:57,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:59,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:59,602 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:01,496 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:01,498 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:03,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:03,276 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:05,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:05,111 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:05,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:05,416 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:06,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:06,850 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:08,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:08,636 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1786817 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:09,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:10,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:10,699 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1786817 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:11,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:11,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:12,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:12,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:12,591 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1786817 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:14,524 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:14,526 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:14,531 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:15,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:16,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:16,439 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:17,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:17,100 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:17,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:17,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:18,960 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:18,961 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1645899 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:19,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:19,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:19,876 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:22,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:22,351 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1675586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:23,042 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:23,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 467965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:24,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:24,936 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1675586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:25,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:25,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:25,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:25,807 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:26,047 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:28,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:28,126 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1675586 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:28,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:28,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:28,964 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 396539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:28,976 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:30,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:31,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:31,035 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1907675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:32,002 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:32,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:32,004 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 396539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:32,015 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 572980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:33,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:34,000 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:34,001 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1907675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:35,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:35,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:35,306 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 396539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:35,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:37,142 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:37,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:37,408 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1907675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:38,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:38,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:38,739 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80422 input tokens (60000 > 129024 - 80422). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:38,744 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:40,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:41,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:41,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1703055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:42,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:42,408 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:42,409 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80422 input tokens (60000 > 129024 - 80422). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:42,415 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 800216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:44,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:44,375 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1703055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:46,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:47,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:47,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:47,230 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80422 input tokens (60000 > 129024 - 80422). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:47,234 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:47,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:49,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:49,837 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1703055 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:52,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:52,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:52,619 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:54,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:54,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:54,974 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1986895 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:57,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:58,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:58,234 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:00,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:00,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:00,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 671529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:00,581 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1986895 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:00,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:02,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:04,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:04,253 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:06,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:06,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:06,810 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 671529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:06,827 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1986895 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:09,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:09,897 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:11,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:12,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:12,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:12,664 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 671529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:12,667 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2020812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:16,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:16,033 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:18,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:18,290 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2020812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:19,791 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:21,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:22,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:22,213 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:24,344 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:24,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2020812 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:27,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:28,226 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:28,227 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:30,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:30,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:30,636 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2165244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:33,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:34,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:34,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:36,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:36,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:36,584 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2165244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:40,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:41,132 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:41,133 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3218848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:43,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:43,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:43,771 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2165244 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:44,057 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:46,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:48,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:48,291 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3218848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:50,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:50,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:50,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2034746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:53,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:55,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:55,864 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3218848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:57,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:58,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:58,402 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2034746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:59,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:03,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:03,831 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3766543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:06,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:06,349 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2034746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:06,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:08,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:12,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:12,542 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3766543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:14,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:15,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:15,716 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2045583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:17,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:21,117 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:21,118 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3766543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:24,035 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:24,036 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2045583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:26,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:29,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:29,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3791578 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:30,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:31,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:32,230 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:32,232 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2045583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:33,531 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:37,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:37,861 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 578206 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:37,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:37,877 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3791578 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:40,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:40,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1717652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:42,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:42,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 578206 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:44,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:44,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:47,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:47,286 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3791578 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:47,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:47,337 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1717652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:48,410 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:48,411 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 578206 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:50,429 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:51,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:54,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:54,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:54,010 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:54,055 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1717652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:56,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:57,767 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:00,350 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:00,351 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:03,488 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:05,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:05,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:07,517 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:07,518 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:07,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:07,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:07,986 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 295514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:11,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:11,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:13,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:14,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:14,623 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:15,101 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:15,103 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 295514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:17,861 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:19,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:20,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:22,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:22,214 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:22,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:22,707 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 295514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:27,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:28,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:28,369 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:31,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:34,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:34,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3883474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:40,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:40,791 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:43,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:44,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:47,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:48,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:48,253 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:52,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:52,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:52,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:55,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:55,195 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4097660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:58,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:59,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:01,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:01,791 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4097660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:04,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:06,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:07,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:08,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:09,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:09,957 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4097660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:11,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:12,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:12,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:12,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:15,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:15,297 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2913780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:18,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:20,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:21,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:21,695 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2913780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:22,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:26,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:28,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:28,254 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2913780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:28,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:30,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:32,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:33,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:35,803 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:35,804 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:36,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:36,078 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 146522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:38,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:43,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:43,340 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:43,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:43,773 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 146522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:48,655 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:50,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:52,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:52,226 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:52,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:52,493 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 146522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:55,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:59,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:59,074 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3993034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:02,603 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:03,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:05,804 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:06,868 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:06,869 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3993034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:08,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:10,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:10,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:13,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:13,867 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3993034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:16,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:18,593 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:19,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:19,493 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3551519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:20,131 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:21,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:21,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:23,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:28,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:28,027 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3551519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:34,212 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:34,212 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3551519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:34,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:37,612 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:37,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:43,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:43,676 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4343141 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:44,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:44,265 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:50,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:50,969 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4343141 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:52,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:52,374 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:55,153 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:55,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:59,543 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:59,544 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4343141 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:00,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:00,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:03,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:05,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:05,249 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2901537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:05,297 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:05,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:05,486 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 159752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:09,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:10,452 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:10,454 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2901537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:10,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:10,882 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 159752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:14,519 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:15,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:17,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:17,455 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2901537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:17,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:17,771 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 159752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:20,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:21,738 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:11:24,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:24,370 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4019025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:24,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:24,701 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100416 input tokens (60000 > 129024 - 100416). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:25,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:31,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:34,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:34,015 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4019025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:34,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:34,207 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100416 input tokens (60000 > 129024 - 100416). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:37,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:40,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:40,303 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4019025 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:40,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:40,634 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100416 input tokens (60000 > 129024 - 100416). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:45,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:45,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:49,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:49,399 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3896529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:52,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:53,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:53,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:56,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:56,669 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3896529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:01,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:02,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:02,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:02,881 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3896529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:03,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:05,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:06,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:10,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:10,199 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4118375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:15,487 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:17,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:17,021 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4118375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:19,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:21,743 [WARNING] gen_code timeout (attempt 2)
2025-12-10 18:12:22,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:22,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:23,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:23,646 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4118375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:25,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:27,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:28,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:29,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:29,355 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3317133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:31,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:32,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:32,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:34,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:34,576 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3317133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:38,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:38,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:39,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:39,463 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3317133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:40,910 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:42,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:43,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:45,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:45,888 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771446 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:49,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:50,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:52,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:52,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:52,744 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771446 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:56,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:56,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:56,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:58,576 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:58,577 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771446 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:02,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:02,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:03,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:04,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:04,957 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3699313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:06,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:07,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:08,035 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:10,647 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:10,648 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3699313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:11,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:11,923 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689528 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:14,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:18,242 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:18,243 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3699313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:19,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:19,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689528 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:21,746 [WARNING] gen_code timeout (attempt 3)
2025-12-10 18:13:22,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:22,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:27,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:27,382 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4017094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:28,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:28,612 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689528 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:31,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:32,217 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:33,259 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:36,116 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:36,117 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4017094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:37,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:40,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:41,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:42,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:44,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:44,486 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4017094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:45,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:48,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:49,715 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:52,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:53,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:53,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3875060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:54,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:57,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:03,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:03,042 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3875060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:04,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:08,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:09,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:10,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:10,999 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3875060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:13,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:13,829 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:15,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:16,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2822397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:17,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:19,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:19,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:20,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:20,925 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2822397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:22,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:25,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:25,591 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2822397 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:26,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:27,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:31,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:31,940 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3936085 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:34,384 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:35,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:37,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:39,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:39,319 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3936085 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:42,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:43,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:43,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:46,191 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3936085 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:48,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,720 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:14:51,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:54,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:54,682 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:55,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:57,149 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:57,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,434 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:02,435 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:03,761 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:06,484 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:09,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:09,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:09,982 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3999464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:11,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:12,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:16,182 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:16,184 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3890660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:17,530 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:19,039 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,600 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:23,601 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3890660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:24,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:25,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:25,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:31,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:31,858 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3890660 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:34,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:39,529 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386286 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:41,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:44,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:45,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:47,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:47,238 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386286 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:49,323 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:50,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:54,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:55,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:55,135 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386286 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:56,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:57,766 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:15:59,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:02,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:02,437 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:02,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:03,964 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:07,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:10,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:10,349 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:11,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:13,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:14,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:17,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:17,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4446156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:19,148 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:20,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:22,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:22,967 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3123568 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:25,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:25,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:26,725 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:29,322 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:29,324 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3123568 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:30,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,088 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:35,156 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3123568 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:35,376 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,180 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,916 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:41,917 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3809041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:43,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,977 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:46,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:47,906 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3809041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:49,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:50,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:51,793 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:53,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:53,734 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3809041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:56,233 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:57,771 [WARNING] gen_code timeout (attempt 2)
2025-12-10 18:16:58,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:58,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:00,958 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:00,960 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4443823 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:03,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:08,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:08,827 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4443823 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:10,211 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:13,391 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:14,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:16,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:16,245 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4443823 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:18,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:20,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:20,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:22,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:25,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:25,732 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:27,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:27,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:29,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:31,280 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:31,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:34,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:35,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:38,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:38,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:40,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:40,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:41,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:42,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:43,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:45,071 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:48,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:48,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:50,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:52,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:53,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:54,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:57,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:57,773 [WARNING] gen_code timeout (attempt 3)
2025-12-10 18:17:59,050 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:59,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:01,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:01,612 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:01,613 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111465 input tokens (60000 > 129024 - 111465). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:01,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:01,836 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111465 input tokens (60000 > 129024 - 111465). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:01,887 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:02,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:02,060 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 111465 input tokens (60000 > 129024 - 111465). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:02,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:02,249 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:02,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:02,462 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:02,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:02,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:02,672 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:03,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:06,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:06,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:07,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:07,194 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100341 input tokens (60000 > 129024 - 100341). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:07,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:07,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:07,446 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100341 input tokens (60000 > 129024 - 100341). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:07,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:07,636 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100341 input tokens (60000 > 129024 - 100341). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:07,866 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:07,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101337 input tokens (60000 > 129024 - 101337). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:08,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:08,090 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101337 input tokens (60000 > 129024 - 101337). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:08,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:08,313 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101337 input tokens (60000 > 129024 - 101337). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:08,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:08,513 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92942 input tokens (60000 > 129024 - 92942). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:08,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:08,708 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92942 input tokens (60000 > 129024 - 92942). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:08,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:08,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92942 input tokens (60000 > 129024 - 92942). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:09,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:09,123 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106525 input tokens (60000 > 129024 - 106525). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:09,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:09,346 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106525 input tokens (60000 > 129024 - 106525). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:09,354 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:09,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:09,587 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106525 input tokens (60000 > 129024 - 106525). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:09,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:09,814 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100388 input tokens (60000 > 129024 - 100388). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:09,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:10,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:10,013 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100388 input tokens (60000 > 129024 - 100388). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:10,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:10,211 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100388 input tokens (60000 > 129024 - 100388). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:18,467 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:20,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,035 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:24,236 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107690 input tokens (60000 > 129024 - 107690). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:24,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,454 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:24,464 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107690 input tokens (60000 > 129024 - 107690). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:24,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:24,747 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107690 input tokens (60000 > 129024 - 107690). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:24,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:24,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:24,935 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 96367 input tokens (60000 > 129024 - 96367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:25,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:25,111 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 96367 input tokens (60000 > 129024 - 96367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:25,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:25,288 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 96367 input tokens (60000 > 129024 - 96367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:25,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:25,486 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106585 input tokens (60000 > 129024 - 106585). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:25,689 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:25,691 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106585 input tokens (60000 > 129024 - 106585). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:25,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:25,898 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106585 input tokens (60000 > 129024 - 106585). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:26,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:26,086 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90898 input tokens (60000 > 129024 - 90898). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:26,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:26,266 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90898 input tokens (60000 > 129024 - 90898). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:26,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:26,446 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90898 input tokens (60000 > 129024 - 90898). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:26,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:26,650 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100389 input tokens (60000 > 129024 - 100389). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:26,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:26,885 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:26,886 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100389 input tokens (60000 > 129024 - 100389). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:27,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:27,063 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100389 input tokens (60000 > 129024 - 100389). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:27,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:28,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:29,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:30,903 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:31,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:31,170 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:31,171 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:31,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:31,371 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:31,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:31,586 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:31,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:31,798 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:31,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:32,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:32,010 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:32,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:32,203 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 133721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:34,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:52,272 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:53,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:55,837 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:57,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:01,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:01,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:01,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:01,607 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100286 input tokens (60000 > 129024 - 100286). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:01,790 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:01,792 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100286 input tokens (60000 > 129024 - 100286). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:01,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:02,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:02,005 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100286 input tokens (60000 > 129024 - 100286). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:03,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:06,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:06,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:06,181 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:09,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:09,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:10,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:11,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:13,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:15,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:16,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:16,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:17,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:19,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:21,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:22,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:23,312 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:25,525 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:26,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:27,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:30,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:31,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:31,728 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:32,868 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:34,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:36,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:36,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:39,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:39,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:41,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:42,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:44,366 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:44,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:45,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:45,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:49,593 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:49,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:51,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:52,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:53,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:56,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:58,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:58,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:00,872 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:01,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:03,440 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:06,092 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:20:06,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:07,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:07,244 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:09,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:10,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:11,400 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:14,826 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:15,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:19,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:20,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:23,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:23,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:27,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:29,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:30,807 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:34,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:35,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:37,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:37,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:38,807 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:40,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:41,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:41,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:44,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:45,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:46,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:48,575 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:49,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:52,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:52,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:53,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:56,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:56,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:56,777 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:57,138 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:59,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:00,731 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:02,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:02,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:05,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:06,097 [WARNING] gen_code timeout (attempt 2)
2025-12-10 18:21:06,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:07,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:10,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:11,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:12,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:13,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:18,551 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:21,006 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:23,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:25,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:26,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:27,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:28,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:32,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:32,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:33,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:36,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:36,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:37,981 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:39,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:42,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:42,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:45,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:47,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:47,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:48,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:49,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:49,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:52,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:53,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:57,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:58,370 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:59,671 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:01,857 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:03,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:04,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:04,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:06,101 [WARNING] gen_code timeout (attempt 3)
2025-12-10 18:22:08,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:09,357 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:11,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:16,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:17,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:19,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:21,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:21,442 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:23,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:25,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:26,314 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:27,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:27,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:30,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:31,045 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:31,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:32,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:34,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:35,527 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:37,016 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:38,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:39,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:40,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:45,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:47,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:47,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:51,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:51,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:52,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:52,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:55,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:55,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:56,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:56,993 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:00,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:01,950 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:03,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:03,501 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:04,586 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:23:05,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:06,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:07,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:09,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:10,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:10,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:11,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:11,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:13,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:13,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:14,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:15,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:16,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:17,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:19,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:22,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:23,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:24,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:25,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:34,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:37,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:38,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:52,774 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:54,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:58,218 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:00,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:01,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:05,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:08,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:09,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:11,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:11,622 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:12,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:14,506 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:24:17,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:18,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:19,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:19,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:19,780 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 766014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:20,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:21,281 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:21,283 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 766014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:21,667 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:21,832 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:22,763 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:22,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:22,774 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 766014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:23,905 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:24,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:24,480 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:24,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:26,103 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:26,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:26,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:26,913 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:27,676 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:27,677 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:28,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:29,984 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:30,398 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:30,399 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1441626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:31,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:32,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:32,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:32,825 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1441626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:33,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:33,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:35,356 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:35,357 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1441626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:35,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:37,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:37,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1472862 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:40,144 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:40,145 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1472862 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:42,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:42,773 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1472862 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:45,836 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:45,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:47,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:48,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:48,816 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:50,833 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:51,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:51,140 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1582752 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:52,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:52,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:53,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:53,439 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1647393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:53,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:54,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:55,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:56,068 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:56,069 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1647393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:56,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:56,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78415 input tokens (60000 > 129024 - 78415). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:58,982 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:58,984 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1647393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:59,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:59,238 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78415 input tokens (60000 > 129024 - 78415). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:00,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:01,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:01,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:01,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:01,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:01,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1666858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:01,787 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78415 input tokens (60000 > 129024 - 78415). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:03,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:03,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:04,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:04,840 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1666858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:07,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:07,335 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1666858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:07,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:08,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:08,688 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:09,355 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:09,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:11,597 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:11,598 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:14,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:14,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:15,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:16,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:16,326 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:18,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:20,912 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:20,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2758592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:23,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:25,948 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:25,950 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:27,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:30,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:30,696 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2758592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:32,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:34,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:35,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:35,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:37,059 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:39,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:39,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2758592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:41,869 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:44,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:44,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:46,399 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:49,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:49,393 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2773708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:50,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:53,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:53,967 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2871451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:55,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:58,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:58,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2773708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:01,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:02,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:02,353 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2948675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:04,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:07,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:07,083 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2773708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:09,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:11,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:11,925 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2948675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:15,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:15,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:16,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:16,549 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:18,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:21,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:21,285 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2948675 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:23,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:25,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:25,826 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:28,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:30,688 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:30,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978835 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:33,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:35,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:35,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:35,277 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128645 input tokens (60000 > 129024 - 128645). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:35,285 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:39,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:39,667 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978835 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:40,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:40,029 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128645 input tokens (60000 > 129024 - 128645). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:42,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:43,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:45,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:45,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:45,297 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978835 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:45,348 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3347006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:46,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:46,041 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128645 input tokens (60000 > 129024 - 128645). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:48,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:50,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:50,832 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2979037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:51,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:55,360 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:55,362 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3347006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:59,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:59,738 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2979037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:02,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:05,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:05,027 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3347006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:05,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:06,125 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:09,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:09,620 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2979037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:11,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:15,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:15,960 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4189137 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:16,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:17,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:20,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:23,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:23,682 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4189137 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:24,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:24,867 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:24,969 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:26,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:29,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:29,924 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4189137 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:31,840 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:31,841 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077991 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:41,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:41,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:41,083 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077991 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:41,110 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6044965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:43,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:43,877 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077991 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:44,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:45,456 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:49,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:52,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:52,705 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6044965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:54,714 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:54,715 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078252 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:56,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:57,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:59,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:03,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:03,972 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6044965 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:05,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:05,835 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078252 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:08,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:09,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:11,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:20,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:20,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:20,537 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078252 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:20,563 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 10153046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:22,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:22,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:22,825 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:23,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:24,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:36,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:36,807 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 10153046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:37,424 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:38,818 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:38,820 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:40,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:55,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:55,495 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 10153046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:57,374 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:57,376 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829260 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:57,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:58,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:59,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:16,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:16,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11553048 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:18,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:18,209 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:19,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:20,253 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:21,449 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:37,865 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:37,866 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11553048 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:40,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:40,146 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:41,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:42,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:57,389 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:57,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11553048 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:59,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:59,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078014 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:00,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:00,838 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:16,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:16,851 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11995076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:18,276 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:19,194 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:19,196 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:20,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:21,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:36,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:36,339 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11995076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:37,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:38,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:38,562 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:39,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:56,325 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:56,326 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11995076 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:58,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:58,686 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:07,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:10,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:11,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:17,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:17,676 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11994952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:20,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:20,085 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:21,041 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:22,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:22,721 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:37,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:37,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:37,291 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:37,312 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11994952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:39,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:39,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:40,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:40,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:41,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:57,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:57,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 11994952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:58,420 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:58,421 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:58,435 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:59,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:59,438 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:59,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:59,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:02,308 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:02,310 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:02,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:02,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:03,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:03,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:03,603 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:03,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:04,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:05,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:05,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:05,692 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:06,197 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:06,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:06,276 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:07,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:07,873 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:07,874 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:08,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:08,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:09,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:10,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:10,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:10,610 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:10,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:10,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:10,998 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:11,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:12,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:12,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:12,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:12,337 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:12,357 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:13,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:14,061 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:14,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:14,431 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:15,025 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:15,027 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:15,558 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:15,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:15,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:16,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:16,557 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:16,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:17,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:17,063 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:18,049 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:18,050 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:18,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:18,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:18,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:19,468 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:19,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:19,575 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:20,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:20,100 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300223 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:20,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:21,581 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:22,118 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:22,119 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:22,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:22,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300223 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:24,044 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:24,045 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:24,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:24,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:24,783 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300223 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:24,803 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:25,573 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:26,199 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:26,200 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:26,202 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:26,223 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 551210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:27,628 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:27,630 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:28,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:28,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 551210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:29,513 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:29,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:30,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:30,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:30,016 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:30,036 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:31,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:31,114 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 551210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:32,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:32,876 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:33,100 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:34,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:34,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:34,817 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:35,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:35,568 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:37,026 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:37,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:37,545 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:38,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:38,255 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:38,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:38,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:40,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:40,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:40,437 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:41,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:41,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:41,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:41,205 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:43,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:43,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:44,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:44,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:44,125 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:44,135 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:44,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:45,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:45,144 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533127 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:45,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:45,892 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:45,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:45,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:46,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:46,906 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533127 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:47,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:47,363 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:48,669 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:48,671 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533127 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:50,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:50,057 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:51,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:51,921 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:52,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:52,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:52,889 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:52,891 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:54,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:54,754 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:54,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:54,774 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1017159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:55,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:55,898 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:57,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:57,750 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:57,764 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:57,765 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1017159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:58,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:58,351 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:59,521 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:59,707 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:59,708 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 495004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:59,721 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:59,722 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1017159 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:00,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:00,373 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:01,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:01,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:01,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:01,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 495004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:01,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:02,635 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:02,637 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:02,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:04,069 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:04,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:04,072 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 495004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:04,092 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:05,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:05,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:05,987 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:05,988 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928504 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:06,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:06,009 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:06,549 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:06,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:08,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:08,261 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:08,262 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:08,263 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928504 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:08,284 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:08,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:08,925 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:10,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:10,851 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:10,852 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928504 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:10,874 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:11,656 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:11,658 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:11,692 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:13,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:13,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:13,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:13,539 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089136 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:13,559 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 413064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:15,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:15,956 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:15,958 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089136 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:17,021 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:17,265 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:18,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:18,090 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089136 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:18,800 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:18,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:19,483 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:19,485 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:20,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:21,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:21,418 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:22,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:22,879 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:24,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:24,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:24,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:24,543 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:25,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:26,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:26,049 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:28,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:28,109 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077986 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:29,152 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:30,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:30,136 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1139375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:31,548 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:31,549 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:31,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:31,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:33,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:33,920 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1139375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:34,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:35,295 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:35,296 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:37,335 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:37,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:37,554 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1139375 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:39,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:39,056 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:40,334 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:41,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:42,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:49,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:50,093 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:51,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:51,685 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:52,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:55,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:57,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:59,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:59,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:59,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:00,004 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:11,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:12,773 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:14,095 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:14,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:15,160 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:16,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:16,110 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:18,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:18,248 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:18,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:18,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:19,081 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:19,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:19,935 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:21,892 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:25,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:25,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:26,471 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:27,198 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:27,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:27,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:28,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:28,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:29,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:29,048 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309505 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:29,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:29,286 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79072 input tokens (60000 > 129024 - 79072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:29,491 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:29,493 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79072 input tokens (60000 > 129024 - 79072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:29,563 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:29,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:29,712 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79072 input tokens (60000 > 129024 - 79072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:38,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:38,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:39,107 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:39,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:41,646 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:44,382 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:44,558 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:44,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81190 input tokens (60000 > 129024 - 81190). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:44,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:44,735 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81190 input tokens (60000 > 129024 - 81190). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:44,909 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:44,910 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81190 input tokens (60000 > 129024 - 81190). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:44,974 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:46,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:48,227 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:48,995 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:51,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:51,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:51,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:51,925 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378735 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:52,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:52,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:52,611 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378735 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:53,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:53,302 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378735 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:53,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:53,806 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:55,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:55,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:55,349 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:57,477 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:57,479 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:57,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:58,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:59,648 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:59,649 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1028489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:00,585 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:00,586 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 525859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:01,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:01,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:01,576 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 525859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:01,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:01,725 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 73888 input tokens (60000 > 129024 - 73888). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:02,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:02,453 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 525859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:02,757 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:02,758 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 73888 input tokens (60000 > 129024 - 73888). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:02,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:02,947 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 73888 input tokens (60000 > 129024 - 73888). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:18,893 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:19,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:25,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:26,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:27,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:27,411 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:33,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:34,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:34,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:35,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:36,370 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:39,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:40,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:42,564 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:43,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:44,944 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:49,352 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:51,611 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:51,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:51,845 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:51,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:52,108 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:52,109 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:52,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:52,340 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:52,347 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:52,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:52,605 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:52,886 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:52,888 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:52,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:53,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:53,164 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:53,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:53,395 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:53,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:53,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:53,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:53,937 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:54,208 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:54,209 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:54,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:54,480 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:54,745 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:54,746 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100812 input tokens (60000 > 129024 - 100812). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:56,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:56,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:03,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:05,266 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:06,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:06,397 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:10,163 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:12,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:13,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:13,478 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:13,479 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71443 input tokens (60000 > 129024 - 71443). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:13,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:13,628 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71443 input tokens (60000 > 129024 - 71443). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:13,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:13,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:13,809 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71443 input tokens (60000 > 129024 - 71443). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:15,324 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:15,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:18,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:18,885 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:19,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:20,985 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:21,669 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:26,906 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:27,954 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:29,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:31,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:31,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:33,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:33,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:35,417 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:36,405 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:44,946 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:46,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:48,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:54,562 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:54,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:54,982 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:55,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:55,462 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:55,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:55,941 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:55,942 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:56,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:56,390 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 225974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:56,580 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:56,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:56,876 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 225974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:57,330 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:57,331 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 225974 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:58,735 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:58,736 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 730037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:59,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:59,572 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:59,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:00,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:00,260 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 730037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:03,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:03,516 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:03,678 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:05,160 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:05,161 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 730037 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:05,924 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:05,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:07,794 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:07,795 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:09,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:09,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 736668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:10,038 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:10,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:12,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:12,031 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804590 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:13,178 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:13,180 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 736668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:14,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:14,723 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 736668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:16,636 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:16,637 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 940192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:18,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:18,728 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 940192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:20,734 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:20,736 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 940192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:23,181 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:23,182 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1448522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:25,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:25,542 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1448522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:26,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:27,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:28,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:28,034 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1448522 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:29,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:31,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:31,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:31,745 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1921342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:32,765 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:34,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:35,173 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:35,516 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:35,518 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1921342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:39,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:39,757 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178225 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:39,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:39,793 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1921342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:43,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:43,833 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178225 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:48,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:48,494 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2789515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:50,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:52,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:52,635 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178225 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:52,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:53,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:57,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:57,132 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2789515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:59,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:00,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:00,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:03,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:03,093 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2789515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:06,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:06,488 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:06,688 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:07,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:08,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:08,186 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:12,258 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:13,482 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:13,483 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:13,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:16,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:16,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:18,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:18,373 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:20,791 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:23,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:23,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:23,709 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:26,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:26,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:28,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:29,825 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:29,826 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:31,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:34,363 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:34,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:34,750 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3156907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:35,245 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:35,318 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:37,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:38,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:39,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:42,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:42,874 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:43,630 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:45,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:48,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:49,766 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:50,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:52,423 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:56,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:56,595 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:58,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:00,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:00,111 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:00,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:01,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:02,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:03,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:03,724 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:04,506 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:06,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:07,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:07,062 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:07,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:22,645 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:23,015 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:23,378 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:23,378 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:23,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:23,781 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:24,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:24,184 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:25,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:25,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:25,374 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:26,530 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:26,531 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:27,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:27,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:27,739 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:30,708 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:34,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:38,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:40,087 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:42,130 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:43,379 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:44,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:45,378 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:47,986 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:48,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:48,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:48,490 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:48,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108692 input tokens (60000 > 129024 - 108692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:48,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:48,717 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108692 input tokens (60000 > 129024 - 108692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:48,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:48,943 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108692 input tokens (60000 > 129024 - 108692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:50,744 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:53,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:54,269 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:54,639 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:58,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:58,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:58,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:58,734 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83474 input tokens (60000 > 129024 - 83474). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:58,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:58,973 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83474 input tokens (60000 > 129024 - 83474). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:59,216 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:59,217 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83474 input tokens (60000 > 129024 - 83474). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:00,335 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:01,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:02,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:02,718 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:06,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:06,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:06,665 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:06,666 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85820 input tokens (60000 > 129024 - 85820). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:06,870 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:06,871 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85820 input tokens (60000 > 129024 - 85820). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:07,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:07,115 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85820 input tokens (60000 > 129024 - 85820). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:07,222 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:07,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:07,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:07,338 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77264 input tokens (60000 > 129024 - 77264). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:07,566 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:07,572 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77264 input tokens (60000 > 129024 - 77264). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:07,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:07,818 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77264 input tokens (60000 > 129024 - 77264). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:09,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:10,672 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:13,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:17,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:18,792 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:20,271 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:21,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:21,505 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:23,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:24,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:25,897 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:27,743 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:40:27,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:28,384 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:30,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:31,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:33,652 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:34,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:36,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:36,953 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:37,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:37,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101901 input tokens (60000 > 129024 - 101901). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:37,486 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:37,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101901 input tokens (60000 > 129024 - 101901). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:37,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:37,754 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101901 input tokens (60000 > 129024 - 101901). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:41,309 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:41,402 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:42,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:44,875 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:45,395 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:45,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:46,231 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:49,799 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:50,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:51,333 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:52,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:54,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:55,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:55,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:56,698 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:57,196 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:57,197 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089496 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:58,458 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:58,907 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:58,908 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089496 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:59,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:59,990 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:59,992 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:00,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:02,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:02,048 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089496 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:03,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:03,250 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:05,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:05,077 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:05,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:05,918 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:06,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:06,160 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:06,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:08,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:08,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:08,177 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75818 input tokens (60000 > 129024 - 75818). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:08,182 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:08,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:08,557 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177403 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:08,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:08,737 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75818 input tokens (60000 > 129024 - 75818). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:10,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:10,713 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177403 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:10,721 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:10,722 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:11,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:11,012 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75818 input tokens (60000 > 129024 - 75818). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:12,065 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:12,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:13,048 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:13,049 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177403 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:13,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:13,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:15,188 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:15,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:15,288 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:15,973 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:17,014 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:17,015 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089797 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:33,743 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:34,514 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:42,307 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:42,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:42,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:43,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:46,001 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:46,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:46,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:49,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:49,945 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:49,946 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:53,342 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:53,344 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:55,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:57,697 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:57,698 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:59,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:00,930 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:03,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:04,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:05,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:09,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:12,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:13,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:13,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:15,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:16,560 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:17,448 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:17,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:18,430 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:18,431 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:18,797 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:18,798 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:18,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:18,965 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:19,812 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:19,814 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:20,156 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:20,157 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:21,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:21,095 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:21,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:21,471 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:22,654 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:22,655 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:23,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:23,930 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:25,240 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:25,242 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:38,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:39,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:42,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:43,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:44,915 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:45,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:45,904 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:46,250 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:46,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:46,453 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 345610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:47,003 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:47,005 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 345610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:47,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:47,579 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 345610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:49,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:50,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:50,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:53,316 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:55,610 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:55,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:55,853 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:56,110 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:56,112 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:56,372 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:56,373 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:56,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:56,626 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:56,628 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:56,860 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:56,861 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:57,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:57,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:57,156 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:57,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:00,999 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:02,131 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:04,332 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:05,931 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:07,752 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:09,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:13,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:13,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:14,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:14,608 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:14,609 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:14,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:14,851 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:15,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:15,107 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:16,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:17,523 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:18,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:19,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:20,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:22,287 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:22,541 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:22,542 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108583 input tokens (60000 > 129024 - 108583). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:22,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:22,812 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108583 input tokens (60000 > 129024 - 108583). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:23,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:23,077 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108583 input tokens (60000 > 129024 - 108583). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:23,288 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:23,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:24,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:24,876 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 867492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:25,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:25,081 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100309 input tokens (60000 > 129024 - 100309). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:25,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:26,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:26,462 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 867492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:26,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:26,652 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100309 input tokens (60000 > 129024 - 100309). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:27,980 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:27,981 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 867492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:28,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:28,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100309 input tokens (60000 > 129024 - 100309). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:28,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:28,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:28,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:28,621 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:28,622 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:28,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:28,845 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:30,703 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:33,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:42,284 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:45,428 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:45,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:45,661 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:45,902 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:45,903 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:46,154 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:46,156 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:46,253 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:43:46,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:46,408 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:46,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:46,630 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:46,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:46,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:46,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:54,629 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:56,147 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:56,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:00,633 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:00,925 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:01,346 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:03,669 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:03,805 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:04,808 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:08,109 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:08,160 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:08,162 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:11,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:12,171 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:12,835 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:13,810 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:16,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:16,246 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:16,248 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:16,471 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:16,472 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:16,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:16,692 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:20,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:21,213 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:21,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:21,326 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:21,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:21,635 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:22,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:22,019 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:22,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:22,495 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:22,497 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:22,922 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:22,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:23,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:23,422 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:23,498 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:25,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:26,582 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:28,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:30,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:31,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:32,515 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:32,788 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:33,085 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:35,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:36,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:36,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:36,883 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 212941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:37,339 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:37,341 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 212941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:37,349 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:37,445 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:37,754 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:37,755 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 212941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:37,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:39,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:39,226 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 785513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:40,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:40,722 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 785513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:41,010 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:42,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:42,298 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 785513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:42,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:43,653 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:45,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:46,204 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:46,438 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:46,439 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:46,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:46,695 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:46,951 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:46,953 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100318 input tokens (60000 > 129024 - 100318). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:48,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:51,072 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:51,367 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:53,353 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:53,568 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:56,205 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:56,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:57,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:58,657 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:59,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:02,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:03,378 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:05,996 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:06,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:07,475 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:08,594 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:08,929 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:09,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:10,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:10,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:13,911 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:15,091 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:16,269 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:17,443 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:17,862 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:18,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:18,215 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:18,217 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 418381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:19,020 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:19,021 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 418381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:19,750 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:19,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:19,849 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 418381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:20,243 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:20,244 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 201621 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:20,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:20,672 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 201621 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:21,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:21,085 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 201621 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:21,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:22,036 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:22,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:23,921 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:24,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:24,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:24,772 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:27,267 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:27,268 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:28,488 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:28,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:28,625 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:28,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:28,880 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102420 input tokens (60000 > 129024 - 102420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:29,155 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:29,157 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102420 input tokens (60000 > 129024 - 102420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:29,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,422 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:29,423 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 102420 input tokens (60000 > 129024 - 102420). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:31,855 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,357 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:34,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:36,383 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:38,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:40,351 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:41,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:44,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:45,963 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:46,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:47,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:48,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:55,939 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:57,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,207 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:59,209 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:00,933 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:00,934 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:03,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:03,800 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:04,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,596 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:05,598 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:08,542 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:08,544 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:10,932 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:10,933 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:10,937 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,175 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,797 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:16,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:16,272 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:18,183 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:20,123 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:20,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:20,918 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:21,227 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:21,228 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:22,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:26,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:26,252 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:26,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:26,706 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177352 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:28,493 [WARNING] gen_code timeout (attempt 1)
2025-12-10 18:46:28,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:29,992 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:30,267 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:30,268 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:32,042 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:33,553 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:33,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:33,797 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:35,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:36,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:36,843 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:37,311 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:40,135 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:40,138 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114594 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:42,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:43,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:43,357 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:43,358 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114594 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:44,223 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:46,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:46,581 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:47,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:49,032 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:49,936 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:49,937 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114594 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:51,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:53,104 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:53,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:55,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:55,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:57,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:57,629 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:59,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:01,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:01,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:02,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:05,741 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:05,743 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:07,785 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:08,120 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:10,201 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:10,668 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:10,669 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:12,775 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:13,052 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:13,431 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:13,432 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840803 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:16,034 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:16,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:16,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840803 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:16,365 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:18,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:19,221 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:19,223 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840803 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:21,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:21,463 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:21,464 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:21,753 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:23,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:23,694 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:23,695 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:24,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:25,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:25,440 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:26,051 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:26,736 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:28,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:28,039 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:29,975 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:29,975 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:30,030 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:31,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:34,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:34,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:34,659 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:35,823 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:35,825 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:36,883 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:38,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:38,915 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:40,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:40,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:40,063 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177253 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:40,070 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 697186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:40,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:44,008 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:44,406 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:44,406 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:45,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:45,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177253 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:45,671 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:45,671 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 697186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:47,784 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:48,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:48,958 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:49,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:49,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:49,879 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177253 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:49,883 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 697186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:50,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:53,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:53,024 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:54,459 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:55,783 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:55,784 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:59,046 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:59,047 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881487 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:01,336 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:02,675 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:03,083 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:03,084 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:07,290 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:07,291 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:09,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:10,664 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:10,666 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:11,418 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:14,121 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:14,123 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:15,535 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:16,776 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:17,094 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:17,095 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:20,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:21,076 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:21,078 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578221 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:23,605 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:24,421 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:24,422 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:26,943 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:28,590 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:28,591 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:32,098 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:32,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:32,100 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:32,130 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:32,446 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:37,492 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:37,494 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:38,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:40,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:40,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:40,685 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:40,693 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:44,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:44,688 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:46,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:47,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:47,877 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:47,878 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:47,882 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:50,940 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:50,941 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:50,988 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:52,060 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:54,493 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:54,494 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:57,011 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:57,012 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:58,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:59,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:00,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:01,080 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:01,081 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:03,884 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:04,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:04,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:04,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:04,757 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:06,078 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:07,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:07,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:08,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:08,717 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:10,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:11,229 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:12,033 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:12,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:12,434 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578216 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:12,690 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:14,079 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:14,712 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:14,713 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572614 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:14,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:15,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:16,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:16,614 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1153463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:17,543 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:18,814 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:18,815 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572614 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:20,209 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:20,882 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:20,883 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1153463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:22,187 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:22,949 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:22,951 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572614 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:23,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:24,317 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:24,720 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:24,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1153463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:25,599 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:28,739 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:28,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:30,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:30,014 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839171 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:30,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:31,154 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:31,294 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:34,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:34,760 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:35,809 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:35,810 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839171 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:37,441 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:37,695 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:37,942 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:39,833 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:39,835 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:40,881 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:40,882 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839171 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:42,381 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:42,849 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:43,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:45,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:45,322 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:47,864 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:47,865 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:50,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:51,247 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:52,579 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:52,581 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:54,455 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:55,147 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:55,148 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:56,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:58,843 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:58,844 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:00,759 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:02,241 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:02,243 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:03,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:05,519 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:05,521 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:05,559 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:06,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:08,600 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:08,601 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:09,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:12,474 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:12,476 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:13,691 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:15,331 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:15,333 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:15,371 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:16,179 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:18,876 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:18,878 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324981 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:20,449 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:21,976 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:21,977 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:24,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:25,670 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:26,099 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:26,100 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:27,315 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:28,461 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:28,463 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:29,359 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:32,279 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:32,280 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:33,444 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:34,369 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:34,371 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:36,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:39,113 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:39,114 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:39,989 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:41,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:41,575 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:44,644 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:45,427 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:45,637 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:45,639 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:46,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:48,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:48,744 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:50,868 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:50,917 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:53,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:53,069 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:54,238 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:56,415 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:56,417 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:58,577 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:01,064 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:01,065 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:02,696 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:04,299 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:04,302 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:07,043 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:08,377 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:08,378 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2355020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:10,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:12,096 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:12,361 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:13,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:13,250 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2355020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:14,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:16,027 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:16,219 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:16,545 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:16,717 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:16,718 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2355020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:19,891 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:19,893 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:22,070 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:22,967 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:23,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:23,822 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:24,896 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:26,392 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:26,394 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:27,959 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:31,074 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:31,075 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:33,920 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:34,203 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:34,208 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:35,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:38,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:38,700 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:40,075 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:40,076 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 664804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:40,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:42,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:42,863 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:42,864 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:44,146 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:44,147 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 664804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:44,337 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:45,504 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:46,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:46,881 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:47,749 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:48,143 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:48,144 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 664804 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:51,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:51,609 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:51,610 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:52,788 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:54,716 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:54,718 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:56,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:56,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:57,166 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:57,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:00,451 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:00,452 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:00,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:02,935 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:02,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:03,031 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:04,305 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:05,613 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:05,614 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:07,722 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:07,723 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:08,017 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:10,022 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:10,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:10,320 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1423519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:11,769 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:13,751 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:13,753 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:15,724 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:16,236 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:16,238 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1423519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:18,770 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:20,348 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:20,349 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:21,830 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:22,385 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:22,387 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1423519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:22,426 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:24,561 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:26,168 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:26,170 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577726 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:28,013 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:29,393 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:29,394 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:31,009 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:31,650 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:31,652 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:33,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:34,606 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:34,608 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:35,661 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:36,327 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:37,161 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:37,163 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:40,544 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:40,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000394 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:40,555 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:42,404 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:43,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:43,266 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590902 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:43,592 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:43,593 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:46,529 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:46,693 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:46,693 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:47,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:47,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:48,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:49,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:50,798 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:50,800 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:51,235 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:51,237 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 186632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:54,254 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:54,256 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:55,296 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:55,494 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:56,727 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:56,729 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:57,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:58,115 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:59,174 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:59,175 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:59,528 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:00,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:00,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:01,340 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:01,342 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:02,997 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:03,073 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:04,263 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:04,264 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:05,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:07,433 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:07,434 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:08,730 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:09,165 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:10,401 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:10,402 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948696 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:11,756 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:13,396 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:13,398 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:14,077 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:14,839 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:15,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:16,206 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:16,207 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:19,184 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:19,187 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856475 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:19,310 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:21,390 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:21,801 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:21,802 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397771 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:22,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:23,139 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:24,469 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:24,470 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397771 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:25,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:25,844 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:25,938 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:26,574 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:26,627 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:28,500 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:28,503 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397771 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:29,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:29,891 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:30,742 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:31,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:32,320 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:34,285 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:34,287 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:35,663 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:35,665 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:36,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:38,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:40,056 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:40,057 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:41,842 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:42,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:42,274 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1368577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:43,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:46,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:46,302 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:48,473 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:48,474 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1368577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:51,256 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:51,258 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1386028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:51,286 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:53,150 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:54,319 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:54,321 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1368577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:54,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:56,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:56,780 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1386028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:57,270 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:57,927 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:57,929 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 728824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:00,190 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:00,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1386028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:01,432 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:01,435 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 728824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:02,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:02,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:02,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:06,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:06,008 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818276 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:07,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:07,292 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 728824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:08,824 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:09,382 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:09,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:10,439 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:10,440 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818276 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:11,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:12,018 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:14,277 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:14,278 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818276 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:15,252 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:15,701 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:16,128 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:16,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:16,998 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:16,999 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:17,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:18,133 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:18,462 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:19,584 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:19,585 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:20,852 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:21,082 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:21,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:21,615 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:21,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:22,092 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:22,345 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:22,598 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:23,848 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:23,849 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069891 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:25,248 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:25,306 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:26,419 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:26,420 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069891 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:27,364 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:27,578 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:27,738 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:28,583 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:28,889 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:28,890 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069891 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:30,172 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:30,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:30,619 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:33,111 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:33,117 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:33,119 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:33,850 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:34,649 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:36,991 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:36,992 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:38,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:38,762 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:39,028 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:40,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:41,062 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:41,063 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578235 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:42,966 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:43,037 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:43,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:43,251 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100214 input tokens (60000 > 129024 - 100214). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:43,502 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:43,504 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100214 input tokens (60000 > 129024 - 100214). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:43,740 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:43,741 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100214 input tokens (60000 > 129024 - 100214). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:43,983 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:43,984 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:44,228 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:44,229 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:44,460 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:44,461 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:44,706 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:44,707 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:44,947 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:44,948 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:45,055 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:45,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:45,193 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100219 input tokens (60000 > 129024 - 100219). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:46,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:47,114 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:47,275 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:49,709 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:49,796 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:51,210 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:53,550 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:54,193 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:56,237 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:57,472 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:57,737 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:58,176 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:01,534 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:01,643 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:02,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:02,778 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:03,414 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:04,029 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:04,313 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:05,733 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:06,710 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:06,858 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:06,860 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 570368 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:07,024 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:07,952 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:07,953 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 570368 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:08,859 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:09,019 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:09,021 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 570368 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:09,436 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:10,474 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:12,091 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:12,141 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:13,831 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:13,880 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:14,519 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:15,220 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:16,274 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:17,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:17,640 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:17,642 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1866198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:18,386 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:20,301 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:20,394 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:20,395 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1866198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:20,677 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:20,926 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:21,970 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:23,302 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:23,303 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1866198 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:25,249 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:25,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:25,890 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:25,891 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109330 input tokens (60000 > 129024 - 109330). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:26,124 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:26,125 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109330 input tokens (60000 > 129024 - 109330). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:26,368 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:26,369 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109330 input tokens (60000 > 129024 - 109330). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:26,604 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:26,605 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109339 input tokens (60000 > 129024 - 109339). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:26,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:26,847 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109339 input tokens (60000 > 129024 - 109339). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:27,086 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:27,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109339 input tokens (60000 > 129024 - 109339). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:27,329 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:27,330 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109367 input tokens (60000 > 129024 - 109367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:27,570 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:27,572 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109367 input tokens (60000 > 129024 - 109367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:27,618 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:27,817 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:27,818 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109367 input tokens (60000 > 129024 - 109367). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:27,820 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:30,634 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:31,807 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:32,900 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:33,854 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:34,437 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:36,343 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:36,601 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:37,264 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:37,265 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 318608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:37,771 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:37,919 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:37,920 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 318608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:38,540 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:38,541 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 318608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:38,620 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:41,699 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:42,632 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:43,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:43,507 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:43,914 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:43,915 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:44,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:44,381 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:44,870 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:44,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234723 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:00,508 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:02,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:03,291 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:03,711 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:05,994 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:06,485 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:06,888 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:09,539 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:09,979 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:10,465 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:12,189 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:13,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:14,450 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:14,451 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:15,191 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:16,232 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:16,233 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:16,802 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:18,012 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:18,014 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:18,158 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:20,066 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:21,251 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:23,844 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:23,845 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:25,489 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:29,278 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:29,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:34,325 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:35,846 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:36,552 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:37,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:38,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:38,721 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:38,722 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 74910 input tokens (60000 > 129024 - 74910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:38,923 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:38,924 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 74910 input tokens (60000 > 129024 - 74910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:39,127 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:39,129 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 74910 input tokens (60000 > 129024 - 74910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:39,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:40,338 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:41,407 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:43,105 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:44,795 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:44,962 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:48,538 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:50,358 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:52,702 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:54,780 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:56,225 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:58,614 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:59,957 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:00,821 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:01,224 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:01,822 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:02,084 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:05,145 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:05,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:05,472 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:05,899 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:05,901 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:07,089 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:08,971 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:10,682 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:10,684 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:10,685 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100380 input tokens (60000 > 129024 - 100380). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:10,690 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:11,403 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:11,404 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:13,167 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:14,768 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:15,565 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:15,567 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:15,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100380 input tokens (60000 > 129024 - 100380). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:15,574 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:16,196 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:16,198 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:18,607 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:19,894 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:19,895 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:19,897 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100380 input tokens (60000 > 129024 - 100380). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:19,900 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:20,453 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:23,289 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:23,380 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:25,341 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:25,342 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:27,067 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:28,058 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:29,303 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:29,304 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:29,705 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:33,470 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:33,472 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:34,040 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:34,681 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:37,388 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:37,390 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:37,511 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:37,547 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:38,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:38,816 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 808148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:43,185 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:43,186 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:43,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:43,257 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 147009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:44,466 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:44,468 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 808148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:44,683 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:50,053 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:51,587 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:51,589 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:51,590 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 147009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:51,596 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:53,497 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:53,499 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 808148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:55,151 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:00,781 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:00,782 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:00,783 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 147009 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:00,787 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:01,474 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:01,475 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:02,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:03,457 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:09,522 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:09,523 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:10,202 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:10,203 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:14,362 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:18,726 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:18,727 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:19,097 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:19,098 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:22,687 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:23,972 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:25,298 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:29,273 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:29,275 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:32,122 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:32,537 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:32,879 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:37,134 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:37,135 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5067816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:40,554 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:41,090 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:42,955 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:44,532 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:44,534 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5068148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:45,512 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:45,514 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 439399 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:47,623 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:47,961 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:52,746 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:52,746 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5068148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:53,704 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:53,705 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 439399 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:56,373 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:56,509 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:57,709 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:03,887 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:03,888 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5068148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:04,666 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:04,668 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 439399 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:06,811 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:06,878 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:11,300 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:22,815 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:22,816 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7555813 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:25,499 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:25,602 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:26,536 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:26,834 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:36,789 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:36,791 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7555813 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:38,638 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:43,479 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:44,007 [INFO] HTTP Request: POST http://localhost:8084/v1/chat/completions "HTTP/1.1 200 OK"
