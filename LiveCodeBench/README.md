# LiveCodeBench Evaluation Scripts

This directory contains scripts to generate code predictions and evaluate them using the [LiveCodeBench](https://github.com/LiveCodeBench/LiveCodeBench) dataset.

## Structure

- **`run_expr.py`**: The main entry point for running evaluations. It orchestrates the process of generating code using LLMs and saving the results.
- **`evaluation.py`**: Compares the generated predictions against ground truth (GT) to calculate accuracy metrics and generate reports.

## Prerequisites

Ensure you have the necessary Python packages installed:

```bash
pip install openai anthropic datasets tqdm aiofiles
```

Set up your API keys as environment variables or pass them as arguments:
- `OPENAI_API_KEY` (if using OpenAI models)
- `ANTHROPIC_API_KEY` (if using Anthropic models)

## Usage

### 1. Generation & Execution (`run_expr.py`)

Run the evaluation script to generate predictions or evaluate code validation tasks.

```bash
python run_expr.py \
  --problems <path_to_problem_json> \
  --out <output_results_json> \
  --model <model_name> \
  --platform <vllm|openai|anthropic> \
  --task <pred|bug_local|bug_report>
```

**Arguments:**
- `--problems`: Path to the input JSON file containing problems.
- `--out`: Path to save the output results.
- `--task`: Task type (`pred`, `bug_local`, `bug_report`). Default is `pred`.
- `--model`: Model name to use (e.g., `Qwen/Qwen3-Coder-30B-A3B-Instruct`).
- `--platform`: Platform backend (`vllm`, `openai`, `anthropic`).
- `--temperature`: Sampling temperature (default 0).
- `--top_p`: Top-p sampling (default 1).
- `--api_key`: Optional API key argument.
- `--base_url`: Optional Base URL for API.

### 2. Accuracy Evaluation (`evaluation.py`)

After generating results, evaluate the accuracy against ground truth.

```bash
python evaluation.py \
  --pred_file <path_to_predictions_json> \
  --gt_file <path_to_ground_truth_json>
```

**Arguments:**
- `--pred_file`: Path to the prediction file generated by `run_eval.py`.
- `--gt_file`: Path to the ground truth JSON file.
- `--level_dir`: Directory containing difficulty level information (optional).
- `--target_code_idx`: Index of the code snippet to evaluate (default 0).

## Features

- **Multi-Platform Support**: Supports OpenAI compatible APIs (vLLM) and Anthropic API.
- **Concurrent Execution**: Uses `asyncio` for concurrent task processing (default 5 tasks).
- **Resumable**: Skips already processed question IDs in the output file.
- **Detailed Logging**: Logs execution details to `log_code_eval1.txt`.
