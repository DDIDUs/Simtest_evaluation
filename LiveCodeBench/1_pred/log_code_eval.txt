2025-12-10 15:12:19,217 [INFO] HTTP Request: GET https://huggingface.co/api/datasets/livecodebench/code_generation_lite "HTTP/1.1 200 OK"
2025-12-10 15:12:27,850 [INFO] Indexed livecodebench rows: 1055
2025-12-10 15:12:31,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,160 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:31,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:32,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:33,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:33,647 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:33,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:33,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:33,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:33,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:34,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:34,862 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:34,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:35,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:35,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:35,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:36,123 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:36,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:36,769 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:36,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:36,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:37,023 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:37,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:37,261 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:37,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:37,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:37,821 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:38,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:38,024 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:38,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:38,232 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:38,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:39,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:39,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:39,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:40,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:41,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:41,042 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:42,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:42,095 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:42,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:42,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:43,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:43,147 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:43,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:45,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:45,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:45,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:46,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:46,586 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:49,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:49,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:49,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:50,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:50,193 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:50,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:50,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:51,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:51,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:51,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:54,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:54,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:54,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:54,703 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:55,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:55,290 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:55,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:55,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:55,680 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:55,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:57,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:57,972 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:58,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:58,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:12:58,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:12:58,512 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:12:58,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:00,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:00,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:00,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:00,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:01,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:01,422 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:01,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:03,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:03,665 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:03,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,187 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:04,490 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:04,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:04,781 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:04,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:04,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:04,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:05,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:05,152 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:05,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:05,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:05,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:05,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:05,649 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:05,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:05,789 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:05,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:06,364 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:06,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:06,491 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:06,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:06,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:06,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:07,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,844 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:07,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:08,250 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:08,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:08,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,606 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:08,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:08,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:08,903 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:09,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:09,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:14,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:14,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:14,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:14,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:14,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:15,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:15,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:15,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:15,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:15,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:16,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:16,425 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:16,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:16,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:16,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:16,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:17,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:17,512 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:17,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:17,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:17,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:17,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:18,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:18,697 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:20,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:20,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:20,812 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:21,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:21,240 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:21,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:21,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:21,642 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:22,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:22,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:28,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:28,927 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:29,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:29,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:29,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:31,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:35,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:35,834 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:36,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:36,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:36,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:37,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:42,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:42,790 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:44,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:45,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:45,202 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:46,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:46,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:52,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:52,210 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:52,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:13:54,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:54,732 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:54,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:13:54,751 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:13:55,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:01,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:01,993 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:03,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:03,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:03,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:03,904 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:07,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:10,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:10,239 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:10,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:11,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:11,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:11,932 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:11,953 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:12,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:18,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:18,567 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:18,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:20,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:20,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:20,674 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:20,697 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:26,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:26,276 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:26,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:26,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:26,580 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:26,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:31,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:31,544 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:31,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:31,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:31,585 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:31,592 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:32,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:32,228 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:32,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:40,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:40,495 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:40,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:40,541 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:42,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:42,425 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:42,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:42,448 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:42,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:42,897 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:42,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:48,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:48,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:48,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:48,489 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:50,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:50,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:50,405 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:50,411 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:50,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:50,701 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:57,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:57,817 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:57,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:57,862 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:58,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:14:59,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:14:59,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 829199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:14:59,560 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:05,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:05,648 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:05,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:05,691 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:07,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:07,863 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:07,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:13,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:13,690 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:13,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:13,731 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:13,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:16,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:16,096 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:16,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:22,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:22,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:22,055 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:22,098 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:23,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:23,877 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:23,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:24,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:24,283 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:24,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:31,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:31,865 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:31,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:31,912 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:31,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:32,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:32,688 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:32,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:38,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:38,663 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:38,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:38,721 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:38,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:39,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:39,412 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:39,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:45,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:45,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:45,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:45,101 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:45,144 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:45,153 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:45,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:45,922 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:46,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:51,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:51,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:51,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:51,805 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:51,845 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:51,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:52,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:52,694 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:59,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:15:59,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:59,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:15:59,109 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:15:59,146 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:01,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:01,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:01,152 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:01,158 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:07,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:07,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:07,018 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:07,066 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:07,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:07,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:07,753 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:07,759 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:13,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:13,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:13,040 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:13,080 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 499999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:13,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:13,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:13,922 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:13,929 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:14,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:19,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:19,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:19,174 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:19,215 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 499999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:19,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:19,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:19,947 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:19,952 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:25,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:25,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:25,667 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:25,708 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 499999 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:25,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:26,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:26,595 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:26,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:31,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:31,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:31,882 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:31,920 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:32,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:32,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:33,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:33,852 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:36,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:40,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:40,066 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:40,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:42,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:42,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:42,172 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:42,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 887291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:42,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:49,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:49,119 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:49,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:51,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:51,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:51,308 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:51,330 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:57,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:57,298 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:57,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:16:58,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:16:58,727 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:16:58,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:04,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:04,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:04,135 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:04,173 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:04,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:05,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:05,796 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:05,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:11,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:11,044 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:11,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:11,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:11,803 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:11,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:11,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:18,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:18,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:18,895 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:18,936 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:19,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:19,753 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:19,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:24,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:24,922 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:25,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:25,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:25,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:25,637 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:25,643 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 390458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:25,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:31,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:31,903 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:32,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:33,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:33,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:33,885 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:33,889 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:33,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:39,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:39,047 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:39,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:41,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:41,123 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:41,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:41,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:46,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:46,181 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:46,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:47,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:47,948 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077980 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:48,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:53,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:53,372 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:53,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:17:55,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:17:55,005 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:17:55,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:01,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:01,481 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:01,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:01,533 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:03,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:03,928 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:04,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:07,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:07,152 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:07,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:12,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:12,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:12,418 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:12,458 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:16,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:16,214 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:16,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:22,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:22,948 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:22,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:22,993 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:23,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:23,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:23,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:25,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:25,970 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:32,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:32,966 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:33,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:33,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:33,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:40,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:40,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:40,295 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:40,334 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1078265 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:40,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:40,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:41,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:41,847 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:41,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:47,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:47,870 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:48,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:48,896 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:48,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:55,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:56,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:57,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:57,606 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:58,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:18:58,575 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 533066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:18:58,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:18:58,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:05,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:05,774 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:05,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:07,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:07,468 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:07,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:14,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:14,548 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:14,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:14,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:16,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:16,264 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:16,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:22,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:22,545 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:22,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:22,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:23,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:23,955 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 879543 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:24,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:31,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:31,269 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:31,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:32,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:32,393 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:32,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:39,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:39,324 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:39,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:40,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:40,365 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:44,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:46,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:46,455 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:46,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:46,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:47,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:47,614 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494943 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:47,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:53,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:53,694 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:53,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:55,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:19:55,267 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:19:55,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:19:55,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:00,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:00,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:00,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:02,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:02,372 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:02,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:02,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:07,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:07,991 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:08,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:09,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:09,591 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 928443 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:09,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:15,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:15,689 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:15,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:17,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:17,366 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:17,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:17,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:22,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:22,724 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:22,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:24,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:24,666 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:24,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:24,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:30,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:30,746 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:30,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:32,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:32,670 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:32,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:38,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:38,569 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:38,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:38,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:40,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:40,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077925 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:40,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:46,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:46,550 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:46,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:48,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:48,732 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077925 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:48,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:48,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:53,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:53,968 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:54,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:55,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:20:55,874 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1077925 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:20:56,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:20:56,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:01,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:01,640 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:01,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:03,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:03,511 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:03,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:03,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:09,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:09,205 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:09,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:11,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:11,053 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:11,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:11,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:11,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:13,108 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1042064 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:13,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:13,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:14,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,579 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:15,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:16,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:16,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:16,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:16,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:19,632 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:19,633 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:22,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:22,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:22,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:22,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:22,979 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:25,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:25,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:25,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:25,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:26,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:26,472 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:29,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:29,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:29,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:30,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:30,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:30,325 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:30,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:30,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:30,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:31,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:34,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:34,141 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:34,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:34,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:34,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:34,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:38,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:38,760 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:40,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:40,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:40,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:43,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:43,339 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:47,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:47,093 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:48,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:48,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:48,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:49,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:51,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:51,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:51,389 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:51,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:51,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:51,838 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:51,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:52,117 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:52,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:52,484 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:52,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:52,831 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:52,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:52,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:53,125 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:53,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:53,513 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:53,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:53,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:53,904 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:54,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:54,295 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:54,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:54,535 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:54,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:54,771 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:54,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:54,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:55,042 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:21:55,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:55,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:57,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:21:59,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:21:59,959 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:00,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:00,273 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:00,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:00,582 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:00,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:00,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:01,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:02,185 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:02,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:02,457 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:02,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:02,769 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:02,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:02,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:03,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:03,270 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:03,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:03,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:03,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:03,663 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:03,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:03,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:04,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:04,049 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:04,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:04,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:07,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:08,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:08,218 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:08,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:08,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:08,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:08,592 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:08,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:08,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:08,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:08,983 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:09,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,632 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:09,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:10,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:10,144 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:10,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:10,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:10,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:10,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:10,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:10,722 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:10,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:11,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:11,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:11,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:11,424 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:11,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:13,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:13,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:13,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:13,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:13,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:14,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:14,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:14,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:15,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:15,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:19,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:19,082 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:20,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:20,093 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:20,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:20,105 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:20,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:21,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:25,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:25,120 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:26,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:26,094 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:26,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:26,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:26,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:26,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:31,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:31,304 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:32,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:32,290 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:32,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:32,315 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:32,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:32,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:37,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:37,967 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:39,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:39,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:39,763 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:39,784 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:40,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:42,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:44,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:44,381 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:44,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:44,427 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:47,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:47,122 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:47,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:47,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:47,906 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:48,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:51,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:51,606 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:51,635 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:52,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:52,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:52,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:53,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:55,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:55,374 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:55,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:55,404 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:56,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:56,017 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:56,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:22:59,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:59,114 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:59,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:59,146 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:59,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:22:59,620 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:22:59,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:00,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:02,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:02,910 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:02,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:02,966 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:03,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:03,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:04,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:09,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:09,678 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:09,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:09,744 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:10,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:10,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:11,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:12,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:12,354 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:18,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:18,321 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:18,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:18,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:19,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:20,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:20,943 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:26,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:26,296 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:26,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:26,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:28,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:28,766 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:29,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:32,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:32,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:32,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:34,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:34,959 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:35,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:38,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:38,862 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:38,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:40,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:40,867 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:40,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:41,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:44,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:44,670 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:44,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:46,174 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:46,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:46,177 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:46,190 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:49,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:49,206 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:49,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:49,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:50,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:50,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:50,475 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:50,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:53,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:53,639 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:53,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:55,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:55,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:55,182 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601093 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:55,198 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:55,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:23:58,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:23:58,184 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:23:58,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:00,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:00,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:00,015 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:00,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:04,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:04,025 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:04,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:04,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:05,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:05,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:05,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:05,974 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:10,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:10,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:10,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:11,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:11,851 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789766 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:11,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:15,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:15,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:15,713 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:15,746 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:16,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:16,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:17,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:19,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:19,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:19,665 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:19,701 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:20,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:20,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:21,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:21,698 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:21,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:25,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:26,001 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:26,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:28,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:28,343 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:28,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:31,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:31,961 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:32,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:32,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:34,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:34,382 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:34,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:37,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:37,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:37,483 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:37,517 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:39,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:39,263 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:39,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:39,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:39,447 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:39,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:42,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:42,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:42,510 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:42,542 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:42,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:42,834 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:42,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:44,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:44,476 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:44,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:47,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:47,131 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:47,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:48,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:48,898 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:49,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:49,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:51,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:51,374 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:51,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:53,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:53,626 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:53,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:56,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:56,777 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:56,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:59,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:24:59,028 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:24:59,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:24:59,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:01,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:01,512 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:03,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:03,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:03,873 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:03,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:07,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:07,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:08,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:08,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:09,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:09,177 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:09,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:12,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:12,366 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:12,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:12,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:14,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:14,907 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:15,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:18,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:18,982 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:19,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:21,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:21,377 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:21,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:27,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:27,119 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:27,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:28,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:28,955 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:29,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:34,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:34,350 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:34,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:34,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:36,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:36,649 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:36,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:42,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:42,363 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:42,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:44,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:44,617 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:44,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:48,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:48,718 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:48,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:48,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:51,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:51,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:51,050 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:51,071 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:55,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:55,154 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:55,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:55,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:55,474 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:55,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:25:59,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:59,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:59,046 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:59,082 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:59,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:25:59,394 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:25:59,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:02,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:02,343 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:02,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:04,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:04,216 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:04,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:07,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:07,305 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:07,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:09,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:09,313 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:09,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:09,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:12,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:12,452 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:12,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:14,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:14,912 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:15,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:15,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:20,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:20,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:20,087 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:20,118 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:22,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:22,474 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:22,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:23,632 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:23,633 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:23,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:27,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:27,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:27,491 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:27,522 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:27,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:28,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:28,600 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 601051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:28,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:32,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:32,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:32,773 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:32,802 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:33,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:33,593 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:34,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:34,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:34,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:36,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:36,608 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:37,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:37,132 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:37,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:37,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:37,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:39,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:39,750 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:40,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:40,027 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:40,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:41,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:41,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:41,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:41,573 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:44,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:44,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:44,815 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:44,847 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:45,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:45,144 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:45,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:46,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:46,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:46,267 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:46,282 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:50,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:50,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:50,218 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:50,250 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:50,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:52,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:52,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:52,387 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789806 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:52,391 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:56,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:26:56,828 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:26:57,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:57,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:26:57,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:00,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:00,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:00,697 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:00,733 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:01,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:01,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:02,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:02,557 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:02,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:05,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:05,274 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:05,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:06,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:06,697 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:06,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:09,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:09,436 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:10,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:11,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:11,017 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:11,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:14,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:14,293 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:14,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:14,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:16,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:16,196 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:16,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:19,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:19,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:20,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:20,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:22,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:22,292 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:22,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:26,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:26,918 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:27,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:27,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:29,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:29,328 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:29,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:34,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:34,321 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:34,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:36,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:36,679 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:36,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:36,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:40,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:40,434 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:40,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:41,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:41,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:41,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:45,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:45,584 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:46,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:46,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:46,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:46,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:50,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:50,158 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:50,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:50,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:50,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:51,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:54,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:54,140 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:54,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:54,345 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:55,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:55,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:55,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:27:59,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:59,317 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:27:59,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:27:59,513 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:00,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:00,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:01,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:04,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:04,216 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:04,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:04,410 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:05,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:05,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:05,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:09,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:09,503 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:13,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:13,486 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:14,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:14,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:14,160 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:19,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:19,263 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:23,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:23,934 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:24,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:24,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:24,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:29,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:29,011 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:32,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:32,955 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:32,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:32,972 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:34,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:34,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:37,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:37,129 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:37,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:37,806 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:37,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:37,820 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:38,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:38,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:41,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:41,991 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:42,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:42,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:42,705 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:42,716 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:43,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:43,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:46,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:46,821 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:47,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:47,583 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:48,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:48,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:49,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:53,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:53,182 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:57,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:28:57,554 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:28:57,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:57,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:28:58,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:02,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:02,229 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:02,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:02,261 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:03,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:04,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:06,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:06,510 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:06,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:11,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:11,012 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:11,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:13,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:13,455 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:13,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:17,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:17,195 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:17,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:19,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:19,363 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:20,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:20,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:22,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:22,878 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:22,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:25,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:25,554 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:25,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:25,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:29,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:29,828 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:29,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:34,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:34,886 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:35,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:35,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:35,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:39,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:39,492 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:39,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:39,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:39,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:42,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:42,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:45,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:45,925 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:46,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:46,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:46,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:48,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:48,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:52,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:52,623 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:52,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:52,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:52,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:55,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:55,137 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:59,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:29:59,034 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:29:59,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:59,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:29:59,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:03,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:03,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:03,769 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:03,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:04,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:04,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:04,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:05,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:05,964 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:12,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:12,095 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:12,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:12,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:12,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:17,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:17,948 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:17,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:17,995 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:18,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:18,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:18,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:21,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:21,965 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:27,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:27,434 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:27,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:27,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:31,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:31,383 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:31,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:35,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:35,584 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:35,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:39,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:39,774 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:39,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:44,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:44,068 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:44,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:44,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:48,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:48,622 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:48,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:53,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:53,252 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:53,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:57,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:30:57,678 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:30:57,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:30:57,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:01,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:01,645 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:01,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:06,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:06,096 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:06,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:06,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:11,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:11,095 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:11,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:13,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:13,438 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:13,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:18,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:18,643 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:18,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:18,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:20,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:20,302 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:20,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:23,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:23,637 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:23,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:25,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:25,675 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:25,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:29,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:29,382 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:29,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:34,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:34,523 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:34,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:38,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:38,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:38,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:43,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:43,158 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:43,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:46,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:46,490 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:46,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:46,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:51,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:51,136 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:51,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:54,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:54,134 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:54,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:54,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:56,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:56,341 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:56,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:31:59,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:31:59,683 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:31:59,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:01,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:01,608 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:01,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:01,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:04,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:04,701 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:04,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:06,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:06,977 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:07,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:07,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:10,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:10,808 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:10,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:15,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:15,288 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:15,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:19,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:19,197 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:19,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:23,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:23,060 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:23,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:23,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:26,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:26,319 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:26,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:30,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:30,321 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:30,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:34,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:34,234 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:34,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:36,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:36,201 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:36,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:36,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:39,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:39,495 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:39,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:41,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:41,739 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:41,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:41,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:45,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:45,118 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:45,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:47,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:47,596 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:47,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:50,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:50,594 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:50,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:50,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:50,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:50,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:53,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:53,351 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:53,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:53,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:53,632 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:53,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:57,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:32:57,352 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:32:57,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:57,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:57,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:32:57,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:01,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:01,657 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:01,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:01,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:01,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:01,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:06,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:06,237 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:06,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:06,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:06,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:06,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:12,370 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:12,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:12,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:18,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:18,231 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:18,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:18,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:18,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:18,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:24,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:24,860 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:25,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:25,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:26,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:27,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:28,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:29,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:30,249 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:30,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:30,890 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:30,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:30,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:31,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:31,225 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:31,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:32,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:32,073 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:32,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:32,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:32,410 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:32,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:33,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:33,289 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:33,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:33,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:33,627 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:33,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:34,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:34,552 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:34,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:34,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:34,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:34,886 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:34,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:35,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:35,792 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:35,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:36,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:36,135 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:36,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:37,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:37,005 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:37,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:37,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:37,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:37,347 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:37,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:40,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:40,529 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:40,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:40,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:40,977 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:41,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:43,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:43,658 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:43,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:43,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:43,962 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:44,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:46,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:46,198 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:46,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:46,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:46,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:46,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:49,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:49,601 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:49,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:49,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:49,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:49,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:52,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:52,751 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:52,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:52,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:52,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:53,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:56,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:56,247 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:33:56,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:56,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:56,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:56,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:33:59,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:33:59,894 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:00,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:00,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:00,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:00,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:03,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:03,729 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:03,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:03,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:03,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:03,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:07,465 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:07,627 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:07,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:11,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:11,431 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:11,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:11,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:11,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:11,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:15,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:15,392 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:15,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:15,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:15,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:15,598 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:19,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:19,831 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:19,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:20,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:20,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:20,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:23,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:23,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:23,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:23,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:23,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:24,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:26,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:26,789 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:26,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:26,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:26,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:27,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:30,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:30,084 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:30,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:30,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:30,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:30,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:33,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:33,789 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:33,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:34,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:34,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:34,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:37,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:37,579 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:37,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:37,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:37,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:37,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:41,236 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:41,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:41,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,160 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:42,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:43,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:43,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:44,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:44,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:44,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:44,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:45,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:45,025 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:45,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:45,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:45,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:45,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:46,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:46,176 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:47,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:47,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:47,629 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:48,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:48,803 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:49,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:49,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:49,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:49,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:50,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:50,065 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:51,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:51,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:51,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:51,538 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:52,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:53,089 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:53,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:53,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:34:54,388 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:34:54,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:54,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:57,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:58,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:34:59,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:00,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:01,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:01,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:01,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:01,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:02,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:02,491 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:02,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:02,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:02,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:02,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:04,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:04,034 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:04,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:04,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:04,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:04,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:05,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:05,666 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:06,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:06,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:06,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:06,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:06,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:07,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:07,844 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:08,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:08,019 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:08,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:08,190 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:08,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:08,669 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:08,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:09,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:09,181 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:09,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:09,677 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:11,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:11,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:12,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:12,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:22,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:24,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:24,314 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:25,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:25,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:25,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:25,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:26,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:26,254 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:27,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:27,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:28,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:28,260 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:30,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:31,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:33,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:33,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:33,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:34,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:34,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:39,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:41,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:43,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:44,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:44,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:45,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:45,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:45,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:46,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:46,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:46,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:46,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:47,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:49,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:49,517 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:50,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:50,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:50,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:51,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:52,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:52,528 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:52,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:52,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:52,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:52,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:55,002 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:55,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:55,299 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:55,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:55,483 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:55,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:35:55,706 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:35:55,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:55,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:56,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:56,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:57,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:57,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:59,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:59,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:59,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:35:59,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:00,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:02,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:02,197 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:02,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:02,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:02,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:02,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:03,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:03,796 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:05,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:05,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:05,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:05,487 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:07,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:07,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:07,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:07,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:07,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:08,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,375 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:09,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:10,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:13,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:13,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:13,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:13,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:13,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:14,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:14,471 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:14,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:14,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:14,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:14,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:15,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:15,737 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:17,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:17,042 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:21,407 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:21,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:22,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:25,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:26,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:26,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:26,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:26,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:30,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:30,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:30,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:30,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:30,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:31,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:31,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:32,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:32,146 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:32,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:32,569 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:32,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:32,997 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:33,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:33,445 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:33,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:33,898 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:34,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:34,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:34,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:34,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:34,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:34,404 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:34,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:34,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:39,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:39,147 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:39,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:39,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:39,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:43,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:43,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:44,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:44,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:44,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:44,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:48,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:48,818 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:49,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,375 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:49,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:50,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:50,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:50,820 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:50,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:51,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:51,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:51,531 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:51,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:52,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:52,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:52,269 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:52,273 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:53,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:54,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:54,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:54,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:54,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:55,068 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:55,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:55,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:55,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:55,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:56,053 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:56,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:56,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,263 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:36:57,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:57,983 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:58,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:58,498 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:59,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:36:59,016 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:36:59,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:02,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:03,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:08,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:08,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:11,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:11,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:11,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:11,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:11,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:12,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:13,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:16,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,598 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:18,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:19,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:19,338 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:19,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:19,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:19,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:19,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:19,817 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:19,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:20,323 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:20,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:20,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:20,748 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:20,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:21,150 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:21,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:21,629 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:21,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:21,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,432 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:22,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:23,440 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:23,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:23,695 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:23,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:23,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:23,986 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:24,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:24,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:24,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:24,650 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:24,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:24,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:25,005 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:25,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:25,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:26,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:33,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:34,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:39,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:40,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:40,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:40,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:40,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:43,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:49,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:50,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:54,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:55,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:55,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:56,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:56,450 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:56,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:56,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:37:57,001 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:37:57,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:37:57,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:03,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:03,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:03,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:03,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:04,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:05,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:06,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:09,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:10,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:10,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:10,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:16,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:17,147 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:17,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:17,553 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:17,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:17,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:17,952 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:17,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:24,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:24,444 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:24,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:24,655 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:24,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:24,828 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:30,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:30,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:30,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:34,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:34,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:34,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:34,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:34,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:35,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:35,896 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:36,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:36,397 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:36,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:36,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:36,930 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:37,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:37,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:38,575 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:38,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:38,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:38,965 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:39,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:39,329 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:39,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:39,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:40,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:40,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:40,243 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:40,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:40,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:40,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:40,756 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:40,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:41,288 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:41,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:41,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:42,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:42,859 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:43,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:43,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:43,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:43,263 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:43,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:43,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:43,630 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:43,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:43,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:45,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:47,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:47,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:48,303 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:48,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:48,578 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:48,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:48,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:48,817 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:49,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:49,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,055 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:50,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:51,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:52,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:52,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:52,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:52,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:52,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:53,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:54,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:55,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:56,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:57,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:58,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:59,565 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:59,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:59,760 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:38:59,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:38:59,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:38:59,946 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:00,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:00,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:00,302 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:00,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:00,541 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:00,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:00,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:00,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:00,799 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:00,969 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:01,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:01,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:01,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:01,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:01,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:01,316 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:01,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:03,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:03,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:05,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:05,102 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:05,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:05,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:05,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:06,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:08,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:08,888 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:09,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:10,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:10,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:12,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:12,028 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:12,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:12,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:12,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:13,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:14,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:14,013 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:14,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:14,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:14,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:15,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:15,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:15,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:17,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:17,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:17,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:17,838 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:17,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:17,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:18,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:18,115 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:18,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:18,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:18,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:18,334 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:18,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:18,531 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:18,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:18,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:18,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:19,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:19,187 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:19,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:19,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:19,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:19,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:19,834 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:19,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:19,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:20,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:20,487 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:20,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:20,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:20,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:21,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:23,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:23,518 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:23,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:23,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:24,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:25,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:26,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:26,452 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:26,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:26,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:27,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:27,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:28,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:28,956 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:29,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:29,837 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:30,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:30,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:30,664 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:30,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:31,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:31,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:31,380 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:31,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:31,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:33,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:33,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:34,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:34,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:35,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:35,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:36,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:36,282 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:37,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:37,304 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:40,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:40,334 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:40,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:40,365 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:40,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:40,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:41,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:44,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:44,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:44,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:44,116 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:44,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:44,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:45,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:46,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:48,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:48,411 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:51,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:52,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:52,127 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:53,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:54,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:54,602 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:54,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:55,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:55,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:56,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:56,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:56,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:56,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:57,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:57,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:58,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:58,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:59,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:59,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:59,322 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:59,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:59,601 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:39:59,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:39:59,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:39:59,755 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:00,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:00,282 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:00,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:00,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:00,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:00,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:00,954 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:01,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:01,124 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:01,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:01,851 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:02,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:02,132 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:02,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:02,340 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:02,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:05,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:05,827 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:05,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:05,992 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:06,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:06,161 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:06,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:06,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:07,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:08,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:09,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:09,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:10,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:13,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:13,023 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:14,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:14,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:16,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:16,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:16,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:16,444 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:22,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:26,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:26,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:26,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:26,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:26,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:27,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:27,663 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:27,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:27,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:27,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:28,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:28,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:28,543 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:29,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:29,592 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:29,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:30,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:30,975 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:30,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:30,990 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:32,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:32,406 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:33,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:33,156 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:33,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:33,954 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:34,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:34,655 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:34,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:34,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:35,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:35,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:35,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:35,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:35,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:35,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:35,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:36,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:36,852 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:37,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:37,962 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:38,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:38,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:38,666 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:39,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:39,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:40,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:40,018 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:40,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:40,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:40,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:41,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:42,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:42,858 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:43,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:43,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:43,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:45,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:45,350 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:45,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:45,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:45,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:47,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:47,649 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:47,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:47,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:48,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:48,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:48,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:48,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:48,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:49,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:49,347 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:50,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:50,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:50,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:51,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:51,091 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:51,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:51,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:40:52,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:52,065 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:53,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:53,001 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:40:53,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:40:53,876 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:07,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:07,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:08,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:12,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:12,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:12,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:12,756 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:13,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:13,769 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:14,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:14,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:14,869 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:15,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:15,844 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:20,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:20,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:21,738 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:21,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:21,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:21,988 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:22,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:22,222 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:22,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:22,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:22,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:22,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:23,300 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:23,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:23,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:23,818 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:24,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:24,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:24,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:24,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:24,740 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:24,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:25,664 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:25,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:25,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:26,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:26,583 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:26,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:26,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:26,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:27,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:27,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:27,707 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:28,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:28,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:28,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:28,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:28,861 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:29,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:29,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:29,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:29,881 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:30,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:30,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:31,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:31,485 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:31,627 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:31,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:32,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:33,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:33,204 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:33,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:33,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:33,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:33,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110828 input tokens (60000 > 129024 - 110828). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:35,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:35,134 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:35,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:35,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:35,386 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110828 input tokens (60000 > 129024 - 110828). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:35,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:36,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:37,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:37,579 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:37,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:37,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:37,818 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110828 input tokens (60000 > 129024 - 110828). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:37,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:40,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:40,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:40,673 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:40,691 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:41,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:41,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:42,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:42,509 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:42,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:44,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:44,987 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:45,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:46,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:46,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:46,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:46,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:49,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:49,263 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:50,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:51,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:51,419 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:51,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:51,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:55,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:55,864 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:55,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:41:58,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:41:58,209 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:41:58,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:02,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:02,420 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:02,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:04,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:04,349 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:04,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:08,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:08,447 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:08,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:10,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:10,519 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:10,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:14,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:14,903 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:15,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:15,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:17,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:17,334 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:17,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:21,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:21,531 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:21,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:23,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:23,859 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:23,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:27,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:27,636 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:27,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:27,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:30,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:30,091 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:30,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:34,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:34,139 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:34,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:36,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:36,502 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:36,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:36,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:40,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:40,157 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:40,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:42,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:42,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:42,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:46,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:46,337 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:46,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:46,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:48,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:48,329 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:48,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:48,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:50,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:50,215 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:50,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:50,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:50,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:50,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:51,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:51,882 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:52,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:52,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:52,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:52,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:53,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:53,439 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:57,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:57,589 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:42:57,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:57,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:57,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:42:59,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:42:59,285 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:03,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:03,367 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:03,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:03,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:03,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:07,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:07,579 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:07,580 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:07,599 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:07,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:07,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:08,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:12,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:12,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:12,206 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:12,223 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:12,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:12,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:13,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:13,907 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:14,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:17,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:17,609 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:17,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:19,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:19,132 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:19,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:19,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:22,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:22,794 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:23,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:24,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:24,578 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:24,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:24,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:28,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:28,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:29,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:30,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:30,612 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:30,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:34,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:34,653 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:34,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:34,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:34,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:35,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:39,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:39,101 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:39,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:39,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:39,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:39,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:40,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:40,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:41,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:41,211 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:41,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:41,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:42,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:42,130 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:42,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:42,658 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:42,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:42,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:42,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:42,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:43,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:43,241 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:43,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:43,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:43,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:43,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:43,826 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:45,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:45,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:45,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:45,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:47,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:47,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:47,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:47,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:47,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:48,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:51,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:51,535 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:51,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:51,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:51,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:51,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:55,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:43:55,917 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:43:56,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:56,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:56,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:43:57,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:00,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:00,725 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:00,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:00,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:00,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:01,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:05,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:05,589 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:05,756 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:05,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:05,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:06,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:10,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:10,509 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:10,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:10,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:10,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:11,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:15,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:15,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:16,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:17,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:18,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:20,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:20,749 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:24,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:24,598 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:24,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:24,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:24,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:24,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:25,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:26,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:27,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:28,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:29,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:29,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:34,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:34,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:34,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:34,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:36,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:37,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:37,434 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:41,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:41,457 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:41,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:41,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:42,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:44,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:44,670 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:48,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:48,263 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:48,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:48,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:49,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:53,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:53,158 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:56,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:44:56,745 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:44:57,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:44:58,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:01,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:01,645 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:05,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:05,313 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:05,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:05,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:05,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:10,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:10,474 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:14,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:14,523 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:20,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:20,792 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:23,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:23,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:23,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:25,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:25,425 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:31,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:31,115 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:31,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:31,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:32,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:36,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:36,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:36,844 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:36,887 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:37,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:37,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 15:45:42,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:42,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 15:45:42,716 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3591046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 15:45:42,760 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:23,292 [INFO] HTTP Request: GET https://huggingface.co/api/datasets/livecodebench/code_generation_lite "HTTP/1.1 200 OK"
2025-12-10 16:00:32,934 [INFO] Indexed livecodebench rows: 1055
2025-12-10 16:00:36,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:36,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:37,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:49,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:49,497 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:54,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:00:54,940 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:00:55,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:55,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:00:55,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:07,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:07,297 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:07,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:07,336 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:08,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:08,924 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:09,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:14,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:14,360 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955828 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:14,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:26,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:26,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:26,401 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:26,415 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:32,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:32,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:32,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:32,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:36,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:36,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:36,602 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 577932 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:36,605 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:41,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:41,987 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:42,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:45,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:45,529 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:45,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:45,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:50,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:51,001 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:51,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:54,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:54,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:54,966 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:54,971 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:58,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:01:58,872 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2403912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:01:58,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:01:58,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:10,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:10,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:10,993 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:10,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8171955 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:14,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:14,917 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2403912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:15,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:15,225 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78404 input tokens (60000 > 129024 - 78404). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:15,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:15,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:27,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:27,365 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2403912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:27,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:27,403 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8171955 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:29,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:29,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:33,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:33,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3860138 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:33,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:46,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:46,205 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:46,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:46,212 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8171955 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:52,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:52,358 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3860138 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:52,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:02:52,895 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:02:56,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:02:56,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:06,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:06,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:06,648 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3860138 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:06,688 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:07,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:07,380 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78761 input tokens (60000 > 129024 - 78761). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:09,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:13,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:13,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:13,432 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:13,474 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:25,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:25,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:25,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:26,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:26,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:29,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:03:38,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:38,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:38,200 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:38,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:39,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:39,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:39,568 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:39,580 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:52,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:52,024 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:52,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:52,060 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7904733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:54,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:54,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:03:54,454 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:03:54,467 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:05,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:05,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:05,782 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2228059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:05,819 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7904733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:07,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:07,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:07,855 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 513049 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:07,882 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:19,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:19,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:19,534 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2228059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:19,568 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7904733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:22,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:22,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:22,684 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:22,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100440 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:29,263 [WARNING] gen_code timeout (attempt 1)
2025-12-10 16:04:34,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:34,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:34,815 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2228059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:34,853 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8159670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:38,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:38,948 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:39,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:44,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:04:51,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:51,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:51,356 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:51,395 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8159670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:54,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:04:54,715 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778116 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:04:54,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:06,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:06,350 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:06,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:06,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8159670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:06,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:07,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:07,437 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:07,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:19,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:19,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:19,008 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:19,045 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:20,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:20,365 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:20,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:20,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:32,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:32,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:32,469 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:32,505 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:33,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:33,458 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 255869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:33,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:33,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:45,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:45,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:45,320 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:45,354 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:46,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:46,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:46,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:57,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:57,909 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2333550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:57,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:05:57,948 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8150444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:05:58,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:05:59,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:02,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:02,099 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2398438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:02,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:14,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:14,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:14,443 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:14,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8150444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:18,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:18,521 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2398438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:18,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:18,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:18,913 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:18,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:30,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:30,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:30,667 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2398438 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:30,708 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8150444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:30,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:31,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:31,469 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89032 input tokens (60000 > 129024 - 89032). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:31,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:37,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:37,534 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:49,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:49,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:49,554 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:49,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8145249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:49,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:06:55,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:06:55,874 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:06:56,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:08,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:08,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:08,182 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:08,191 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8145249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:14,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:14,516 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956144 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:14,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:16,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:16,081 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 286775 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:16,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:28,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:28,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:28,669 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:28,705 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8145249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:29,643 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:29,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:33,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:33,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:33,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:44,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:44,996 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8134034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:45,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:07:49,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:07:49,161 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:07:49,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:01,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:01,661 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8134034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:01,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:01,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:05,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:05,465 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:05,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:17,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:17,986 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8134034 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:18,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:18,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:21,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:21,995 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:22,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:35,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:35,150 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:35,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:39,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:39,203 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2400345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:39,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:39,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:51,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:51,505 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:51,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:08:57,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:08:57,996 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:08:58,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:10,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:10,593 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133869 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:10,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:16,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:16,914 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:17,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:20,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:20,892 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414697 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:21,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:21,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:27,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:27,114 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956713 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:27,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:31,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:31,720 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414697 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:31,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:37,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:37,136 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:37,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:40,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:40,959 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2414697 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:41,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:09:47,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:09:47,416 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:09:47,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:00,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:00,321 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:00,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:06,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:06,944 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:06,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:06,985 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:07,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:19,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:19,552 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:19,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:25,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:25,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:25,722 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3838476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:25,758 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:25,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:37,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:37,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:37,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:10:43,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:43,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:43,786 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3838476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:43,825 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 539369 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:55,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:10:55,997 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:10:56,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:02,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:02,094 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3838476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:02,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:14,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:14,104 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:14,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:20,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:20,294 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3833006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:20,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:20,428 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:33,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:33,432 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:33,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:39,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:39,683 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3833006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:39,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:49,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:49,321 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6665821 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:49,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:49,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:11:55,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:11:55,756 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3833006 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:11:55,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:05,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:05,569 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6665821 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:05,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:05,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:12,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:12,505 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955879 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:12,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:24,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:24,027 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6665821 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:24,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:30,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:30,169 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955879 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:30,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:42,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:42,217 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:42,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:42,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:12:48,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:12:48,365 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955879 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:12:48,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:00,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:00,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:00,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:00,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:07,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:07,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:07,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:21,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:21,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:21,893 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:21,898 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:28,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:28,352 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:28,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:28,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:28,838 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:28,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:41,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:41,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:41,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:41,320 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8132677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:42,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:42,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 82065 input tokens (60000 > 129024 - 82065). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:42,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:47,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:47,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:47,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:13:59,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:13:59,398 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8132677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:13:59,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:05,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:05,798 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:05,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:05,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:18,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:18,325 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8132677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:18,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:24,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:24,797 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:24,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:24,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:25,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:25,444 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:25,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:30,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:30,980 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955657 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:31,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:31,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:31,440 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:31,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:43,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:43,134 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955657 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:43,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:43,174 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:43,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:43,987 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 128394 input tokens (60000 > 129024 - 128394). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:44,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:14:49,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:14:49,222 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955657 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:14:49,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:01,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:01,096 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:01,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:01,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:01,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:01,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:14,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:14,654 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:14,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:14,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:14,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:14,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:18,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:18,254 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1810298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:21,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:21,761 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1810298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:22,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:22,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:22,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:22,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:25,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:25,532 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1810298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:25,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:25,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:25,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:25,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:39,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:39,214 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:39,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:39,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:39,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:39,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:15:53,328 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:15:53,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:15:53,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:07,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:07,961 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133953 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:09,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:09,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:09,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:09,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:24,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:24,869 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133877 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:25,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:25,141 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:25,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:25,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:25,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:41,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:41,173 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133877 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:42,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:42,124 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:42,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:42,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:53,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:53,488 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133877 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:54,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:16:54,106 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:16:54,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:16:54,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:05,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:05,667 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:05,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:06,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:06,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:08,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:09,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:23,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:23,360 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:23,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:23,608 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:23,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:23,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:23,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:39,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:39,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8105853 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:40,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:40,351 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:40,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:52,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:52,814 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:53,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:17:53,366 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70530 input tokens (60000 > 129024 - 70530). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:17:53,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:53,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:17:53,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:10,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:10,470 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:10,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:10,730 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:11,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:12,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:16,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:23,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:23,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:23,206 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:23,218 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8133692 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:23,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:23,945 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90772 input tokens (60000 > 129024 - 90772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:24,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:24,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:24,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:41,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:41,300 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8147290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:41,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:41,556 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:41,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:41,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:41,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:58,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:58,501 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8147290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:59,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:59,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:18:59,454 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:59,462 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:18:59,579 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:18:59,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:15,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:15,974 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8147290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:16,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:16,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:16,235 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:16,242 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80169 input tokens (60000 > 129024 - 80169). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:22,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:22,240 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:22,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:22,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:22,769 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 199053 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:22,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:27,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:27,971 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:28,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:28,453 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:28,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:28,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:33,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:33,701 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3900336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:33,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:34,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:34,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:34,117 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:34,123 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:39,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:39,371 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:39,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:39,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:39,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:39,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:39,982 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120473 input tokens (60000 > 129024 - 120473). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:39,986 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:45,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:45,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:46,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:46,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:46,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:51,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:51,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:51,512 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90139 input tokens (60000 > 129024 - 90139). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:51,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:51,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:52,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:52,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:52,476 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:52,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:57,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:57,854 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:57,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:58,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:19:58,503 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:19:58,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:19:58,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:03,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:03,482 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:03,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:04,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:04,226 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:04,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:04,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:09,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:09,335 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3654686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:09,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:11,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:11,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:11,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:11,774 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:11,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:17,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:17,220 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:17,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:19,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:19,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:19,622 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:19,644 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:25,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:25,602 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:25,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:25,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:27,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:27,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:27,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:27,735 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 363129 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:33,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:33,819 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955674 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:33,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:34,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:34,282 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:34,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:40,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:40,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:40,369 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:40,381 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:44,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:44,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:44,116 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:44,142 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:50,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:50,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:50,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:50,222 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:50,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:20:53,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:53,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:53,947 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:53,977 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 140232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:59,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:59,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:20:59,545 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400472 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:20:59,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955982 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:02,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:02,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:05,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:05,153 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2162082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:05,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:05,173 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:05,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:10,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:11,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:11,238 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:11,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:16,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:16,780 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:19,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:20,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:20,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:20,464 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:20,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:20,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:26,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:26,057 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3655047 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:26,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:29,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:29,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:29,529 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:29,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:35,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:35,593 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:36,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:38,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:38,864 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:38,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:45,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:45,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:45,158 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1200267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:45,162 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:48,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:48,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:21:49,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:49,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:49,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:21:57,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:21:57,389 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:01,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:01,583 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178459 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:01,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:01,586 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:01,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:02,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:09,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:09,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:09,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:09,867 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:11,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:18,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:18,173 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:18,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:18,341 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 91614 input tokens (60000 > 129024 - 91614). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:19,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:19,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:19,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:27,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:27,095 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955949 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:27,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:27,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:27,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:27,331 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:28,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:33,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:33,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:33,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:33,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:33,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:33,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:39,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:39,537 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:41,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:42,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:42,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:42,843 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:42,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115326 input tokens (60000 > 129024 - 115326). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:42,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:48,401 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:48,402 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3940133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:49,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:51,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:51,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:51,501 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:51,532 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:57,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:57,114 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:57,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:22:57,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:22:57,590 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:22:59,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:03,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:03,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:03,334 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1988577 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:03,336 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:03,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:03,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:03,799 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:03,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:09,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:09,104 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:09,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:09,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:09,647 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:09,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:15,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:15,527 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:15,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:16,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:16,054 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:16,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:17,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:21,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:21,893 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:22,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:22,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:22,433 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:22,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:28,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:28,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:28,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:28,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:28,663 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:28,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:29,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:35,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:35,841 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:36,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:36,007 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:36,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:36,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:37,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:42,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:42,284 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:42,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:42,455 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94805 input tokens (60000 > 129024 - 94805). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:42,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:42,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:42,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:48,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:48,467 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3621139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:48,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:48,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:48,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:49,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:54,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:23:54,722 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:23:58,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:58,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:58,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:23:59,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:01,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:01,144 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:01,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:01,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:01,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:01,371 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:02,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:06,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:06,933 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3629580 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:07,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:07,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:07,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:07,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:13,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:13,331 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:13,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:13,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:13,661 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 122243 input tokens (60000 > 129024 - 122243). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:13,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:19,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:19,725 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:20,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:20,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:20,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:20,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:27,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:27,602 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956344 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:27,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:27,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:27,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:29,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:34,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:34,411 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:34,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:34,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:34,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:35,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:41,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:41,426 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:41,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:41,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:41,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:42,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:48,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:48,521 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955738 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:48,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:48,721 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:48,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:48,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:50,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:55,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:55,591 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:55,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:24:55,808 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:24:55,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:56,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:24:57,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:02,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:02,887 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:03,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:03,101 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 112083 input tokens (60000 > 129024 - 112083). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:03,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:03,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:04,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:10,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:10,486 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3868351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:10,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:10,809 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:10,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:11,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:11,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:18,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:18,024 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:18,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:18,352 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:18,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:18,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:20,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:26,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:26,326 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:26,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:26,658 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:26,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:26,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:26,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:33,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:33,054 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3735858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:36,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:36,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:36,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:39,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:39,696 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:39,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:40,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:40,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:46,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:46,383 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:46,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:47,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:47,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:47,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:54,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:25:54,387 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3668942 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:25:54,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:54,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:25:54,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:01,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:01,831 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:01,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:02,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:02,375 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:02,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:09,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:09,378 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:09,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:09,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:09,967 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:10,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:17,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:17,051 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955641 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:17,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:17,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:17,532 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200329 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:17,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:18,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:23,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:23,699 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:24,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:24,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:24,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:29,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:29,722 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:30,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:30,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:31,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:35,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:35,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:35,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:36,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:36,222 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:36,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:36,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:36,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:36,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:36,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:36,812 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:37,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:37,199 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262656 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:37,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:37,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:38,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:39,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:39,161 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:39,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:39,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:39,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:39,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:39,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:39,926 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:40,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:40,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:40,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:40,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:40,964 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:41,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:41,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:41,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:41,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:41,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:41,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:42,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:42,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:42,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:42,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:42,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:42,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:43,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:43,206 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:43,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:43,607 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:44,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:44,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:44,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:44,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:44,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:44,350 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:44,361 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:45,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:45,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:47,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:47,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:47,742 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:47,746 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178164 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:50,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:50,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:50,347 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:50,372 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 470848 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:50,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:50,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:54,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:54,482 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178164 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:57,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:57,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:26:57,357 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:57,385 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:26:57,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:26:57,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:01,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:01,084 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178164 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:03,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:03,523 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1804529 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:03,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:03,525 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:03,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:03,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:03,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:05,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:05,607 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:05,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:05,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:05,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:07,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:07,438 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:07,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:07,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:07,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:09,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:09,234 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:09,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:09,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:09,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:09,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:11,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:11,385 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:11,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:11,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:11,725 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:11,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:14,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:14,119 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:14,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:14,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:14,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:14,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:16,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:16,871 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:16,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:17,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:17,311 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:17,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:17,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:19,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:19,083 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:19,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:20,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:20,108 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:20,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:20,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:23,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:23,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:23,791 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:23,812 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:25,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:25,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:25,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:28,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:28,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:28,906 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:28,928 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:29,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:30,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:30,228 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:30,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:33,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:33,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:33,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:33,604 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:34,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:34,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:34,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:34,964 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:35,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:35,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:35,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:35,868 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:35,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:37,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:37,036 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:37,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:37,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:37,863 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:37,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:38,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:38,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:38,986 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 804447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:39,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:39,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:39,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:39,952 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:39,959 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:42,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:42,152 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:42,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:42,479 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:42,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:42,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:44,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:44,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:44,138 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:44,160 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:44,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:44,472 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:44,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:44,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:44,625 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:45,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:46,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:46,426 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:46,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:46,455 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:47,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:47,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:47,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:48,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:48,260 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:48,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:48,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:48,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:50,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:50,457 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:50,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:50,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:50,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:50,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:50,703 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85759 input tokens (60000 > 129024 - 85759). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:52,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:52,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:52,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:52,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:52,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:54,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:54,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:54,360 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:54,384 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85759 input tokens (60000 > 129024 - 85759). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:54,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:54,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:54,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:54,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:54,779 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:54,788 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 85759 input tokens (60000 > 129024 - 85759). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:56,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:56,589 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:56,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:56,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:56,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:56,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:56,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:56,963 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77203 input tokens (60000 > 129024 - 77203). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:58,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:58,887 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:58,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:27:59,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:27:59,328 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 183456 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:27:59,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:00,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:00,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:00,989 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:01,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77203 input tokens (60000 > 129024 - 77203). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:01,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:02,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:02,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:02,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:04,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:04,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:04,360 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:04,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 77203 input tokens (60000 > 129024 - 77203). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:05,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:05,258 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:05,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:05,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:07,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:07,843 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:07,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:08,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:08,804 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:08,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:10,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:10,505 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:10,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:10,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:10,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:12,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:12,321 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:12,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:12,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:12,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:14,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:14,135 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:14,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:14,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:14,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:14,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:16,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:16,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:16,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:16,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:16,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:16,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:18,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:18,948 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:19,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:19,094 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101840 input tokens (60000 > 129024 - 101840). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:19,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:19,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:19,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:21,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:21,614 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:21,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:21,922 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:21,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:21,930 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101840 input tokens (60000 > 129024 - 101840). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:22,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:22,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:23,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:23,917 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:24,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:24,225 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:24,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:24,232 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101840 input tokens (60000 > 129024 - 101840). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:24,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:24,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:26,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:26,474 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:26,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:26,787 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177358 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:26,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:26,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:26,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:28,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:28,962 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1209872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:29,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:29,326 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200379 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:29,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:29,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:29,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:31,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:31,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:32,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:32,050 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200379 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:32,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:32,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:34,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:34,251 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:34,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:34,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:34,695 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200379 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:34,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:36,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:36,338 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:36,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:37,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:37,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:38,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:38,737 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:38,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:38,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:38,916 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75757 input tokens (60000 > 129024 - 75757). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:39,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:40,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:40,615 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:40,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:40,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:40,826 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75757 input tokens (60000 > 129024 - 75757). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:40,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:42,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:42,445 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:42,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:42,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:42,657 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75757 input tokens (60000 > 129024 - 75757). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:42,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:43,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:43,046 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:43,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:43,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:43,500 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:43,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:44,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:44,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:44,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:44,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:44,339 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300748 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:44,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:44,843 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:45,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:45,308 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:45,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:45,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:45,682 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 157540 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:47,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:47,756 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:50,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:50,096 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:52,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:52,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1159683 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:53,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:53,969 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:55,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:55,922 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:57,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:28:57,930 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004016 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:28:59,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:59,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:59,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:28:59,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:00,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:00,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:00,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:00,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:00,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:00,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:02,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:02,622 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:06,401 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:06,402 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:06,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:06,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:06,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:08,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:08,936 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289274 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:12,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:12,688 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:13,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:13,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:13,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:13,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:16,044 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:16,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:16,274 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:16,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:16,761 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:16,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:16,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:17,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:17,076 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177291 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:17,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:17,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:17,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:17,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:17,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:18,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:18,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:28,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:28,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:28,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:28,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:29,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:30,557 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:30,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:30,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:30,915 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:31,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:31,288 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:31,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:31,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:32,773 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:32,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:32,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:33,117 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:33,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:33,499 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 177267 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:33,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:33,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:34,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:35,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:36,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,598 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:37,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:38,518 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:38,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:38,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:38,881 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:39,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:39,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:39,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:39,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:39,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:39,245 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:39,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:39,995 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:40,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:40,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:40,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:40,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:40,337 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:41,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:41,109 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:41,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:41,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:41,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:41,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:41,488 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:42,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:42,256 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 440519 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:42,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:42,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:42,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:42,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:42,756 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:43,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:43,444 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:43,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:43,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:43,775 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:43,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:44,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:44,592 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:44,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:44,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:44,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:44,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:45,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:45,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:45,836 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 492402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:45,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:46,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:46,158 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100300 input tokens (60000 > 129024 - 100300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:46,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:48,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:48,856 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:49,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:49,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:49,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:49,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:51,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:51,775 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:52,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:52,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:52,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:52,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:54,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:54,096 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1644689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:54,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:54,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:54,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:54,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:57,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:29:57,019 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:29:57,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:57,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:57,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:29:58,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:00,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:00,016 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:00,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:00,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:00,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:00,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:03,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:03,088 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1869022 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:03,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:03,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:03,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:03,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:06,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:06,310 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:06,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:06,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:06,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:06,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:09,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:09,507 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:09,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:09,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:16,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:16,365 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4479854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:16,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:19,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:19,757 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1993113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:19,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:26,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:26,239 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4479854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:26,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:26,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:29,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:29,383 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:29,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:36,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:36,219 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4479854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:36,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:39,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:39,113 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:39,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:39,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:46,375 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:46,377 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:46,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:49,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:49,613 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:49,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:30:57,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:30:57,197 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:30:57,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:00,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:00,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:00,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:08,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:08,667 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:08,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:08,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:12,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:12,698 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:12,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:18,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:18,932 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4064191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:19,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:19,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:22,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:22,368 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2093112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:22,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:29,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:29,711 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4064191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:29,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:33,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:33,308 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:33,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:33,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:39,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:39,261 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4064191 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:39,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:42,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:42,859 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:42,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:47,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:47,765 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185507 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:47,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:47,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:50,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:50,851 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2100245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:50,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:55,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:31:55,553 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185507 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:31:55,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:55,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:56,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:31:56,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:01,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:01,338 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3185507 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:01,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:01,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:01,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:01,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:08,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:08,771 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4348489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:08,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:08,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:08,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:09,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:16,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:16,468 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4348489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:19,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:19,310 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:19,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:19,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:19,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:27,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:27,528 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4348489 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:30,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:30,422 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:30,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:30,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:30,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:37,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:37,211 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3471653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:40,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:40,066 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1710422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:40,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:40,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:40,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:46,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:46,551 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3471653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:50,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:50,955 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578356 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:51,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:51,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:51,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:56,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:56,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:32:56,702 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578356 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:56,737 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3471653 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:32:57,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:32:57,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:00,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:00,647 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578356 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:00,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:04,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:04,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2899773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:04,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:07,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:07,357 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:07,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:07,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:11,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:11,388 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2899773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:11,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:14,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:14,268 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:14,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:18,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:18,837 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2899773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:18,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:18,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:21,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:21,441 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1730017 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:21,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:27,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:27,291 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3717680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:27,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:31,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:31,039 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114533 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:31,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:36,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:36,806 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3717680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:36,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:40,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:40,526 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114533 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:40,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:40,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:46,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:46,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3717680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:46,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:50,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:50,243 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2114533 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:50,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:33:57,187 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:33:57,188 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078172 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:33:57,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:01,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:01,570 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578211 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:01,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:01,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:08,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:08,631 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078172 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:08,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:13,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:13,066 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578211 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:13,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:13,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:20,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:20,494 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078172 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:20,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:25,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:25,303 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578211 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:25,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:31,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:31,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:31,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:34,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:34,410 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:34,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:40,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:40,832 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:40,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:44,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:44,245 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:44,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:44,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:50,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:50,467 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:50,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:54,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:54,095 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1840742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:54,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:34:58,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:34:58,907 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2902694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:34:59,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:01,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:01,627 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:01,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:01,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:05,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:05,670 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2902694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:05,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:08,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:08,262 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:08,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:08,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:13,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:13,231 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2902694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:13,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:16,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:16,252 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1620030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:16,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:22,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:22,342 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3531192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:22,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:26,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:26,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:26,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:31,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:31,959 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3531192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:32,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:32,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:36,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:36,574 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:36,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:43,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:43,187 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3531192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:43,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:43,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:47,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:47,747 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2236224 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:47,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:56,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:56,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:56,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:59,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:35:59,842 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:35:59,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:35:59,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:07,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:07,293 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:07,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:11,844 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:11,845 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:11,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:18,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:18,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3767246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:18,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:22,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:22,439 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1881426 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:22,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:22,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:29,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:29,728 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:29,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:34,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:34,932 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:35,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:42,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:42,336 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:42,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:46,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:46,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:47,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:54,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:54,021 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3771188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:54,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:36:59,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:36:59,045 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578160 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:36:59,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:04,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:04,350 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3075686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:04,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:04,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:09,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:09,321 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:09,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:14,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:14,564 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3075686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:14,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:14,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:19,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:19,917 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:20,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:27,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:27,120 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3075686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:27,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:32,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:32,357 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:32,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:32,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:34,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:34,572 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:34,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:37,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:37,803 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:37,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:37,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:40,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:40,120 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:40,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:43,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:43,792 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:43,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:43,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:46,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:46,422 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289585 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:46,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:50,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:50,432 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1806518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:50,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:37:59,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:37:59,010 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:37:59,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:04,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:04,606 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578155 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:04,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:12,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:12,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:12,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:12,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:16,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:16,912 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578155 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:17,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:25,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:25,209 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:25,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:29,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:29,539 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578155 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:29,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:29,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:38,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:38,218 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:38,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:41,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:41,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:41,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:41,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:50,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:50,389 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:50,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:53,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:38:53,146 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:38:53,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:38:53,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:01,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:01,833 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4659772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:01,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:04,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:04,650 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572553 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:04,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:11,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:11,078 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3479933 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:11,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:15,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:15,901 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:16,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:21,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:21,435 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3479933 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:21,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:25,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:25,775 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:25,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:25,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:31,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:31,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3479933 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:32,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:37,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:37,005 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:37,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:45,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:45,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:45,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:49,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:49,841 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578640 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:49,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:49,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:39:58,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:39:58,299 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:39:58,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:02,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:02,707 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578640 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:02,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:10,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:10,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:10,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:10,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:15,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:15,308 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578640 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:15,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:15,348 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:17,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:17,771 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:17,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:17,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:21,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:21,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:21,658 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:21,694 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:24,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:24,274 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:24,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:24,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:28,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:28,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:28,472 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:28,507 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789227 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:30,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:30,964 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289606 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:31,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:31,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:35,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:35,362 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2324920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:35,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:35,393 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:35,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:35,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:39,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:39,515 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:39,627 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:40,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:40,837 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:40,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:46,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:46,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:46,082 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:46,116 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3472107 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:47,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:47,861 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:47,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:51,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:51,981 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2326382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:52,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:40:57,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:57,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:40:57,217 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3472107 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:40:57,261 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:01,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:01,287 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:01,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:01,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:06,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:06,476 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3472107 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:06,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:06,546 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:10,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:10,837 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:10,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:19,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:19,537 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:19,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:19,595 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 900332 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:19,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:24,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:24,089 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:24,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:31,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:31,895 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:32,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:35,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:35,509 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2354959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:35,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:43,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:43,778 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4956241 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:43,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:43,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:47,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:47,991 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2354959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:48,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:55,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:55,578 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4678232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:55,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:55,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:41:59,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:41:59,647 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2354959 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:41:59,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:08,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:08,353 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4678232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:08,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:12,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:12,882 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:13,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:13,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:20,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:20,313 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4678232 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:20,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:25,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:25,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:25,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:25,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:25,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:25,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:29,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:29,674 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:29,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:30,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:30,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:33,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:33,693 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:36,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:36,712 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:37,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:37,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:37,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:37,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:41,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:41,803 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2025582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:43,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:43,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:43,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:45,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:45,426 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:45,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:45,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:45,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:45,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:48,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:48,532 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:48,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:48,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:48,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:50,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:50,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:50,745 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:50,782 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:52,432 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:52,433 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:53,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:53,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:53,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:56,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:56,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:58,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:42:58,029 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1115112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:42:58,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:58,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:42:58,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:01,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:01,823 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:02,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:02,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:02,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:06,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:06,512 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2577665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:06,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:06,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:06,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:06,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:09,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:09,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:09,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:09,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:09,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:12,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:12,071 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:12,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:12,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:12,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:14,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:14,380 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1590841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:14,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:15,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:15,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:15,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:18,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:18,412 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:18,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:18,448 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:18,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:18,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:18,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:18,835 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:18,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:22,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:22,071 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:22,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:22,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:22,353 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 81240 input tokens (60000 > 129024 - 81240). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:22,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:26,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:26,271 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:26,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:26,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:26,865 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:26,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:29,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:29,074 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:29,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:29,706 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:30,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:30,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:30,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:31,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:31,456 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:32,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:32,045 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 343827 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:32,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:32,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:32,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:33,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:33,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1245906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:34,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:34,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:34,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:34,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:37,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:37,226 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948635 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:38,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:38,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:38,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:40,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:40,831 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948635 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:43,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:44,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:44,166 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1948635 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:47,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:47,135 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:49,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:49,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:49,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:50,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:50,181 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:51,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:51,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:51,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:51,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:54,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:54,498 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1856414 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:56,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:43:56,978 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:43:58,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:43:59,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:00,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:00,121 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:00,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:02,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:02,697 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:02,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:05,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:05,256 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:07,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:07,652 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:08,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:09,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:09,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:09,749 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1397710 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:09,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:13,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:13,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:13,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:13,962 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:14,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:14,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:18,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:18,826 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:19,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:20,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:20,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:20,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:23,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:23,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578218 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:23,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:23,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:23,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:23,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:25,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:25,696 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1385967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:27,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:28,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:28,961 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1385967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:29,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:29,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:29,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:29,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:31,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:31,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1385967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:32,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:33,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:33,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:33,974 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:34,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:34,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:34,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:37,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:37,031 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:37,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:37,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:37,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:37,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:39,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:39,692 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1818215 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:40,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:40,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:40,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:40,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:41,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:41,888 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:44,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:44,908 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:45,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:45,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:46,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:47,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:47,331 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:49,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:49,734 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:50,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:50,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:51,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:52,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:52,221 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:54,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:54,519 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1767075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:54,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:54,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:55,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:56,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:56,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:56,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:56,926 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:57,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:57,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:57,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:44:58,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:58,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:58,567 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:58,588 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:44:58,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:44:58,905 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 97981 input tokens (60000 > 129024 - 97981). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:00,969 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:00,971 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1069830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:02,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:02,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:02,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:03,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:04,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:04,603 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:05,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:05,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:05,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:05,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:08,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:08,552 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:08,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:08,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:08,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:08,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:13,059 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2578174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:13,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:13,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:15,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:15,824 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:16,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:16,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:16,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:16,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:17,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:17,339 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:17,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:17,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:17,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:17,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:18,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:18,854 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1110343 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:19,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,844 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:19,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,560 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,643 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:20,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:21,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:21,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:21,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:21,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:21,981 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:22,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:22,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:22,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:22,202 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:22,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:23,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:23,391 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:23,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:23,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:23,582 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:23,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:24,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:24,724 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 675214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:24,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:24,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:25,004 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114763 input tokens (60000 > 129024 - 114763). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:25,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:25,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:25,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:25,698 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:25,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:25,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:26,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166582 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:26,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:26,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:27,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:27,368 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:27,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:27,869 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:27,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:28,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:28,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:28,386 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:33,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:33,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:39,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:39,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:39,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:39,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:40,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:43,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:45,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:50,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:55,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:56,096 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:56,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:56,453 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:56,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:56,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:56,822 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200248 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:57,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:45:57,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:57,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:57,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:57,478 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:57,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:45:57,663 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70657 input tokens (60000 > 129024 - 70657). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:45:57,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:03,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:03,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:03,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:03,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:04,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:04,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:06,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:07,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:08,534 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:08,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:08,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:09,022 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:09,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:09,540 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:09,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:09,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:10,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:10,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:10,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:10,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:14,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:20,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:21,172 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:21,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:21,512 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:21,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:21,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:21,871 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:22,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:22,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:22,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:22,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:22,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:22,843 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:23,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:23,353 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:23,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:23,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:23,863 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 282245 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:28,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:28,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:35,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:36,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:36,281 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:36,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:36,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:36,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:36,638 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:36,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:36,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:36,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:37,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:37,017 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:38,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:38,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:38,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:38,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:43,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:43,930 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:44,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:44,160 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:44,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:44,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100462 input tokens (60000 > 129024 - 100462). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:44,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:44,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:45,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:46,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:47,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:49,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:50,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:50,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:50,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:50,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:51,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:51,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:51,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:51,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:52,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:52,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:55,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:55,766 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3067648 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:46:56,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:56,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:56,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:46:59,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:46:59,980 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3067648 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:00,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:00,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:01,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:04,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:04,604 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3067648 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:05,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:05,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:06,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:08,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:10,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:10,991 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4156113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:11,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:11,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:12,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:16,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:16,652 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4156113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:17,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:17,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:17,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:22,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:22,594 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4156113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:23,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:23,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:23,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:23,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:24,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:25,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:25,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:25,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:26,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:26,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:26,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:26,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:27,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:28,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:32,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:32,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:32,136 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 156499 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:32,142 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:32,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:32,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:32,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:32,619 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 156499 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:33,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:36,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:36,217 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:36,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:36,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:36,612 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 156499 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:36,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:40,210 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196546 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:40,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:40,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:41,046 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 158565 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:41,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:41,438 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 158565 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:41,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:41,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:41,814 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 158565 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:41,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:50,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:50,124 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334307 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:50,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:50,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:50,577 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 129195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:50,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:55,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:47:59,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:59,441 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334307 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:59,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:47:59,664 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 129195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:47:59,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:08,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:08,786 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334307 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:08,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:08,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:09,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:09,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 129195 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:09,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:09,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:09,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:09,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:09,862 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:10,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:10,584 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:10,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:10,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:10,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:10,900 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 130054 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:10,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:11,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:11,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:11,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:11,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:12,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:13,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:13,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:13,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:14,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:14,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:14,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:14,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:14,847 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 460132 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:15,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:15,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:15,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:15,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:15,794 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 460132 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:15,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:15,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:15,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:16,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:16,809 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 460132 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:17,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:17,507 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:17,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:17,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:17,777 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:17,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:18,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:18,510 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:18,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:18,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:18,798 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:18,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:19,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:19,576 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:19,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:19,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:19,892 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132295 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:20,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:20,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:20,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:20,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:20,951 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 450363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:21,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:21,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:21,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:21,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:21,818 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 450363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:24,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:25,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:25,566 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:25,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:25,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:26,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:26,571 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 450363 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:26,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:30,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:30,158 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:30,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:30,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:33,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:33,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:34,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:34,055 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2196209 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:34,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:34,284 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:34,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:34,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:34,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:34,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:34,726 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:35,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:35,053 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:35,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:35,969 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 595364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:36,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:36,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:36,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:36,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:36,966 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 595364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:37,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:37,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:37,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:37,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:37,836 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 595364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:38,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:38,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:39,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:41,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:48,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:48,244 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:48:48,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:48,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:48,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:48:57,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:48:57,078 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:01,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:01,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:05,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:05,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:05,968 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6334059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:06,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:06,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:06,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:07,004 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:07,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:07,814 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:07,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:07,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:08,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:08,765 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 518474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:09,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:09,528 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 401000 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:09,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:10,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:10,445 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 518474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:10,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:10,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:10,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:11,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:11,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:11,569 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 518474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:12,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:12,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:12,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:13,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:13,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:13,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:13,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:14,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:14,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:14,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:14,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:14,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:14,799 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 443451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:15,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:15,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:15,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:15,689 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 443451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:15,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:15,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:15,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:16,515 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 443451 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:16,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:16,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:17,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:17,945 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 629104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:20,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:20,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389052 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:20,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:20,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:21,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:21,548 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 629104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:21,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:23,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:23,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389052 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:23,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:25,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:25,066 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 629104 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:25,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:27,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:27,715 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389052 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:27,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:28,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:28,510 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:28,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:31,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:31,171 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:31,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:31,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:31,823 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:31,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:33,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:33,929 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:34,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:34,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:34,585 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:34,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:35,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:36,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:36,661 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389082 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:36,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:37,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:37,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:38,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:39,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:39,422 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:40,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:41,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:42,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:42,096 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:42,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:42,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:42,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:44,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:44,449 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:44,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:44,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:45,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:45,555 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 454576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:47,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:47,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:47,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:48,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:48,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:48,887 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 454576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:48,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:51,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:51,307 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:52,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:52,087 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 454576 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:52,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:52,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:54,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:54,848 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:55,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:55,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:55,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:55,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:57,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:57,444 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388880 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:57,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:49:57,655 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 161634 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:49:57,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:49:57,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:00,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:00,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:00,369 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388880 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:00,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:00,657 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 161634 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:00,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:00,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:03,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:03,139 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388880 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:03,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:03,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:03,501 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 161634 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:03,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:03,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:04,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:04,613 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:04,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:05,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:05,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:06,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:06,117 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:06,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:06,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:06,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:07,549 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:07,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:07,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:10,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:10,061 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1339628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:10,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:10,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:10,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:10,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:12,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:12,555 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1339628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:12,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:12,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:13,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:14,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:14,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:14,422 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1339628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:14,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:14,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:14,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:16,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:16,746 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1341492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:16,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:17,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:17,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:18,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:18,626 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1341492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:18,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:18,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:18,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:18,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:21,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:21,075 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1341492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:21,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:21,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:21,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:24,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:24,437 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:24,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:24,465 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:24,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:26,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:26,540 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:26,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:26,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:29,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:29,338 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:29,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:31,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:31,515 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:31,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:34,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:34,302 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2051892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:36,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:36,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:36,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:36,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:38,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:38,150 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:39,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:39,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:39,883 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:39,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:41,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:41,666 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:41,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:42,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:42,969 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 932638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:44,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:44,672 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:44,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:47,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:47,116 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:47,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:47,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:47,366 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:47,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:49,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:49,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:49,928 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:50,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:50,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:50,192 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:50,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:52,187 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:52,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1389091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:52,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:52,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:52,459 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90284 input tokens (60000 > 129024 - 90284). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:52,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:52,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:52,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:52,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:53,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:53,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:54,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:54,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:54,754 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:54,759 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 897404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:55,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:55,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:55,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:55,506 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:57,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:57,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:57,039 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:57,044 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 897404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:57,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:57,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:57,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:57,670 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:57,675 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389721 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:59,206 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:50:59,208 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 897404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:50:59,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:50:59,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:02,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:02,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:02,033 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:02,038 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:04,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:04,531 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:04,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:04,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:04,775 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:06,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:07,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:07,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:07,530 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:07,552 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:07,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:07,858 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:08,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:10,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:10,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:10,372 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1778108 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:10,375 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388746 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:10,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:10,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:10,780 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:10,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:11,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:11,484 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:11,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:11,652 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:12,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:12,182 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:12,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:12,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100230 input tokens (60000 > 129024 - 100230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:12,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:12,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:12,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:13,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:13,119 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400796 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:13,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:13,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:15,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:15,059 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:15,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:17,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:17,557 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:17,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:20,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:20,166 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:20,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:22,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:22,164 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:22,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:24,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:24,588 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1556083 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:24,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:26,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:26,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388952 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:27,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:30,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:30,711 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:30,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:32,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:32,774 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1120400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:32,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:35,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:35,694 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:35,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:37,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:37,373 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1120400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:37,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:40,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:40,740 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:40,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:40,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:43,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:43,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1120400 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:43,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:43,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:43,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:43,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:45,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:45,720 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:45,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:45,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:46,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:46,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:48,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:48,099 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:48,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:48,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:48,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:48,305 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:48,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:51,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:51,118 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1388712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:51,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:51,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:51,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:51,391 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:51,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:51,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:51,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:51,600 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70277 input tokens (60000 > 129024 - 70277). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:51,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:52,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:52,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:52,852 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:53,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:53,776 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:54,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:54,802 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 494941 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:55,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:55,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:55,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:55,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:56,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:56,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:56,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:56,990 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:56,995 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:57,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:57,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:57,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:57,285 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:58,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:58,338 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:58,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:58,616 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100275 input tokens (60000 > 129024 - 100275). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:51:59,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:59,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:51:59,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:51:59,702 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:00,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:00,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:02,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:02,372 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:02,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:02,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:02,614 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:02,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:03,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:05,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:05,268 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:05,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:05,396 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:07,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:07,694 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1740807 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:07,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:07,995 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70281 input tokens (60000 > 129024 - 70281). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:09,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:09,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:09,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:10,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:10,621 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:11,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:11,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:11,534 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:11,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:14,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:14,634 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:15,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:15,378 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:15,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:15,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:16,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:18,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:18,437 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1955873 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:19,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:19,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373220 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:19,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:19,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:19,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:19,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:19,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:20,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:20,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:20,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:21,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:22,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:22,309 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:23,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:23,308 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:23,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:24,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:24,573 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 700279 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:25,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:25,285 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:25,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:25,993 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:26,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:26,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389700 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:27,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:27,855 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:28,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:29,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:29,001 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:30,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:30,108 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:30,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:30,839 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:31,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:31,556 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:32,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:32,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 389620 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:32,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:32,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:32,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:32,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:35,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:35,057 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:37,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:38,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:38,322 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:40,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:40,666 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1600283 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:40,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:40,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:40,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:40,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:41,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:41,889 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:42,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:42,906 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:43,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:43,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:43,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:44,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:44,002 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:47,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:47,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:48,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:51,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:52,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:52,468 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:52,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:52,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:52,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:52,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:53,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:53,387 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:53,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:53,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:53,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:53,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:54,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:52:54,369 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 512293 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:52:55,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:52:56,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:02,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:02,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:02,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:02,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:03,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:03,014 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:03,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:03,269 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:03,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:03,553 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107833 input tokens (60000 > 129024 - 107833). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:03,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:03,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:03,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:04,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:04,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:04,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:04,776 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:05,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:05,410 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:05,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:06,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:06,440 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:07,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:07,555 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:08,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:08,612 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 483784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:09,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:09,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:10,401 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:10,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:11,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:11,181 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:12,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:12,536 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 530334 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:14,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:14,471 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:16,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:16,403 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:18,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:18,071 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 812868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:20,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:20,446 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:22,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:22,864 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:23,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:23,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:23,432 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:25,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:25,352 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1173461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:25,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:25,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:25,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:26,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:27,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:27,448 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:27,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:27,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:27,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:29,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:29,648 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:29,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:29,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:30,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:30,733 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:33,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:33,131 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289652 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:33,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:34,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:34,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:34,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:35,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:36,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:36,740 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:36,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:37,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:37,668 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:37,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:40,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:40,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:40,683 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:40,710 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:41,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:41,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:41,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:44,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:44,701 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:46,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:46,487 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:46,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:46,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:46,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:50,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:50,280 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1748305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:52,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:52,059 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:52,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:52,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:52,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:57,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:57,166 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:59,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:53:59,592 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:53:59,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:59,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:53:59,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:03,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:03,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:03,509 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:03,543 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:03,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:03,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:06,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:06,435 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:10,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:10,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:10,959 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667301 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:11,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:13,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:13,602 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:13,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:17,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:17,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:17,706 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:17,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:20,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:20,080 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:20,187 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:21,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:21,438 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:21,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:25,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:25,648 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:25,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:25,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:26,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:26,624 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 542532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:26,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:30,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:30,462 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667378 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:30,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:30,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:30,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:30,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:35,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:35,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:36,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:36,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:36,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:36,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:41,107 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:41,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:41,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:45,149 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667600 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:45,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:45,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:47,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:47,328 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:47,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:47,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:47,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:48,432 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:48,434 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:48,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:48,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:48,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:49,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:49,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:49,699 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789825 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:54,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:54,036 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:54:54,174 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:54,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:54,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:54:55,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:54:55,920 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:00,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:00,004 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:00,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:00,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:00,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:02,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:02,342 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:05,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:05,915 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667401 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:06,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:06,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:06,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:08,055 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:08,057 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089671 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:11,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:11,691 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:11,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:11,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:11,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:13,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:13,890 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:17,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:17,471 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:17,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:17,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:17,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:19,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:19,766 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:23,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:23,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:23,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:23,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:23,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:27,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:27,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:27,761 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:27,796 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089679 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:28,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:28,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:29,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:32,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:32,525 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:32,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:32,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:32,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:36,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:36,815 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:37,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:37,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:37,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:37,968 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:38,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:38,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:38,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:38,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:39,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:39,478 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:39,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:39,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:39,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:40,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:40,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:40,814 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600672 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:43,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:43,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:44,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:44,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:44,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:44,931 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:44,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:48,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:48,128 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:48,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:48,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:48,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:48,751 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:49,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:51,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:51,559 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1980492 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:51,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:52,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:52,232 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 328830 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:52,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:55:57,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:55:57,043 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:55:57,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:01,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:01,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:01,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:05,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:05,848 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:05,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:09,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:09,809 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:10,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:14,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:14,307 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3071684 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:14,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:19,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:19,586 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2664542 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:19,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:26,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:26,367 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:31,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:31,734 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:35,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:35,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:39,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:39,121 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:39,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:43,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:43,967 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:44,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:49,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:49,791 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3644203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:50,643 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:56:54,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:56:54,389 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667010 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:56:54,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:01,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:01,258 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3591046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:01,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:02,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:06,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:06,404 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:06,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:12,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:12,370 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3591046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:12,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:13,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:17,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:17,185 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:17,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:24,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:24,685 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3591046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:24,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:25,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:30,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:30,468 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:30,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:37,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:37,986 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666666 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:38,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:38,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:38,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:40,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:44,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:44,878 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666666 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:45,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:46,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:51,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:51,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:51,084 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:51,120 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666666 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:57:53,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:53,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:53,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:57:56,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:57:56,327 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:02,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:02,416 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:04,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:07,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:07,255 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:11,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:11,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:13,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:13,067 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:13,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:17,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:17,852 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:18,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:24,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:24,960 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667199 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:25,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:29,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:29,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:30,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:36,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:36,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3689210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:38,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:38,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:41,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:41,606 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667131 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:41,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:49,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:49,084 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3689210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:49,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:58:54,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:58:54,054 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:58:54,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:00,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:00,344 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3689210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:00,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:05,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:05,829 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:05,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:06,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:13,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:13,199 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4451057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:13,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:17,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:17,891 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:18,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:25,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:25,311 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4451057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:26,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:30,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:30,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:30,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:38,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:38,620 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4451057 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:39,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:43,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:43,591 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:47,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:47,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:51,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:51,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 16:59:52,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 16:59:56,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 16:59:56,388 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:05,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:05,149 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:05,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:05,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:10,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:10,226 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:19,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:19,126 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:23,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:23,812 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:23,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:23,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:24,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:33,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:33,591 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:37,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:37,944 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2667442 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:38,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:38,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:38,395 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 275779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:39,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:46,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:46,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:46,848 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:46,885 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:47,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:47,695 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 275779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:49,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:00:55,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:55,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:55,508 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:55,545 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:56,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:56,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:00:56,214 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 168410 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:56,221 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 275779 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:00:56,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:04,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:04,331 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2700680 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:04,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:04,331 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5289210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:05,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:05,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:05,271 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 168410 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:05,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:13,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:13,710 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5289210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:14,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:14,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:14,297 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 168410 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:14,305 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:14,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:14,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:24,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:24,565 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5289210 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:24,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:24,919 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:25,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:25,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:25,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274658 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:25,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:25,642 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:26,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:27,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:29,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:29,184 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:29,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:29,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:29,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:29,647 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197114 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:29,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:30,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:33,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:33,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:33,128 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:33,153 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:33,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:33,808 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:34,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:37,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:37,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:37,390 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:37,410 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 274731 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:38,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:38,018 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:38,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:39,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:40,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:40,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:40,026 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:40,041 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 271139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:40,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:40,723 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 378386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:43,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:43,222 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:44,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:44,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 874513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:44,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:44,946 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 271139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:48,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:48,943 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:49,285 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:49,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:50,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:50,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:50,663 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 874513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:50,681 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 271139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:51,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:51,359 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:52,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:53,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:53,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:53,198 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 874513 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:53,225 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309758 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:53,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:53,882 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:54,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:54,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:57,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:57,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:57,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1764452 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:57,239 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309758 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:57,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:57,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:58,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:01:58,460 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 309758 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:01:59,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:01:59,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:01,174 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:01,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:01,177 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:01,201 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1764452 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:02,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:02,245 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:03,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:05,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:05,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:05,274 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:05,298 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1764452 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:06,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:06,282 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:10,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:10,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:10,204 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:10,229 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2649098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:11,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:11,678 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:13,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:14,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:16,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:16,298 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1302103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:16,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:16,323 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2649098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:17,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:17,291 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:21,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:21,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1302103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:21,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:21,565 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2649098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:22,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:22,841 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:26,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:26,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:26,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:26,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1302103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:27,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:27,020 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:28,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:28,010 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:30,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:30,119 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:34,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:34,045 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:34,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:34,082 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:37,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:37,112 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:37,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:38,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:41,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:41,626 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:41,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:41,662 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:43,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:43,642 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:44,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:48,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:48,281 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:48,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:48,317 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 541328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:49,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:52,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:52,953 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:52,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:52,989 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 540871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:57,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:02:57,524 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978497 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:02:57,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:02:57,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:01,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:01,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:01,967 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:02,009 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 540871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:05,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:05,187 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978497 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:06,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:06,197 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 540871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:11,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:11,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:11,174 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978497 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:11,200 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:13,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:14,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:14,977 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1607035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:15,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:19,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:19,611 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978596 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:19,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:19,638 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:22,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:22,138 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1607035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:22,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:25,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:25,413 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978596 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:26,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:30,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:30,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:30,401 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:30,439 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1607035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:34,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:34,269 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978596 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:38,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:39,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:40,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:40,874 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:40,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:40,895 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3199317 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:42,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:42,705 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 830434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:43,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:47,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:47,816 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:48,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:53,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:53,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:53,261 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 830434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:53,276 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3199317 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:58,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:58,364 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2689264 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:03:58,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:03:59,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:03:59,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 830434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:01,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:05,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:05,269 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3199317 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:05,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:06,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:06,918 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 709948 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:07,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:07,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:07,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:08,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:08,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:08,911 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 709948 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:09,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:09,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:09,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:10,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:10,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:10,891 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 709948 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:11,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:11,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:11,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:13,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:13,468 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1366262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:14,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:15,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:15,602 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1366262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:17,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:17,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:18,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:18,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:18,216 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1366262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:18,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:18,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:18,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:20,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:21,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:21,861 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:26,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:26,404 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:26,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:26,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:28,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:29,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:29,725 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:34,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:34,277 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:34,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:34,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:36,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:39,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:39,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:39,071 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978222 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:39,098 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2840308 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:39,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:39,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:42,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:42,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:42,799 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:42,826 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:43,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:44,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:44,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:46,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:46,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:46,707 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:46,731 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:47,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:47,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:47,285 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:50,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:50,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:50,328 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:50,354 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1945182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:50,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:50,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:53,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:53,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:53,605 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977801 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:53,630 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:53,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:53,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:53,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:04:56,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:56,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:04:59,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:04:59,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977801 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:00,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:00,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:02,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:02,768 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1513939 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:02,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:05,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:05,579 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1977801 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:05,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:08,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:08,445 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:10,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:10,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:10,962 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:11,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:14,263 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:14,265 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:14,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:14,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:16,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:16,642 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:16,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:19,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:19,747 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978910 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:20,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:20,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:22,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:22,073 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1299444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:22,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:23,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:23,916 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:24,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:27,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:27,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:27,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:28,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:28,643 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:28,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:31,969 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:31,971 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:32,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:33,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:33,572 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 807412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:33,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:33,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:36,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:36,838 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978217 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:36,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:39,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:39,095 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:39,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:41,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:41,577 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:41,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:44,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:44,160 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:44,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:44,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:46,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:46,061 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:46,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:48,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:48,417 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1481799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:48,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:50,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:50,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1300430 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:50,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:51,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:51,036 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:51,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:52,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:52,308 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:52,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:52,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:52,860 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:52,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:53,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:53,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:53,979 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:54,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:54,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:54,471 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:55,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:55,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815432 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:05:56,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:56,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:56,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:05:58,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:05:58,634 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525109 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:01,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:01,288 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:02,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:03,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:03,994 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525109 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:04,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:04,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:06,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:06,745 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:06,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:09,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:09,269 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1525109 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:09,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:13,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:13,068 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:13,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:14,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:14,410 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745503 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:14,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:14,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:14,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:14,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:16,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:16,094 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745503 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:16,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:16,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:16,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:17,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:17,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 745503 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:17,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:17,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:17,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:17,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:20,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:20,477 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:20,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:20,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:20,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:20,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:23,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:23,278 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:23,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:23,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:23,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:23,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:26,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:26,157 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1771207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:26,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:26,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:26,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:26,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:28,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:28,001 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:28,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:28,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:28,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:28,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:29,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:29,811 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:30,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:30,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:30,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:30,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:31,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:31,623 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 904736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:31,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:31,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:31,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:31,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:34,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:34,396 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:34,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:34,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:34,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:34,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:37,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:37,343 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:37,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:37,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:37,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:37,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:39,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:39,649 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1551702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:40,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:40,072 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 215178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:41,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:41,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:41,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:41,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:41,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:41,543 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 215178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:41,550 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:41,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:41,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:42,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:42,066 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 215178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:42,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:43,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:43,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:43,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:44,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:44,848 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789448 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:44,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:46,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:46,289 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 687404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:46,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:46,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:47,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:47,783 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789448 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:47,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:48,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:48,841 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 482975 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:48,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:48,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:50,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:50,438 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789448 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:50,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:51,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:51,514 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 482975 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:51,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:51,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:53,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:53,156 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:53,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:54,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:54,224 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 482975 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:54,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:54,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:55,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:55,877 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:55,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:56,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:56,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:57,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:06:57,424 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:06:57,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:58,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:06:58,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:02,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:02,661 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800180 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:02,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:02,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:02,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:02,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:07,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:07,325 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800180 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:07,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:07,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:07,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:07,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:12,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:12,685 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800180 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:12,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:12,884 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115518 input tokens (60000 > 129024 - 115518). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:13,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:13,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:13,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:17,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:17,607 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:17,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:17,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:17,823 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115518 input tokens (60000 > 129024 - 115518). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:17,832 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99999 input tokens (60000 > 129024 - 99999). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:17,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:17,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:22,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:22,570 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:22,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:22,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:22,775 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115518 input tokens (60000 > 129024 - 115518). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:22,784 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99999 input tokens (60000 > 129024 - 99999). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:22,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:22,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:27,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:27,726 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2226795 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:27,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:27,937 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115826 input tokens (60000 > 129024 - 115826). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:27,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:27,946 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99999 input tokens (60000 > 129024 - 99999). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:28,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:28,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:29,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:29,368 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:31,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:31,116 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115826 input tokens (60000 > 129024 - 115826). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:31,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:31,125 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:31,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:31,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:32,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:32,422 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:33,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:33,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:33,674 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115826 input tokens (60000 > 129024 - 115826). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:33,682 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:33,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:33,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:34,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:34,952 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 448431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:36,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:36,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:36,751 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115738 input tokens (60000 > 129024 - 115738). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:36,760 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:36,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:37,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:39,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:39,735 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:40,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:40,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:40,877 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115738 input tokens (60000 > 129024 - 115738). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:40,885 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:40,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:43,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:43,514 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:43,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:44,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:44,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:44,583 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115738 input tokens (60000 > 129024 - 115738). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:44,592 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:44,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:47,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:47,326 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1581097 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:47,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:49,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:49,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:49,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:49,516 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115498 input tokens (60000 > 129024 - 115498). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:49,525 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1203058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:49,540 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:50,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:50,420 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:50,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:50,617 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115498 input tokens (60000 > 129024 - 115498). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:50,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:52,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:52,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:52,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:52,209 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1203058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:52,221 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:52,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:52,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:52,455 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115498 input tokens (60000 > 129024 - 115498). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:52,463 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:53,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:53,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:53,477 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589727 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:53,481 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:55,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:55,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:55,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115515 input tokens (60000 > 129024 - 115515). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:55,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1203058 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:56,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:56,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:56,271 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:56,277 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:56,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:56,490 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115515 input tokens (60000 > 129024 - 115515). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:57,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:57,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:57,473 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:57,477 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 713515 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:57,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:07:57,844 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115515 input tokens (60000 > 129024 - 115515). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:07:59,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:07:59,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:02,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:02,067 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:02,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:02,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:02,484 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:02,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:06,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:06,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:06,886 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:06,897 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:07,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:07,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:07,282 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:07,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:11,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:11,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:11,018 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:11,028 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:11,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:11,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:11,677 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 269188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:11,684 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:12,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:12,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:12,549 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108300 input tokens (60000 > 129024 - 108300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:12,554 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:15,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:15,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:15,828 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 269188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:15,835 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:15,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:16,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:16,105 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108300 input tokens (60000 > 129024 - 108300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:16,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:18,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:18,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:18,684 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 269188 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:18,688 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:18,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:18,912 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108300 input tokens (60000 > 129024 - 108300). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:18,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:21,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:21,549 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1849158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:21,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:21,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:25,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:25,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:25,455 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115692 input tokens (60000 > 129024 - 115692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:25,465 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2476073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:25,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:25,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:25,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:25,926 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115692 input tokens (60000 > 129024 - 115692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:27,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:29,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:29,689 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2476073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:29,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:29,983 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115692 input tokens (60000 > 129024 - 115692). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:33,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:33,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:33,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:34,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:34,127 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2476073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:35,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:35,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:35,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115686 input tokens (60000 > 129024 - 115686). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:35,175 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589725 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:35,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:35,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:35,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:36,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:36,498 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115686 input tokens (60000 > 129024 - 115686). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:36,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:36,507 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589725 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:36,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:36,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:36,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:37,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:37,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:37,618 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115686 input tokens (60000 > 129024 - 115686). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:37,626 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589725 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:37,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:37,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:39,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:39,153 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1179881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:39,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:39,298 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115745 input tokens (60000 > 129024 - 115745). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:39,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:39,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:39,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:40,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:40,878 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1179881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:42,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:42,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:42,662 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115745 input tokens (60000 > 129024 - 115745). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:42,671 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 996868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:42,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:44,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:44,734 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1179881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:45,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:46,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:46,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:46,141 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115745 input tokens (60000 > 129024 - 115745). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:46,151 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 996868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:46,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:46,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:46,621 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:47,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:47,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:47,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:47,975 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110911 input tokens (60000 > 129024 - 110911). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:47,984 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 996868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:48,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:48,341 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:48,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:48,522 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110911 input tokens (60000 > 129024 - 110911). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:48,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:48,972 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 232699 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:49,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:49,201 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110911 input tokens (60000 > 129024 - 110911). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:53,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:54,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:54,329 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78012 input tokens (60000 > 129024 - 78012). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:54,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:54,480 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78012 input tokens (60000 > 129024 - 78012). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:54,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:54,655 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 78012 input tokens (60000 > 129024 - 78012). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:57,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:57,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:08:58,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:08:58,940 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:08:58,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:00,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:00,023 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:01,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:01,100 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:02,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:02,155 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:03,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:03,224 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 616105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:04,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:04,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589673 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:09,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:09,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:09,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:09,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:09,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:09,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:09,752 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71996 input tokens (60000 > 129024 - 71996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:09,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:09,931 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71996 input tokens (60000 > 129024 - 71996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:10,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:10,105 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 71996 input tokens (60000 > 129024 - 71996). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:10,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:10,344 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110072 input tokens (60000 > 129024 - 110072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:10,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:10,584 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110072 input tokens (60000 > 129024 - 110072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:10,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:10,820 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 110072 input tokens (60000 > 129024 - 110072). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:11,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:11,058 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113460 input tokens (60000 > 129024 - 113460). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:11,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:11,297 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113460 input tokens (60000 > 129024 - 113460). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:11,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:11,541 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113460 input tokens (60000 > 129024 - 113460). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:11,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:11,767 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101180 input tokens (60000 > 129024 - 101180). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:11,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:11,996 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101180 input tokens (60000 > 129024 - 101180). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:12,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:12,229 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101180 input tokens (60000 > 129024 - 101180). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:12,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:12,454 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101080 input tokens (60000 > 129024 - 101080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:12,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:12,671 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101080 input tokens (60000 > 129024 - 101080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:12,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:12,891 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 101080 input tokens (60000 > 129024 - 101080). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:17,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:17,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:17,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:17,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:17,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:17,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:18,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:18,720 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115639 input tokens (60000 > 129024 - 115639). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:21,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:21,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:21,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:22,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:22,849 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2555698 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:23,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:23,032 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115639 input tokens (60000 > 129024 - 115639). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:23,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:24,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:24,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:24,091 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:24,103 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99589 input tokens (60000 > 129024 - 99589). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:28,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:28,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:28,176 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115639 input tokens (60000 > 129024 - 115639). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:28,180 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2555698 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:28,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:28,475 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99589 input tokens (60000 > 129024 - 99589). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:28,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:29,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:29,244 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:32,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:32,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:32,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:32,661 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2555698 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:32,687 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99589 input tokens (60000 > 129024 - 99589). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:33,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:33,786 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589742 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:33,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:36,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:36,562 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1673858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:39,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:39,707 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1673858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:42,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:42,467 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1673858 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:50,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:50,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:50,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:50,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:50,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:51,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:51,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:51,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:51,950 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:52,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:52,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:52,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:52,773 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:53,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:54,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:54,101 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:54,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:55,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:55,193 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:56,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:56,511 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000462 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:57,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:09:57,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:57,565 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589602 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:57,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:57,702 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99059 input tokens (60000 > 129024 - 99059). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:57,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:57,949 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99059 input tokens (60000 > 129024 - 99059). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:09:58,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:09:58,176 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 99059 input tokens (60000 > 129024 - 99059). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:07,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:17,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:18,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:18,734 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:19,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:19,993 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:20,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:20,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:20,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:20,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:21,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:21,199 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:22,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:24,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:24,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:24,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:24,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:24,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:26,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:28,408 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:30,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:30,611 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1400381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:32,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:32,547 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1400381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:35,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:35,027 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1400381 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:43,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:43,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:43,964 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:44,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:44,180 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:44,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:44,436 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 109393 input tokens (60000 > 129024 - 109393). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:45,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:45,539 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:46,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:46,629 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:47,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:47,711 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589364 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:48,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:48,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:48,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:48,508 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:48,519 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89357 input tokens (60000 > 129024 - 89357). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:48,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:48,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:48,931 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:48,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89357 input tokens (60000 > 129024 - 89357). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:49,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:49,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:49,380 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 143537 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:49,387 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 89357 input tokens (60000 > 129024 - 89357). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:49,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:49,714 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75384 input tokens (60000 > 129024 - 75384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:49,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:49,996 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75384 input tokens (60000 > 129024 - 75384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:50,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:50,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75384 input tokens (60000 > 129024 - 75384). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:50,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:50,625 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125649 input tokens (60000 > 129024 - 125649). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:50,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:50,993 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125649 input tokens (60000 > 129024 - 125649). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:51,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:51,362 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125649 input tokens (60000 > 129024 - 125649). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:51,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:51,628 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151062 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:51,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:51,895 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151062 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:52,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:52,151 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151062 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:52,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:52,558 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:52,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:52,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:52,990 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:53,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:53,367 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176161 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:53,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:10:53,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:53,648 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118666 input tokens (60000 > 129024 - 118666). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:53,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:53,895 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118666 input tokens (60000 > 129024 - 118666). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:10:54,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:10:54,153 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118666 input tokens (60000 > 129024 - 118666). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:03,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:03,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:03,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:03,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:07,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:08,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:08,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:08,214 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:08,219 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:08,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:08,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:08,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:08,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:08,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:08,568 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:08,574 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:08,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:08,876 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 134605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:09,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:09,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,089 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:09,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:09,548 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185819 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:09,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:09,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:11,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,534 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185819 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:11,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:11,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:12,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:12,324 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185819 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:13,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:13,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:16,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:16,511 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:16,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:16,800 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 127354 input tokens (60000 > 129024 - 127354). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:16,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:16,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:21,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:21,167 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:21,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:21,643 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 127354 input tokens (60000 > 129024 - 127354). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:22,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:22,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:26,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:26,067 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:26,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:26,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:26,554 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 127354 input tokens (60000 > 129024 - 127354). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:27,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:27,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:30,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:30,540 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:30,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:30,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:30,896 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:31,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:34,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:34,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:35,000 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:35,038 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:35,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:35,516 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:35,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:39,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:39,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:39,961 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2332962 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:39,991 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:40,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:40,396 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151075 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:41,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:41,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:44,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:44,375 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:48,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:48,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:48,463 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 123091 input tokens (60000 > 129024 - 123091). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:48,474 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580396 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:50,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:50,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:51,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:51,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:51,230 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 123091 input tokens (60000 > 129024 - 123091). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:51,241 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:51,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:51,879 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:52,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:52,190 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 123091 input tokens (60000 > 129024 - 123091). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:52,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:52,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:54,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:54,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:54,685 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1781784 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:54,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:55,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:55,262 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:55,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:11:55,928 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 346985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:11:56,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:11:56,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:01,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:01,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:01,451 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:01,461 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:05,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:05,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:05,890 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:05,895 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:11,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:11,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:11,933 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 166969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:11,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:12,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:16,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:16,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:16,256 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:16,261 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:21,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:21,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:21,974 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 167157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:21,985 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3709990 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:22,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:22,494 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90673 input tokens (60000 > 129024 - 90673). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:23,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:26,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:26,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:26,923 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 167157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:26,933 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580676 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:31,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:31,232 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:31,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:34,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:34,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:34,536 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 167157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:34,548 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:34,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:38,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:38,554 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:38,657 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:42,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:42,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:42,508 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125831 input tokens (60000 > 129024 - 125831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:42,521 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:46,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:46,758 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755764 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:47,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:47,156 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125831 input tokens (60000 > 129024 - 125831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:48,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:48,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:50,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:50,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:50,880 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:50,916 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2187539 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:51,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:51,365 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 125831 input tokens (60000 > 129024 - 125831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:51,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:54,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:54,440 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:54,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:54,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:55,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:55,048 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:55,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:12:57,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:57,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:57,994 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:58,024 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:58,717 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:12:58,719 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:12:58,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:02,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:02,708 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:02,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:04,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:04,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:04,247 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:04,255 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:08,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:08,256 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:08,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:10,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:10,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:10,281 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:10,288 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:10,428 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:14,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:14,697 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800840 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:14,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:16,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:16,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:16,901 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:16,908 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:21,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:21,213 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:21,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:21,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:21,590 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152066 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:21,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:25,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:25,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:25,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:25,885 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:26,285 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:30,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:30,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:30,159 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2751878 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:30,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:30,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:30,698 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:32,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:32,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:34,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:34,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:34,911 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:34,942 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:35,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:35,339 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:35,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:38,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:38,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:38,148 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:38,165 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:41,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:41,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:41,422 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 88209 input tokens (60000 > 129024 - 88209). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:41,426 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:41,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:41,910 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:42,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:44,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:44,605 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1859201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:45,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:48,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:48,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:48,222 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:48,230 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2109471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:51,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:51,274 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:51,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:51,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:55,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:55,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:55,677 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:55,684 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:58,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:58,670 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:58,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:13:59,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:13:59,012 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:13:59,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:02,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:02,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:02,819 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1744078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:02,855 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:03,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:03,239 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152103 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:03,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:06,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:06,365 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:06,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:10,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:10,113 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2580854 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:10,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:13,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:13,351 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:13,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:15,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:15,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:15,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:15,573 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:15,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:18,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:18,598 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2053719 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:18,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:20,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:20,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:20,308 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:20,318 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:25,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:25,857 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:26,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:26,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:26,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176474 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:26,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:32,174 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:32,175 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:32,177 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:32,234 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:32,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:32,824 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:32,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:38,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:38,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:38,655 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3712073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:38,701 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:38,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:39,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:39,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:39,369 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:39,374 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:43,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:43,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:43,837 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:43,882 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:44,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:44,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:44,477 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:44,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 150393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:44,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:48,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:48,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:48,790 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:48,834 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755718 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:49,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:49,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:49,278 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98935 input tokens (60000 > 129024 - 98935). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:49,282 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:50,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:50,867 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:50,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:54,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:54,741 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2675670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:54,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:14:56,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:56,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:14:56,878 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:14:56,885 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:00,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:00,332 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:00,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:02,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:02,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:02,331 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 152125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:02,345 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000468 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:02,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:05,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:05,443 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:05,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:09,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:09,960 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:09,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:09,974 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:14,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:14,069 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153112 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:14,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:14,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:18,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:18,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:18,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:18,224 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:22,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:22,885 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:23,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:27,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:27,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:27,013 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300393 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:27,014 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2199901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:27,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:30,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:30,987 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:31,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:32,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:32,815 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:32,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:32,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:37,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:37,035 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:37,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:38,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:38,778 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:39,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:42,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:42,257 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:42,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:42,401 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:44,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:44,009 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1000469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:44,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:47,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:47,217 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:47,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:47,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:47,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:50,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:50,479 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1747662 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:51,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:51,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:51,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:55,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:15:55,178 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:15:55,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:56,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:15:56,206 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:00,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:00,387 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:00,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:00,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:00,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:00,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:05,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:05,519 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2755937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:05,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:05,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:05,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:09,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:09,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:09,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:09,093 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:09,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:09,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:09,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:09,440 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:09,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:12,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:12,076 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:12,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:12,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:12,350 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 94359 input tokens (60000 > 129024 - 94359). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:12,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:15,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:15,043 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1862201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:15,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:15,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:15,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:18,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:18,827 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:18,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:19,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:19,246 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:19,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:22,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:22,901 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:23,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:23,322 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:23,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:23,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:23,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:27,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:27,623 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2576270 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:27,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:27,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:27,958 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:27,963 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:28,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:28,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:32,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:32,250 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:32,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:32,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:32,802 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:32,807 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:32,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:32,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:37,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:37,714 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:38,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:38,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:38,278 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 95788 input tokens (60000 > 129024 - 95788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:38,279 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:38,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:38,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:43,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:43,463 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:44,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:44,021 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:44,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:44,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:44,263 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:48,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:48,858 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:49,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:49,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:49,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:49,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:54,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:54,031 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:54,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:54,602 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:16:54,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:54,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:54,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:16:59,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:16:59,651 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2327197 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:00,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:00,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:00,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:00,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:00,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:05,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:05,173 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:05,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:05,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:05,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:05,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:05,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:10,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:10,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:10,165 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:10,171 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:10,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:10,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:10,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:10,755 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:10,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:14,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:14,122 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2329610 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:14,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:14,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:14,533 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:14,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:17,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:17,864 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:17,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:17,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:18,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:18,201 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:18,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:20,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:20,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:20,872 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:20,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:21,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:21,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:21,342 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:21,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:24,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:24,680 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1854091 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:24,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:24,713 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:24,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:25,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:25,122 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:25,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:29,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:29,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:29,581 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:29,610 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:29,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:29,983 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:30,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:30,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:30,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:34,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:34,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:34,610 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:34,615 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:35,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:35,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:36,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:36,251 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:36,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:36,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:40,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:40,199 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800833 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:40,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:42,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:42,077 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:42,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:42,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:44,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:44,249 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:44,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:45,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:45,772 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1030084 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:45,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:48,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:48,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:48,570 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:48,578 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:48,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:48,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:48,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:49,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:49,828 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:51,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:51,892 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1593113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:52,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:52,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:52,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:52,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:52,802 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:56,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:17:56,545 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:17:56,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:56,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:56,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:17:56,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:00,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:00,647 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:00,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:01,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:01,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:05,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:05,480 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:05,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:05,483 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2677638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:05,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:05,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:06,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:06,071 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:06,160 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:10,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:10,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:10,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:10,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:10,981 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:11,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:11,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:14,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:14,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:14,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:15,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:15,374 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:15,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:19,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:19,720 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800882 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:19,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:20,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:20,291 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:20,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:25,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:25,838 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:26,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:26,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:26,520 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:26,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:31,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:31,819 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:32,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:34,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:36,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:37,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:37,399 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3714326 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:37,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:37,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:37,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:41,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:41,621 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:41,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:41,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:41,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:41,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:46,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:46,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:46,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:46,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:46,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:50,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:50,693 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2800856 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:50,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:51,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:51,196 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:51,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:55,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:55,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:55,122 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:55,153 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:55,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:55,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:55,755 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:55,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:18:59,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:59,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:18:59,572 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:18:59,602 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:00,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:00,363 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:00,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:02,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:02,900 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:03,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:07,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:07,290 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756126 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:10,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:10,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:11,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:11,279 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:11,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:11,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:11,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:11,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:14,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:14,440 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:14,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:14,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:14,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:14,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:14,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:18,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:18,346 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2156661 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:18,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:18,680 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:18,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:18,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:18,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:21,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:21,939 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:23,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:23,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:23,721 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:23,727 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:23,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:23,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:27,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:27,368 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:29,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:29,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:29,142 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:29,147 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:29,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:31,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:31,745 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1788888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:31,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:33,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:33,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:33,363 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:33,369 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:33,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:36,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:36,762 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:36,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:37,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:37,158 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:37,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:40,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:40,060 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:40,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:40,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:40,263 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:40,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:44,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:44,347 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2049183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:44,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:44,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:44,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:45,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:45,872 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989135 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:45,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:48,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:48,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:48,831 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:48,837 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:50,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:50,341 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989135 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:50,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:50,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:50,627 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 160340 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:50,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:52,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:52,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:52,986 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989135 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:52,989 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:53,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:53,685 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:54,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:54,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:54,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:57,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:57,421 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1779099 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:57,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:19:57,832 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:19:57,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:57,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:19:57,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:01,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:01,552 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:02,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:02,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:02,477 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:02,486 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:02,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:02,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:06,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:06,177 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:07,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:07,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:07,097 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:07,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:07,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:07,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:10,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:10,836 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1737694 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:11,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:11,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:11,730 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:11,739 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504181 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:11,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:12,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:16,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:16,222 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:17,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:17,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:17,166 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:17,175 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:17,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:17,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:21,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:21,586 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:23,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:23,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:23,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:23,665 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:23,673 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1189145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:23,688 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:23,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:28,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:28,246 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2153059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:30,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:30,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:30,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:30,356 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:30,364 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1189145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:30,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:30,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:36,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:36,194 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:37,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:37,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:37,122 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:37,123 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504212 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:39,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:39,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1189145 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:39,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:43,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:43,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:43,679 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504212 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:43,691 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:44,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:44,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:44,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:45,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:45,529 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504212 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:49,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:49,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:49,790 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:49,800 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3200469 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:49,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:50,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:50,825 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:50,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:51,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:51,196 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:51,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:53,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:53,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:53,425 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:53,443 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:53,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:53,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:53,821 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:53,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:56,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:56,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:56,761 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504266 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:56,774 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:56,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:57,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:57,137 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 246876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:57,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:20:59,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:59,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:59,492 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:59,505 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1766930 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:20:59,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:20:59,939 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 246876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:00,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:00,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:00,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:00,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:00,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:00,998 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 246876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:01,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:01,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:01,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:01,909 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504345 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:01,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:02,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:02,317 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:02,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:04,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:04,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:04,677 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:04,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:05,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:05,145 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:05,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:06,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:06,060 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:06,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:08,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:08,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:08,399 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:08,419 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:09,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:09,159 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504186 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:09,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:11,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:11,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:11,561 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1378040 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:11,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:11,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:12,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:12,299 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:12,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:12,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:12,627 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:12,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:13,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:13,323 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:13,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:13,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:13,658 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 266207 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:13,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:14,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:14,383 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:14,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:14,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:14,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:14,709 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 267521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:14,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:15,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:15,386 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:15,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:15,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:15,714 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 267521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:15,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:16,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:16,428 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:16,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:16,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:16,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:16,752 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 267521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:16,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:17,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:17,424 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:17,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:18,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:18,686 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:19,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:19,692 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:20,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:20,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:20,730 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:21,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:21,680 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:22,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:22,699 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:23,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:23,752 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 504431 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:24,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:24,871 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 442059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:26,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:26,004 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 442059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:27,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:27,195 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 442059 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:32,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:32,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:47,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:48,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:48,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:49,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:49,302 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:49,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:49,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:49,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:49,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:50,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:50,630 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:50,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:50,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:50,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:52,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:52,026 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1004384 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:52,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:52,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:52,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:52,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:53,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:53,946 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:54,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:54,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:54,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:55,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:55,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:55,879 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:56,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:56,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:56,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:57,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:57,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:57,704 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:57,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:57,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:57,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:58,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:59,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:21:59,591 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274512 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:21:59,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:59,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:21:59,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:00,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:01,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:01,059 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274512 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:01,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:01,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:01,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:02,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:02,564 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274512 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:02,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:02,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:02,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:02,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:04,544 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274247 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:04,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:04,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:05,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:06,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:06,111 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274247 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:06,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:06,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:06,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:07,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:07,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:07,689 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274247 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:08,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:08,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:09,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:09,899 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 302328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:10,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:10,494 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 302328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:10,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:10,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:10,932 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 302328 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:11,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:11,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:11,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:11,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:11,860 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:13,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:13,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:14,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:14,157 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:26,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:26,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:27,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:28,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:28,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:28,990 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 882113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:29,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:29,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:29,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:30,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:30,623 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 882113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:30,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:30,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:30,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:31,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:32,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:32,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 882113 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:35,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:35,089 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1484937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:35,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:35,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:35,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:35,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:38,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:38,314 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1484937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:38,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:38,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:38,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:40,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:40,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:40,771 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1484937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:40,796 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:41,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:41,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:42,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:42,083 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:42,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:45,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:45,255 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1527638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:46,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:46,398 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:46,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:46,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:49,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:49,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1527638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:49,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:22:51,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:51,957 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1527638 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:22:56,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:22:56,755 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2635865 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:01,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:01,144 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2635865 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:03,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:03,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:03,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:03,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:06,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:06,020 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2635865 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:07,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:07,250 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 661003 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:07,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:07,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:07,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:07,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:08,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:08,494 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 661003 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:08,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:08,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:08,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:08,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:09,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 661003 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:09,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:09,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:10,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:10,412 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301297 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:10,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:12,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:12,809 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1322275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:12,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:13,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:13,275 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301297 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:13,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:15,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:15,234 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1322275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:15,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:15,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:15,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:15,766 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 301297 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:15,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:20,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:20,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:20,363 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1322275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:20,386 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:20,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:20,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:20,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:22,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:22,580 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1445714 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:22,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:26,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:26,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:26,819 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 84772 input tokens (60000 > 129024 - 84772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:26,824 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:29,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:29,439 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1445714 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:29,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:30,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:30,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:30,042 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 84772 input tokens (60000 > 129024 - 84772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:30,047 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 354068 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:34,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:34,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:34,102 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1445714 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:34,128 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:34,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:34,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:34,843 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 84772 input tokens (60000 > 129024 - 84772). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:34,847 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 354068 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:34,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:39,844 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:39,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:39,847 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2016240 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:39,873 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756829 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:40,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:40,617 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 354068 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:44,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:44,960 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2016240 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:44,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:44,987 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756829 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:45,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:45,374 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79481 input tokens (60000 > 129024 - 79481). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:50,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:50,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:50,185 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2016240 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:50,216 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756829 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:50,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:50,667 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79481 input tokens (60000 > 129024 - 79481). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:53,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:53,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:23:55,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:55,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:55,048 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2738035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:55,078 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756874 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:55,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:55,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:55,518 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100495 input tokens (60000 > 129024 - 100495). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:55,523 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79481 input tokens (60000 > 129024 - 79481). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:59,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:23:59,451 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2738035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:23:59,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:03,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:03,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:03,496 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100495 input tokens (60000 > 129024 - 100495). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:03,501 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756874 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:07,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:07,434 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2738035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:07,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:07,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:11,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:11,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:11,749 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100495 input tokens (60000 > 129024 - 100495). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:11,753 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2756874 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:17,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:17,950 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4000584 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:18,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:18,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:23,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:23,019 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977919 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:23,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:28,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:28,760 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4000584 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:28,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:33,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:33,310 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977919 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:33,433 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:39,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:39,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:39,395 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 344757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:39,403 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4000584 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:44,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:44,194 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977919 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:44,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:45,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:45,163 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 344757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:45,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:53,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:53,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:53,077 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5278194 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:53,120 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3342078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:53,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:24:54,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:24:54,022 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 344757 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:24:54,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:01,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:01,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:01,312 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5278194 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:01,357 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3342078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:01,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:03,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:03,039 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:03,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:10,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:10,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:10,642 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5278194 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:10,693 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3342078 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:10,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:12,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:12,210 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:12,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:20,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:20,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:20,679 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:20,746 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3477906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:20,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:22,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:22,238 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 789330 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:22,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:30,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:30,587 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:30,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:36,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:36,368 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3477906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:36,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:45,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:45,321 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876736 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:51,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:25:51,089 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3477906 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:25:53,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:25:54,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:00,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:00,559 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:00,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:06,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:06,952 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3645466 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:07,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:15,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:15,459 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:15,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:15,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:21,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:21,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3645466 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:21,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:30,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:30,683 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5876774 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:32,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:36,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:36,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:36,748 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3645466 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:36,793 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79355 input tokens (60000 > 129024 - 79355). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:36,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:45,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:45,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877288 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:45,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:26:51,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:51,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:26:51,965 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178847 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:52,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79355 input tokens (60000 > 129024 - 79355). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:26:52,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:00,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:00,860 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877288 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:01,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:07,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:07,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:07,749 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178847 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:07,794 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79355 input tokens (60000 > 129024 - 79355). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:17,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:17,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:17,069 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 289386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:17,076 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877288 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:17,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:24,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:24,271 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178847 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:24,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:35,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:35,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:35,095 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 289386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:35,103 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:35,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:42,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:42,350 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178851 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:42,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:50,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:50,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:50,643 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 289386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:50,646 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:57,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:27:57,469 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178851 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:27:57,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:58,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:27:58,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:06,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:06,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:06,742 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:06,793 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178851 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:07,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:09,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:09,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:09,592 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:09,599 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:18,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:18,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:18,405 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:18,456 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4544004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:19,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:19,317 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:19,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:27,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:27,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:27,486 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:27,535 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:35,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:35,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:35,030 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 235336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:35,038 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4544004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:36,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:44,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:44,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:44,352 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5877776 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:44,353 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:51,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:51,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:51,900 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:51,909 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4544004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:53,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:53,165 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:54,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:28:54,006 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:28:54,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:28:55,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:01,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:01,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:01,729 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:01,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4645617 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:02,579 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:02,581 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:02,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:02,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:09,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:09,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:09,391 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600550 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:09,392 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4645617 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:09,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:09,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:17,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:17,145 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4645617 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:17,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:17,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:17,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:17,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:25,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:25,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:25,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:25,639 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 179342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:25,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:26,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:33,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:33,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:33,472 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:33,517 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:33,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:33,981 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 179342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:34,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:34,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:34,382 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:34,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:42,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:42,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:42,497 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 179342 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:42,505 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4978842 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:42,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:43,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:43,096 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 208809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:43,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:43,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:43,706 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:43,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:43,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:44,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:44,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:44,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:44,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:44,471 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:44,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:44,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:44,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:45,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:45,271 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 310434 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:46,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:46,037 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 202852 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:56,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:56,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:57,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:58,184 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:58,186 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:58,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:58,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:58,722 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:58,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:59,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:29:59,261 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 293300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:29:59,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:59,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:29:59,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:00,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:00,013 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:00,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:00,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:00,505 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:00,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:00,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:00,989 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 261612 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:01,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,624 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:01,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:02,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:02,834 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:02,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:03,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:03,297 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:03,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:03,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:03,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:03,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:03,751 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 243470 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:04,047 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:04,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:05,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:06,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:07,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:07,985 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:08,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:08,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:08,492 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:08,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:08,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:08,974 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 256015 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:09,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:09,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:09,359 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:09,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:09,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:09,749 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:09,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:09,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:10,132 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 204770 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:10,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:10,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,407 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:11,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:12,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:13,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:14,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:15,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:16,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:17,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:18,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:19,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:20,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:21,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:22,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:23,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:24,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:25,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:35,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:37,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:37,447 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:39,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:39,269 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:41,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:30:41,069 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:30:48,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:48,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:50,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:50,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:50,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:51,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:54,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:54,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:54,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:55,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:55,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:55,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:30:55,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:04,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:04,746 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 283110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:05,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:05,491 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 283110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:06,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:06,196 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 283110 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:09,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,693 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:09,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:10,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:10,683 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:10,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:16,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:16,314 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:16,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:18,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:22,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:22,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:22,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:23,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:23,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:23,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:24,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:29,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:29,212 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:31,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:31,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:35,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:35,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:35,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:35,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:37,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:37,737 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:40,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:40,100 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:41,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:31:41,943 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1001527 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:31:48,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:49,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:51,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:51,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:51,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:51,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:52,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:56,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:57,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:57,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:57,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:57,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:31:59,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:02,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:03,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:04,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:04,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:04,484 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 841958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:05,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:05,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:05,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:05,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:06,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:06,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 841958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:07,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:07,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:08,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:08,019 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 841958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:09,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:09,137 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 584481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:09,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:09,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:10,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:10,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:10,178 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 584481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:11,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:11,143 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 584481 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:11,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:12,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:12,605 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:13,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:13,913 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:15,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:15,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:16,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:16,720 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:18,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:18,050 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:19,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:19,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:19,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:19,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:19,381 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 751521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:20,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:32,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:34,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:35,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:35,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:35,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:35,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:35,430 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:35,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:35,806 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:36,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:36,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:36,209 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 151521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:36,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:36,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:36,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:36,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:36,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:39,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:39,360 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1293243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:39,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:39,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:42,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:42,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1293243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:44,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:44,759 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1293243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:49,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:49,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:49,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:49,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:49,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:50,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:50,383 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:50,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:50,879 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251051 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:52,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:52,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:53,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:59,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:32:59,103 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:32:59,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:32:59,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:00,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:00,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:05,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:05,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:06,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:07,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:11,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:11,272 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:12,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:12,135 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500538 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:12,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:12,973 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500538 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:13,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:13,697 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 500538 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:13,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:13,965 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:14,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:14,213 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:14,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:14,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:14,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:14,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:14,496 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:15,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:16,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:16,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:16,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:17,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:17,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:17,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:17,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:20,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:20,512 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1502526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:20,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:20,804 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:20,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:22,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:23,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:23,320 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1502526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:23,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:23,513 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138300 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:23,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:26,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:26,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:26,727 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1502526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:28,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:31,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:31,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:33,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:33,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:33,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:33,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:34,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:34,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:35,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:35,788 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:35,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:37,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:38,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:38,272 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:40,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:40,528 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:46,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:46,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:46,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:46,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:47,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:47,795 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:47,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:48,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:48,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:48,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:49,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:49,086 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:49,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:49,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:49,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:50,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:50,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:50,336 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:50,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:50,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:51,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:51,686 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 557234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:52,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:52,106 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251080 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:52,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:52,263 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:52,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:33:58,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:58,327 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:58,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:33:58,793 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251080 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:33:58,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:00,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:00,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:04,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:04,490 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:04,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:04,898 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251080 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:05,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:06,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:06,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:10,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:10,561 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3002523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:10,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:11,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:15,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:15,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:16,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:16,704 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:17,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:17,162 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:17,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:17,652 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 251038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:20,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:20,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:20,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:21,023 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115871 input tokens (60000 > 129024 - 115871). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:21,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:21,279 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115871 input tokens (60000 > 129024 - 115871). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:21,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:21,529 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115871 input tokens (60000 > 129024 - 115871). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:21,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:21,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:21,965 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115788 input tokens (60000 > 129024 - 115788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:22,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:22,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:22,211 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115788 input tokens (60000 > 129024 - 115788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:22,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:22,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:22,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:22,753 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115788 input tokens (60000 > 129024 - 115788). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:22,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:23,055 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:23,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:23,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:23,372 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:23,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:23,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:23,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:23,690 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:24,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:24,042 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138324 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:24,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:25,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:25,676 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:26,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:26,507 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:27,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:27,359 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400280 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:30,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:30,775 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:33,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:33,575 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:33,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:33,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:34,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:34,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:36,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:36,425 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:38,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:38,490 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:38,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:39,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:39,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:41,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:41,366 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:42,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:42,937 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:43,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:44,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:44,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:45,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:45,994 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:47,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:47,584 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1236362 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:47,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:48,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:48,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:50,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:50,220 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1780318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:50,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:56,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:56,452 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:34:57,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:34:59,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:34:59,927 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1820903 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:00,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:01,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:05,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:05,732 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:07,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:08,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:08,671 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1820903 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:08,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:14,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:14,629 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955860 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:15,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:15,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:17,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:17,338 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1820903 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:17,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:23,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:23,231 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:24,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:25,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:26,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:26,250 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1826096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:26,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:32,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:32,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:33,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:36,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:36,066 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1826096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:37,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:42,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:42,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:42,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 794995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:42,293 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956677 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:43,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:45,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:45,555 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1826096 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:46,428 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:51,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:51,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:51,623 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 794995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:51,638 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:52,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:35:54,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:35:54,688 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1827105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:35:55,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:00,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:00,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:00,388 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 794995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:00,396 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:00,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:03,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:03,414 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1827105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:04,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:09,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:09,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:09,528 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:09,535 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:09,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:12,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:12,811 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1827105 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:12,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:18,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:18,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:18,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:18,952 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:21,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:21,947 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:22,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:28,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:28,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:28,093 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250968 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:28,096 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:28,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:31,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:31,347 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:31,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:37,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:37,475 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:37,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:40,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:40,386 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:40,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:45,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:45,530 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3222646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:45,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:48,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:48,271 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1839355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:48,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:53,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:53,121 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3222646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:53,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:36:56,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:36:56,194 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1839355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:36:56,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:02,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:02,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:02,002 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:02,019 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3222646 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:05,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:05,253 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1839355 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:05,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:10,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:10,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:10,894 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:10,909 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:11,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:14,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:14,412 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:14,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:20,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:20,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:20,808 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:20,809 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:24,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:24,563 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:24,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:30,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:30,701 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956234 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:30,836 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:30,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:34,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:34,432 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1978287 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:34,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:41,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:41,992 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:42,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:45,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:45,470 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1979449 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:45,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:53,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:53,333 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:53,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:37:56,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:37:56,966 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1979449 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:37:57,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:04,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:04,473 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245339 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:04,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:07,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:07,897 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1979449 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:07,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:13,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:13,849 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:13,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:17,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:17,483 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:17,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:23,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:23,413 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:23,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:27,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:27,690 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:27,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:27,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:34,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:34,052 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956154 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:34,196 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:37,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:37,575 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:37,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:37,717 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:44,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:44,205 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:44,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:48,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:48,330 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200281 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:48,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:54,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:54,457 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:54,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:57,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:38:57,768 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200281 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:38:57,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:38:57,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:03,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:03,531 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:03,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:07,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:07,213 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2200281 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:07,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:13,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:13,573 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:13,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:14,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:14,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:20,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:20,088 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:20,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:20,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:20,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:26,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:26,511 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955897 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:26,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:26,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:26,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:27,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:33,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:33,102 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956409 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:33,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:33,516 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:33,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:39,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:39,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956409 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:45,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:45,939 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956409 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:52,355 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:52,356 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:53,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:53,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:53,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:54,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:58,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:39:58,618 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:39:58,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:58,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:58,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:39:59,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:05,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:05,648 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956122 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:05,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:05,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:05,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:06,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:12,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:12,976 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956454 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:13,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:13,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:13,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:14,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:20,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:20,632 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956454 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:20,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:20,810 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100308 input tokens (60000 > 129024 - 100308). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:20,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:20,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:20,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:27,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:27,963 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3956454 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:28,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:28,137 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100308 input tokens (60000 > 129024 - 100308). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:28,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:28,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:28,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:35,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:35,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:35,040 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100308 input tokens (60000 > 129024 - 100308). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:35,044 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955782 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:35,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:35,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:35,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:35,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:43,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:43,546 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955782 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:44,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:44,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:47,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:51,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:40:51,029 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3955782 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:40:54,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:54,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:54,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:54,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:54,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:54,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:56,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:56,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:56,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:56,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:56,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:57,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:57,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:57,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:40:57,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:00,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:01,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:01,231 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100310 input tokens (60000 > 129024 - 100310). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:01,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:01,421 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100310 input tokens (60000 > 129024 - 100310). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:01,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:01,613 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100310 input tokens (60000 > 129024 - 100310). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:04,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:04,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:04,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:04,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:04,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:04,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:05,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:05,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:05,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:05,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:05,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:05,314 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:05,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:05,537 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:05,772 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:05,774 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 132121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:12,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:12,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:12,734 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:12,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:13,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:13,067 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 211072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:13,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:13,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:13,243 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:13,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:13,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:13,579 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 211072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:13,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:13,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:13,748 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:14,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:14,096 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 211072 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:14,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:14,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:14,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:14,469 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:14,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:14,694 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:14,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:14,922 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:15,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:15,146 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:15,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:15,375 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100307 input tokens (60000 > 129024 - 100307). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:15,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:15,604 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:15,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:15,843 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:16,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:16,074 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:17,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:17,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:17,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:17,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:17,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:17,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:18,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:19,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:19,016 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100311 input tokens (60000 > 129024 - 100311). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:19,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:19,242 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100311 input tokens (60000 > 129024 - 100311). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:19,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:19,474 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100311 input tokens (60000 > 129024 - 100311). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:19,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:19,699 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:19,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:19,935 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:20,163 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:20,164 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100315 input tokens (60000 > 129024 - 100315). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:23,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:23,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:23,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:23,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:24,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:26,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:26,385 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1752353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:26,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:27,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:27,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:29,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:29,305 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1752353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:29,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:29,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:29,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:30,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:32,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:41:32,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1752353 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:41:44,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:44,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:52,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:52,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:52,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:41:52,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:02,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:02,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:02,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:02,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:07,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:07,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:07,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:07,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:08,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:08,753 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100316 input tokens (60000 > 129024 - 100316). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:08,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:08,972 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100316 input tokens (60000 > 129024 - 100316). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:08,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:09,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:09,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:09,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:09,224 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 100316 input tokens (60000 > 129024 - 100316). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:10,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:10,429 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 712313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:11,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:11,677 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 712313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:12,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:12,988 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 712313 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:14,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:14,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:14,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:14,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:15,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:15,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1175536 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:16,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:16,643 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1175536 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:18,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:18,308 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1175536 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:20,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:20,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:20,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:20,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:20,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:21,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:21,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:21,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:21,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:24,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:24,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:25,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:26,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:27,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:34,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:35,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:35,106 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:35,582 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:35,583 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:36,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:36,037 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233422 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:42,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:42,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:42,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:42,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:42,897 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 205745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:43,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:43,270 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 205745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:43,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:43,692 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 205745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:43,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:43,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:44,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:44,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:44,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115617 input tokens (60000 > 129024 - 115617). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:44,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:44,542 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115617 input tokens (60000 > 129024 - 115617). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:44,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:44,809 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 115617 input tokens (60000 > 129024 - 115617). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:48,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:48,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:48,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:50,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:51,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:52,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:52,100 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:53,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:53,617 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:55,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:55,203 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 864200 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:57,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:57,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:57,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:57,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:57,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:57,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:58,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:42:59,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:42:59,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 792030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:42:59,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:00,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:00,478 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 792030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:00,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:00,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:01,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:01,868 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 792030 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:01,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:02,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:02,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:02,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:03,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:03,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:03,029 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:03,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:03,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:03,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:03,758 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 178348 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:03,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:03,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:04,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:04,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:04,959 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 663526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:06,145 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:06,147 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 663526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:06,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:07,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:07,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:07,421 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 663526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:08,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:09,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:09,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:10,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:10,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:10,560 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:12,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:12,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:12,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:12,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:13,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:14,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:14,520 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:14,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:14,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:15,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:16,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:16,296 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:17,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:18,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:18,148 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989336 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:18,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:18,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:18,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:19,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:19,581 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 984704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:19,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:19,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:20,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:20,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:20,964 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 984704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:21,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:21,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:22,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:22,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:22,482 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 984704 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:22,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:22,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:23,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:23,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:23,870 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:24,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:24,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:24,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:24,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:25,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:25,694 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:26,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:26,039 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:26,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:27,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:27,972 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989465 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:28,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:28,368 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197386 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:28,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:29,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:30,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:30,388 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:30,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:30,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:32,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:32,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:32,240 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:32,402 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:32,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:33,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:34,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:34,116 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989433 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:34,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:34,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:34,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:35,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:35,952 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:36,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:36,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:37,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:37,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:37,336 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:37,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:37,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:38,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:38,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:38,838 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:39,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:39,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:39,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:39,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:39,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:40,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:40,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:41,190 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:41,192 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 723121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:41,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:41,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:41,618 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:41,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:42,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:42,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 723121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:43,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:43,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:43,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:44,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:44,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:44,972 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 723121 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:45,377 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:45,379 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 240351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:47,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:50,530 [WARNING] gen_code timeout (attempt 1)
2025-12-10 17:43:54,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:54,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:54,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:54,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:54,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:54,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:54,945 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:55,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:55,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:55,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:55,408 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:55,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:55,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:55,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:55,881 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:55,882 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221471 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:56,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:56,055 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:57,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:57,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:57,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:57,672 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:57,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:57,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:59,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:59,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:43:59,588 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:43:59,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:59,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:43:59,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:00,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:01,015 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:01,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:01,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:02,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:02,918 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 747682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:03,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:03,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:03,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:03,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:03,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:03,944 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 747682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:04,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:04,481 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 298341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:04,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:05,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:06,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:06,021 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 747682 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:06,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:06,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:06,524 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 298341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:06,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:06,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:07,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:07,178 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 298341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:07,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:07,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:08,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:08,199 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 527560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:08,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:13,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:13,988 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3444793 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:14,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:15,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:15,210 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 527560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:15,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:16,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:20,533 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:20,535 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3444793 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:21,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:21,458 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 527560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:21,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:22,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:26,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:26,749 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3444793 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:28,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:28,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:28,343 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:28,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:28,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:29,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:30,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:30,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:30,330 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:30,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:30,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:31,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:32,349 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989383 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:32,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:32,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:33,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:33,149 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 192314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:33,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:33,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:33,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:33,452 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 192314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:34,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:34,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:34,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:34,963 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:35,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:35,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:35,388 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 192314 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:36,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:36,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:36,997 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:37,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:37,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:38,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:38,688 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 839691 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:39,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:39,341 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 297004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:39,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:40,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:40,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:40,602 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 615041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:40,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:41,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:41,122 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 297004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:41,206 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:42,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:42,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 615041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:42,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:42,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:42,969 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 297004 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:44,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:44,318 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 615041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:44,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:44,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:44,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:44,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:46,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:46,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:46,270 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:46,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:46,515 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:47,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:47,184 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 445816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:48,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:49,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:49,278 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:49,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:50,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:50,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 445816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:51,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:52,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:52,102 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989587 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:52,207 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:52,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:53,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:53,184 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 445816 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:54,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:55,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:55,163 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:55,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:56,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:56,441 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:57,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:58,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:58,542 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:58,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:44:59,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:44:59,481 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:44:59,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:00,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:01,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:01,077 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 989294 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:01,231 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:02,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:02,141 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588074 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:03,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:03,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:05,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:05,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:05,669 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121282 input tokens (60000 > 129024 - 121282). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:05,675 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2066915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:06,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:06,101 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121282 input tokens (60000 > 129024 - 121282). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:07,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:07,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:07,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:10,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:10,112 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2066915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:10,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:10,327 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121282 input tokens (60000 > 129024 - 121282). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:10,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:11,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:11,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:14,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:14,595 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2066915 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:14,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:14,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:15,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:15,401 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:15,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:15,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:15,916 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:15,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:16,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:16,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:16,274 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 221664 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:16,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:16,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:16,468 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:16,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:16,740 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 142361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:16,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:16,889 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:17,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:17,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:17,182 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 142361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:17,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:17,314 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83413 input tokens (60000 > 129024 - 83413). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:17,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:17,617 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 142361 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:18,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:18,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:18,702 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:18,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:19,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:19,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:19,795 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:19,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:19,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:19,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:20,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:20,914 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 588876 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:21,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:21,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:22,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:22,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:22,814 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:23,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:23,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:23,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:23,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:23,903 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:23,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:24,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:24,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:25,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:25,061 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589417 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:25,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:25,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:25,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:25,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:25,737 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:25,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:25,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:26,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:26,348 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:26,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:26,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:26,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:26,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:27,041 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300514 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:27,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:27,717 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:28,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:28,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:28,673 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:28,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:28,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:29,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:29,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:29,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:30,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:30,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:30,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:31,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:31,040 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 550992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:31,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:31,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:40,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:40,896 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120215 input tokens (60000 > 129024 - 120215). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:41,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:41,160 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120215 input tokens (60000 > 129024 - 120215). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:41,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:41,417 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 120215 input tokens (60000 > 129024 - 120215). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:41,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:41,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:42,411 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:42,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:42,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:42,820 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:42,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:43,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:43,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:43,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:43,203 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 200518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:43,817 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:43,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:43,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:44,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:44,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:44,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:44,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:45,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:45,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:45,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:45,638 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:46,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:46,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:46,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:46,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:46,822 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:47,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:47,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:47,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:47,973 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:48,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:48,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:48,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:49,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:49,090 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:49,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:49,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:49,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:50,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:50,235 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:50,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:50,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:51,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:45:51,382 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 600518 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:45:51,446 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:51,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:51,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:51,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:52,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:52,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:53,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:53,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:53,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:54,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:45:54,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:03,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:04,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:04,138 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:04,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:04,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:05,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:05,104 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:05,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:05,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:06,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:06,128 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 489523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:06,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:06,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:06,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:06,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:06,906 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:07,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:07,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:07,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:07,585 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:07,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:07,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:08,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:08,493 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:08,740 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:08,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:08,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:09,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:10,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:11,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:11,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:11,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:11,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:12,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:17,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:20,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:20,832 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5933872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:21,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:21,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:22,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:22,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:30,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:30,031 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5933872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:30,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:31,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:31,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:32,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:39,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:39,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5933872 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:39,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:40,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:41,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:42,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:50,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:50,295 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:50,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:46:50,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218365 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:46:51,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:52,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:46:52,374 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:01,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:01,062 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:01,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:01,665 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218365 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:01,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:03,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:03,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:12,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:12,482 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934531 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:13,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:13,028 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218365 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:13,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:14,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:14,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:25,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:25,675 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593800 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:26,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:26,256 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 324964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:26,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:26,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:27,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:36,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:36,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:36,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 324964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:36,346 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593800 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:36,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:37,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:37,352 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 324964 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:38,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:38,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:47,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:47,164 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593800 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:47,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:48,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:48,137 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:49,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:49,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:58,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:58,260 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:58,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:47:59,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:47:59,578 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:47:59,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:09,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:09,597 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:09,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:10,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:10,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 262272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:11,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:11,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:20,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:20,188 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6593855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:20,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:21,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:21,126 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 322118 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:21,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:21,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:21,466 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:22,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:22,607 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 322118 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:22,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:22,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:22,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:23,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:23,138 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 322118 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:23,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:23,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:23,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:23,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:24,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:24,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:24,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:24,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:24,531 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:24,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:24,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:25,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:25,196 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 175148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:25,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:25,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:25,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:27,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:27,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:27,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:27,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:28,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:28,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:28,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:28,455 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 364310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:28,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:28,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:29,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:29,103 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 364310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:29,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:29,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:29,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:29,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:29,770 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 364310 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:29,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:30,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:30,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:30,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:30,436 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:30,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:30,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:31,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:31,114 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:31,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:31,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:31,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:31,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:31,777 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350632 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:32,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:32,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:32,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:32,520 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:32,521 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:32,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:32,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:32,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:33,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:33,256 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:33,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:33,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:33,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:33,993 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:34,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:34,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:34,786 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:34,787 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:35,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:35,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:35,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:35,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:35,615 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:35,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:35,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:36,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:36,462 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 373619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:36,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:36,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:36,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:36,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:37,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:37,268 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 411765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:38,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:38,054 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 411765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:38,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:38,822 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 411765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:39,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:39,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:39,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:39,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:40,019 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:40,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:40,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:40,223 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:40,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:40,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:40,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:48:40,942 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 260665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:48:42,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:42,332 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:42,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:43,587 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:43,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:43,690 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:44,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:44,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:44,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:44,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:44,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:46,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:46,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:46,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:47,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:47,432 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:48,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:48,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:48,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:48,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:49,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:49,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:51,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:51,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:51,431 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:52,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:52,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:52,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:53,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:53,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:53,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:54,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:56,241 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:56,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:56,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:56,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:57,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:58,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:58,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:48:58,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:01,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:01,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:02,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:02,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:03,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:04,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:05,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:06,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:07,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:09,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:09,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:09,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:09,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:10,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:10,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:11,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:11,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:12,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:12,460 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:12,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:13,969 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:14,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:15,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:15,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:17,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:18,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:18,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:18,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:18,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:19,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:19,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:20,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:20,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:20,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:20,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:21,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:21,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:21,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:21,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:21,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:22,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:25,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:25,200 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:25,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:28,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:28,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:29,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:29,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:29,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:29,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:31,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:31,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:33,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:33,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:33,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:34,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:36,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:37,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:38,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:40,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:42,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:43,335 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:43,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:43,467 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:45,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:45,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:46,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:47,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:47,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:47,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:47,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:48,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:48,135 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:49,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:50,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:50,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:53,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:53,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:54,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:54,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:54,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:49:57,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:49:57,856 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:49:59,763 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:00,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:02,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:02,512 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:03,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:04,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:04,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:04,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:07,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:07,215 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2409204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:09,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:10,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:10,474 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:10,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:10,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:11,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:13,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:13,282 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:13,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:14,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:14,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:15,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:16,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:16,051 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1876686 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:17,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:18,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:18,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:19,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:19,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:19,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:20,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:20,513 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:20,732 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:22,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:22,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:22,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:23,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:23,301 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:24,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:24,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:25,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:25,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:26,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:26,492 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1817855 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:27,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:27,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:27,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:27,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:28,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:28,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:28,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:30,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:30,235 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:30,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:30,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:31,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:31,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:32,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:32,958 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:34,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:34,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:35,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:35,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:35,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:35,985 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1858702 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:36,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:36,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:36,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:37,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:37,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:37,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:39,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:39,238 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:39,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:39,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:40,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:41,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:42,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:42,382 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:43,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:43,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:44,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:44,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:45,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:45,283 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1995382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:45,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:45,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:46,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:46,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:48,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:48,809 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256205 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:49,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:49,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:49,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:50,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:52,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:52,131 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256205 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:52,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:52,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:53,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:53,473 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:55,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:55,662 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2256205 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:55,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:55,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:56,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:56,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:56,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:56,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:57,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:57,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:59,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:50:59,544 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:50:59,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:50:59,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:00,735 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:00,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:03,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:03,366 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:03,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:03,595 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:03,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:04,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:07,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:07,443 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2455896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:10,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:10,006 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:10,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:11,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:11,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:14,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:14,188 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:16,439 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:16,441 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:16,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:17,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:17,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:20,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:20,830 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:22,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:22,629 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289272 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:22,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:23,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:26,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:26,464 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2247439 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:26,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:26,799 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119271 input tokens (60000 > 129024 - 119271). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:27,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:27,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:27,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:27,781 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119271 input tokens (60000 > 129024 - 119271). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:28,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:28,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:31,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:31,438 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:31,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:31,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:31,750 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119271 input tokens (60000 > 129024 - 119271). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:32,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:35,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:35,348 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:35,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:35,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:35,858 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 229226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:35,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:39,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:39,016 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2170866 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:39,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:39,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:39,533 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 229226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:39,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:42,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:42,707 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2059987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:42,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:42,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:48,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:48,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:48,063 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3475357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:48,102 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 229226 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:51,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:51,831 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2059987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:52,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:52,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:53,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:51:57,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:57,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:57,035 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2059987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:57,058 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3475357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:51:59,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:51:59,947 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1871604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:00,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:02,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:02,680 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:02,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:07,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:07,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:07,725 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3475357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:07,729 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1871604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:07,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:10,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:10,392 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:10,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:13,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:13,098 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1871604 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:13,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:13,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:15,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:15,430 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689214 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:15,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:15,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:15,747 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119229 input tokens (60000 > 129024 - 119229). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:15,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:15,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:18,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:18,361 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:18,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:18,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:18,685 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119229 input tokens (60000 > 129024 - 119229). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:19,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:21,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:21,324 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:21,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:21,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:21,673 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119229 input tokens (60000 > 129024 - 119229). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:21,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:24,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:24,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:24,477 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834754 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:24,480 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1650377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:25,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:25,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:25,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:25,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:27,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:27,979 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1650377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:31,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:31,093 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1888305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:32,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:32,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:33,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:33,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:33,840 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1650377 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:36,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:36,537 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:36,538 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1888305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:36,564 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119230 input tokens (60000 > 129024 - 119230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:36,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:36,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:36,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:37,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:37,072 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119230 input tokens (60000 > 129024 - 119230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:39,707 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:39,709 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1888305 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:39,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:43,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:43,547 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:43,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:47,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:47,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:47,464 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2503805 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:47,492 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 119230 input tokens (60000 > 129024 - 119230). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:47,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:50,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:50,825 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:50,995 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:54,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:54,450 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2503805 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:54,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:52:58,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:52:58,117 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:52:58,206 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:05,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:05,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:05,652 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2503805 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:05,680 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4338140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:05,833 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:06,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:06,322 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:06,979 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:14,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:14,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:14,415 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:14,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4338140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:15,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:15,142 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:15,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:15,339 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:23,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:23,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:23,215 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:23,240 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4338140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:23,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:23,791 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 218163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:23,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:24,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:24,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:26,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:26,548 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1834483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:26,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:26,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:26,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:26,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:29,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:29,104 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1228285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:29,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:29,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:29,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:29,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:31,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:31,565 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1228285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:31,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:31,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:33,977 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:33,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:33,979 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1228285 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:34,006 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:34,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:34,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:34,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:34,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:34,614 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:41,383 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:41,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:41,385 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:41,413 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:41,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:41,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:42,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:42,082 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 217290 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:48,670 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:48,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:48,673 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:48,698 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:48,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:48,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:49,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:51,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:51,947 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2078060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:52,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:53:58,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:53:58,593 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978035 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:53:58,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:00,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:00,855 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1198712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:00,994 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:00,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:01,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:01,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:03,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:03,215 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1198712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:03,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:04,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:05,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:05,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1198712 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:08,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,688 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:08,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:08,695 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:09,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:09,107 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:09,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:09,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:09,426 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 197685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:09,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:09,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:09,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:09,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:09,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:12,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:12,960 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2289491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:13,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:13,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:13,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:13,824 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:16,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:16,504 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2289491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:17,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:20,437 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:20,438 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2289491 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:20,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:20,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:20,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:21,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:23,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:23,893 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:24,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:24,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:24,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:24,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:27,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:27,252 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:27,820 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:28,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:30,800 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:30,802 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2489275 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:30,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:30,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,138 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:31,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:38,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:38,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:38,456 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4522581 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:38,498 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:39,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:39,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:42,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:42,071 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:42,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:48,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:48,857 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4522581 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:49,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:52,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:52,810 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:53,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:53,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:54:59,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:54:59,659 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4522581 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:54:59,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:00,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:00,438 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:00,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:00,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:00,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:01,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:01,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:01,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:10,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:10,013 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6169299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:10,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:10,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:10,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:10,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:19,947 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:19,949 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6169299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:20,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:20,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:20,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:20,731 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:30,445 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:30,446 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6169299 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:30,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:30,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:30,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:30,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,488 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:31,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:34,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:34,579 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689230 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:37,663 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:37,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:37,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:40,822 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:40,823 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:40,824 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689230 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:40,853 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3713042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:41,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:41,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:43,711 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:43,713 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1689230 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:45,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:49,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:49,178 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3713042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:49,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:52,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:52,919 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2183246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:53,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:55:59,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:59,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:55:59,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4520140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:55:59,925 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3713042 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:00,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:03,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:03,395 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2183246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:03,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:12,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:12,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:12,099 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4520140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:12,145 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978319 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:15,855 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:15,857 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2183246 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:15,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:22,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:22,673 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4520140 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:22,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:29,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:29,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:29,894 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:29,935 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978319 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:30,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:30,496 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:30,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:36,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:36,918 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:43,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:43,150 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978319 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:43,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:43,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:43,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:50,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:50,013 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:50,014 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177094 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:50,056 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:50,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:50,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:56:56,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:56:56,450 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:56:56,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:02,398 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:02,400 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:05,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:09,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:09,425 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:09,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:15,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:15,591 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978325 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:15,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:22,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:22,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4177643 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:22,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:29,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:29,545 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:29,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:36,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:36,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178111 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:37,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:40,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:42,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:42,915 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:43,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:49,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:49,442 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178111 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:57:49,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:57:56,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:57:56,661 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3978347 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:00,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:03,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:03,261 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178111 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:03,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:03,560 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:03,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:03,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:10,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:10,846 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:11,442 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:11,443 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 319162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:11,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:11,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:11,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:18,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:18,388 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:18,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:18,936 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 319162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:19,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:19,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:19,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:26,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:26,217 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4178133 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:26,745 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:26,746 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 319162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:26,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:26,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:27,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:35,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:35,154 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:35,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:35,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:35,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:35,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:44,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:44,223 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:51,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:58:51,133 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:58:51,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:51,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:58:51,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:00,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:00,569 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921183 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:07,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:07,257 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:07,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:07,679 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:08,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:15,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:15,464 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4078142 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:15,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:15,466 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:16,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:16,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:23,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:23,700 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:24,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:24,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:24,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:25,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:34,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:34,211 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4921404 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:34,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:34,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:35,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:36,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:44,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:44,246 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4984119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:44,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:44,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:45,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:45,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:54,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:54,087 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4984119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:54,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 17:59:54,273 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114958 input tokens (60000 > 129024 - 114958). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 17:59:54,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:56,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 17:59:57,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:01,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:01,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:01,799 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4984119 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:01,848 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114958 input tokens (60000 > 129024 - 114958). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:02,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:02,345 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 114958 input tokens (60000 > 129024 - 114958). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:03,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:06,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:06,092 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:13,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:13,700 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5826750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:13,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:13,899 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118910 input tokens (60000 > 129024 - 118910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:14,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:14,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:14,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:24,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:24,459 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5826750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:24,648 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:24,649 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118910 input tokens (60000 > 129024 - 118910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:24,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:33,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:33,257 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5826750 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:33,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:33,640 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118910 input tokens (60000 > 129024 - 118910). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:37,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:37,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:37,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:45,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:45,893 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6585760 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:46,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:46,097 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116845 input tokens (60000 > 129024 - 116845). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:46,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:00:55,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:55,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:55,896 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6585760 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:55,951 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70223 input tokens (60000 > 129024 - 70223). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:56,302 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:56,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:56,304 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:56,309 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116845 input tokens (60000 > 129024 - 116845). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:57,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:00:57,128 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70223 input tokens (60000 > 129024 - 70223). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:00:57,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:07,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:07,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:07,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6585760 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:07,531 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:08,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:08,183 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:08,185 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70223 input tokens (60000 > 129024 - 70223). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:08,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116845 input tokens (60000 > 129024 - 116845). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:09,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:09,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:09,471 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:09,485 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:09,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:10,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:10,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:10,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:10,986 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:10,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:11,000 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:11,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:11,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:11,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:12,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:12,436 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 688994 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:12,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:12,451 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:12,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:12,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:13,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:13,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:13,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:13,903 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:13,913 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:14,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:14,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:15,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:15,244 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:15,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:15,258 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70441 input tokens (60000 > 129024 - 70441). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:16,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:16,616 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:16,618 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689139 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:16,632 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70441 input tokens (60000 > 129024 - 70441). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:18,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:18,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:18,013 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:18,027 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 70441 input tokens (60000 > 129024 - 70441). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:19,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:19,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:19,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:19,395 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:19,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:19,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:19,665 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:20,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:20,771 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 689163 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:21,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:21,671 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79329 input tokens (60000 > 129024 - 79329). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:21,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:21,864 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:21,865 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79329 input tokens (60000 > 129024 - 79329). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:22,098 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79329 input tokens (60000 > 129024 - 79329). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:22,311 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:22,521 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:22,739 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 80440 input tokens (60000 > 129024 - 80440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:22,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:22,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:22,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:38,709 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69094 input tokens (60000 > 129024 - 69094). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:38,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,844 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:38,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:38,957 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69094 input tokens (60000 > 129024 - 69094). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:39,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,225 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:39,227 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69094 input tokens (60000 > 129024 - 69094). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:39,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:39,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:49,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:50,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:52,773 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:52,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:52,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:52,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:53,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:53,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90440 input tokens (60000 > 129024 - 90440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:53,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:53,263 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90440 input tokens (60000 > 129024 - 90440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:53,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:53,494 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 90440 input tokens (60000 > 129024 - 90440). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:53,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:53,725 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:53,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:53,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:54,357 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:54,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:54,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:54,841 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69126 input tokens (60000 > 129024 - 69126). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:54,846 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:54,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:54,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:55,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:55,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:55,379 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69126 input tokens (60000 > 129024 - 69126). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:55,383 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 233979 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:55,586 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:01:55,587 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69126 input tokens (60000 > 129024 - 69126). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:01:56,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:56,568 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:01:56,575 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:05,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:05,301 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5967079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:05,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:05,624 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69293 input tokens (60000 > 129024 - 69293). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:10,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:10,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:10,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:14,919 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:14,921 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5967079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:15,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:15,101 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69293 input tokens (60000 > 129024 - 69293). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:15,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:25,173 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:25,174 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5967079 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:25,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:25,550 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69293 input tokens (60000 > 129024 - 69293). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:26,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:33,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:33,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:37,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:37,160 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:38,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:40,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:40,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:40,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:48,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:48,900 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:49,331 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:02:49,332 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69171 input tokens (60000 > 129024 - 69171). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:02:49,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:49,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:02:49,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:01,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:01,795 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078174 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:02,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:02,003 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69171 input tokens (60000 > 129024 - 69171). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:02,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:11,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:11,442 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:11,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:11,987 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 69171 input tokens (60000 > 129024 - 69171). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:12,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:21,526 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:21,721 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:24,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:24,169 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:36,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:36,326 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078179 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:36,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:36,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:45,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:45,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:45,676 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72702 input tokens (60000 > 129024 - 72702). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:45,681 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:45,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:46,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:46,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:47,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:47,244 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72702 input tokens (60000 > 129024 - 72702). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:55,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:55,982 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:03:56,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:56,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:56,390 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:03:56,404 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:03:56,405 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72702 input tokens (60000 > 129024 - 72702). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:05,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:05,463 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6078184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:06,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:06,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:06,524 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79338 input tokens (60000 > 129024 - 79338). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:06,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:06,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:06,876 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79338 input tokens (60000 > 129024 - 79338). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:07,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:07,546 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79338 input tokens (60000 > 129024 - 79338). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:19,227 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:19,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:20,017 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:20,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:20,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:20,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:20,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:24,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:24,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:24,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:24,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:24,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:25,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:25,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:26,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:33,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:33,041 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5398649 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:33,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:33,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:33,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:41,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:41,181 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5398649 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:43,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:50,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:50,065 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5398649 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:51,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:51,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:51,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:51,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:58,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:04:58,870 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5645839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:04:59,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:04:59,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:07,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:07,999 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5645839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:12,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:12,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:12,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:12,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:18,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:18,419 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5645839 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:19,248 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:19,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:20,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:27,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:27,532 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5734987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:28,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:28,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:28,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:36,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:36,346 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5734987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:36,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:37,064 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:37,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:45,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:45,110 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79351 input tokens (60000 > 129024 - 79351). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:45,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:45,116 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5734987 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:45,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:45,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:46,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:55,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:55,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:55,320 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79351 input tokens (60000 > 129024 - 79351). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:55,325 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5889685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:05:55,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:55,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:55,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:05:55,874 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:05:55,876 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 79351 input tokens (60000 > 129024 - 79351). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:04,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:04,496 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5889685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:04,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:04,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:04,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:04,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:13,136 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:13,138 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5889685 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:13,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:13,647 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:13,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:22,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:22,495 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:22,713 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:22,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:22,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:22,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:31,541 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:31,542 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:31,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:40,778 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:40,779 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5934597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:42,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:42,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:42,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:42,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:50,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:06:50,613 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6251665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:06:50,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:50,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:51,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:06:51,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:00,634 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:00,636 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6251665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:01,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:01,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:01,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:01,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:11,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:11,972 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6251665 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:12,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:12,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:12,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:13,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:22,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:22,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:22,812 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 288412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:22,821 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6934668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:23,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:23,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:23,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:23,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:23,469 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 288412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:32,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:32,985 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6934668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:33,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:33,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:33,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:33,728 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 288412 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:33,815 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:43,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:43,250 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6934668 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:43,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:43,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:43,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:54,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:07:54,536 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7358020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:07:55,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:55,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:07:55,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:05,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:05,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:05,515 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 252896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:05,524 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7358020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:07,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:07,115 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606659 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:07,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:17,640 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:17,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:17,642 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 252896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:17,651 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7358020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:17,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:19,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:19,174 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606659 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:19,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:30,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:30,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:30,367 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 252896 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:30,371 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984556 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:31,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:31,893 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 606659 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:32,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:33,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:33,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:45,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:45,563 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984556 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:46,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:46,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:46,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:58,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:08:58,121 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984556 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:08:58,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:58,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:58,842 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:08:58,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:11,416 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:11,417 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:12,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:12,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:12,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:24,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:24,919 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:25,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:25,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:25,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:25,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:37,570 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:37,571 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984560 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:37,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:37,792 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:37,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:37,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:51,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:09:51,277 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:09:51,490 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:51,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:51,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:09:51,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:05,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:05,217 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:05,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:05,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:05,793 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:08,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:20,780 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:20,781 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984583 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:20,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:20,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:21,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:21,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:36,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:36,599 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:37,292 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:37,293 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:51,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:10:51,202 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:10:51,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:51,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:51,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:10:53,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:04,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:04,451 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 7984626 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:04,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:05,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:05,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:05,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:09,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:20,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:20,847 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6944780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:21,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:21,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:21,044 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:23,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:33,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:33,644 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6944780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:34,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:34,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:34,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:36,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:45,455 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:45,456 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6944780 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:45,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:45,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:45,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:55,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:11:55,698 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6173373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:11:56,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:56,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:56,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:11:56,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:05,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:05,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6173373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:06,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:07,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:07,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:08,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:16,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:16,515 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6173373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:17,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:17,348 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:18,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:18,471 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 641868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:18,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:18,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:18,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:19,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:19,816 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:19,817 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 641868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:19,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:20,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:20,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:21,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:21,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:21,316 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 641868 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:21,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:21,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:21,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:22,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:22,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 670619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:23,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:23,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:23,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:24,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:24,195 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 670619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:24,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:24,658 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:24,660 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:24,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:26,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:26,022 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 670619 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:26,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:26,558 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:27,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:27,095 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 234892 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:27,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:27,593 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 222483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:28,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:28,150 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 222483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:28,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:28,698 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 222483 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:40,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:40,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:40,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:40,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,295 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:41,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:42,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:42,574 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:42,746 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:42,751 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:42,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:43,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:43,834 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:43,836 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:44,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:44,386 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 321273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:44,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:44,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:45,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:45,735 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 650651 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:46,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:46,423 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 321273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:47,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:12:47,158 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 321273 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:12:57,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:57,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:57,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:57,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:58,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:58,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,205 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,511 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,517 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:12:59,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:00,286 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:00,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:00,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:00,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:01,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:12,034 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:12,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:12,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:12,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:13,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,674 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:14,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,801 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:15,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:16,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:16,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:16,167 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 104480 input tokens (60000 > 129024 - 104480). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:16,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:16,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:16,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:16,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:16,419 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 104480 input tokens (60000 > 129024 - 104480). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:16,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:16,734 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 104480 input tokens (60000 > 129024 - 104480). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:28,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:28,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:28,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:28,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:28,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:28,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:40,724 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:41,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:54,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:54,934 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:54,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:55,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:55,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:55,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:55,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:55,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:56,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:57,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:57,361 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 667732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:13:57,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:13:58,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:13:58,926 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 667732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:00,282 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:00,284 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 667732 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:11,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:11,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,479 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:12,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,697 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:16,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,146 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,415 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:28,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:39,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:39,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:39,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:43,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:45,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:45,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:45,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:45,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,643 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:46,958 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:47,100 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:47,101 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75398 input tokens (60000 > 129024 - 75398). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:47,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:47,315 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75398 input tokens (60000 > 129024 - 75398). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:47,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:47,550 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 75398 input tokens (60000 > 129024 - 75398). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:47,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:47,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:47,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:47,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,252 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:48,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,020 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:49,371 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 105446 input tokens (60000 > 129024 - 105446). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:49,584 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:49,646 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 105446 input tokens (60000 > 129024 - 105446). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:49,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:14:49,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:14:49,936 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 105446 input tokens (60000 > 129024 - 105446). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:14:50,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,923 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:02,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:03,250 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108596 input tokens (60000 > 129024 - 108596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:03,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,583 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:03,584 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108596 input tokens (60000 > 129024 - 108596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:03,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:03,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:03,969 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108596 input tokens (60000 > 129024 - 108596). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:14,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,729 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:14,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:14,963 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72704 input tokens (60000 > 129024 - 72704). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:15,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:15,178 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72704 input tokens (60000 > 129024 - 72704). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:15,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:15,393 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 72704 input tokens (60000 > 129024 - 72704). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:16,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,738 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:21,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,493 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:22,712 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98523 input tokens (60000 > 129024 - 98523). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:22,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:22,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:23,042 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98523 input tokens (60000 > 129024 - 98523). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:23,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:23,274 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 98523 input tokens (60000 > 129024 - 98523). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:23,553 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,761 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:23,935 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:24,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:24,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:24,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:24,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:24,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:29,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,118 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:39,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,756 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,959 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:40,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,802 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,885 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:41,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:42,878 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:42,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:47,722 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:47,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:47,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:47,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,038 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,094 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,545 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:48,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:49,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:49,712 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 498331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:49,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:49,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:49,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:49,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:50,770 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:50,772 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 498331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:50,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:51,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:51,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:51,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:51,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:51,810 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 498331 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:51,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:52,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:52,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:53,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:53,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:53,398 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:53,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:53,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:53,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:54,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:55,206 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:55,208 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:55,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:55,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:55,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:56,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:57,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:15:57,342 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801969 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:15:57,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:57,544 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:57,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:15:59,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:00,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:00,074 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1094928 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:00,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:00,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:00,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:02,272 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:02,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:02,669 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1094928 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:02,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:02,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:02,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:05,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:05,088 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1094928 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:05,238 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:05,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:05,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:07,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:07,991 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:09,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:09,993 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:10,122 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:10,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:12,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:12,245 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:13,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:13,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:13,997 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:13,998 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:14,017 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:14,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:17,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:17,079 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1089175 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:18,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:18,049 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:19,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:19,694 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:19,695 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1097282 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:19,791 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:21,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:21,837 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 589444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:21,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:21,840 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1090419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:22,221 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:22,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:22,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:24,341 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:24,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:24,551 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1090419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:25,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:27,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:27,551 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1090419 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:29,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:29,400 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:31,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:31,317 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:32,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:32,456 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:32,457 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:33,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1100411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:33,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,598 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:33,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,512 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:34,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:35,089 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:35,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,306 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:35,697 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:35,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:35,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:36,297 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 391611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:36,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:36,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:37,218 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300520 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:37,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,379 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:37,710 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300520 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:37,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:37,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:38,211 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300520 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:38,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,639 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:38,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:39,270 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:39,271 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:39,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:39,420 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:39,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:39,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:39,845 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:39,999 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,400 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:40,401 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 350521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:40,631 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,632 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,908 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:40,992 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,187 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,269 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,702 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:41,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:42,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:42,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:42,261 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:42,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:42,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:42,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:42,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:42,797 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:42,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:43,475 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381153 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:43,719 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,723 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,794 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:43,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,080 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,569 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:44,978 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,051 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,614 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:45,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:46,491 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:46,491 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:46,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:46,646 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:46,716 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:47,108 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:47,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,334 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,436 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:16:47,743 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 300524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:16:47,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:47,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:16:48,055 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:01,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:01,907 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:01,986 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:01,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:02,680 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:02,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:02,682 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,343 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:03,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,065 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:04,072 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303743 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:04,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:04,704 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303743 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:04,984 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,985 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:04,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:05,280 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:05,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 303743 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:05,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:05,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:05,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:05,914 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 339778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:06,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:06,166 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:06,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:06,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:06,660 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 339778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:06,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:06,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:06,839 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92081 input tokens (60000 > 129024 - 92081). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:06,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:07,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:07,432 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 339778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:07,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:07,617 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:07,618 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92081 input tokens (60000 > 129024 - 92081). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:09,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:09,267 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:09,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:09,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:09,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:09,530 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 92081 input tokens (60000 > 129024 - 92081). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:11,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:11,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:11,517 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:13,219 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:13,220 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 826373 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:13,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:13,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:13,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:14,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:17,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:17,926 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2526778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:18,040 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:18,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:20,375 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:20,376 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:21,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:24,939 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:24,940 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2526778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:25,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:27,472 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:27,474 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:27,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:31,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:31,605 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2526778 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:31,762 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:39,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:39,784 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:39,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:39,832 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290689 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:45,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:45,606 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:45,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:48,113 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:48,114 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:48,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:17:56,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:56,064 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:56,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:56,104 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:58,566 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:17:58,567 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:17:58,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:04,571 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:04,573 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169192 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:04,695 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:12,846 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:12,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:12,849 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555920 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:12,895 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:18,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:18,510 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:18,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:21,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:21,516 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544798 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:21,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:29,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:29,785 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:29,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:29,815 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:32,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:32,913 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544798 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:33,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:33,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:40,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:40,910 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3169524 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:40,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:40,950 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:43,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:43,688 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544798 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:43,808 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:43,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:18:52,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:52,552 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4904595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:52,589 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:52,589 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556182 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:55,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:18:55,386 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:18:55,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:02,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:02,872 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4904595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:03,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:10,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:10,083 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4483511 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:10,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:10,130 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:10,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:17,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:17,869 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4904595 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:18,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:24,989 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:24,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:24,991 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4483511 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:25,036 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545263 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:32,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:32,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:33,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:33,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:40,054 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:40,056 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4483511 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:40,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:47,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:47,786 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:47,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:19:54,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:54,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:19:54,594 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555193 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:54,641 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:19:54,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:02,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:02,060 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5244476 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:02,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:09,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:09,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:09,155 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555193 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:09,201 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:09,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:17,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:17,323 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:17,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:24,461 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:24,462 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:24,463 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4555193 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:24,508 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520663 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:32,636 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:32,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:32,639 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:32,689 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:35,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:35,503 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1508630 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:35,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:43,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:43,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:43,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:43,934 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245041 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:43,982 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:43,987 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4194335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:46,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:46,729 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1508630 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:46,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:20:54,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:54,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:54,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:54,786 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245117 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:54,830 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690001 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:54,836 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4194335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:58,340 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:20:58,341 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1508630 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:20:58,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:04,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:04,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:04,578 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:04,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4194335 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:12,892 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:12,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:12,894 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245117 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:12,942 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1489849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:13,073 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:14,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:14,494 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:14,953 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:22,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:22,216 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:22,217 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245117 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:22,265 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1489849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:23,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:23,511 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690039 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:23,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:30,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:30,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:30,215 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4641318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:30,261 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1489849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:38,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:38,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:38,240 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:38,288 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:40,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:40,998 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1523298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:41,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:49,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:49,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:49,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:49,030 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:49,078 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:49,083 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4641318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:51,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:51,868 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1523298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:51,968 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:21:58,996 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:58,998 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:21:58,999 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 690156 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:21:59,000 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4641318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:06,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:06,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:06,653 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5245168 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:06,654 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1523298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:06,911 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:14,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:14,133 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:14,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:16,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:16,874 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:17,004 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:23,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:23,928 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:24,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:26,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:26,269 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:26,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:32,969 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:32,970 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667162 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:33,132 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:34,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:34,982 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289411 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:35,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:35,189 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:41,424 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:41,425 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4167733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:41,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:43,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:43,720 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1478098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:43,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:49,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:49,638 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4167733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:49,785 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:51,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:51,972 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1478098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:52,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:22:58,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:22:58,776 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4167733 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:22:58,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:01,303 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:01,304 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1478098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:01,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:08,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:08,255 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4114450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:08,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:11,085 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:11,086 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1524098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:11,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:17,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:17,221 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4114450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:17,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:19,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:19,753 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1524098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:19,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:19,949 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:25,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:25,687 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4114450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:25,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:28,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:28,206 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1524098 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:28,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:35,075 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:35,077 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4477722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:35,226 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:37,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:37,153 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:37,276 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:37,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:43,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:43,524 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4477722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:43,678 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:45,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:45,574 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:45,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:45,669 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:52,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:52,746 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4477722 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:52,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:54,946 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:23:54,947 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290707 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:23:55,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,068 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,349 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,350 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,523 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:23:56,612 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:03,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:03,489 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:06,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:06,107 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1539067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:06,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:06,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:06,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:13,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:13,651 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:16,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:16,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:16,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1539067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:16,148 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 750574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:16,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:16,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:24,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:24,443 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556701 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:27,048 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:27,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:27,051 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1539067 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:27,076 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 750574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:27,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:27,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:32,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:32,116 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:34,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:34,241 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:34,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:34,265 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 750574 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:34,371 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:34,376 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:39,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:39,332 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:41,948 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:41,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:41,951 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:41,973 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:42,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:42,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:46,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:46,913 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2278203 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:49,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:49,654 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:49,655 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1520046 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:49,659 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:49,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:49,975 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:53,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:53,975 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:55,407 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:55,408 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762413 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:24:55,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:55,637 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:55,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:24:59,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:24:59,500 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:01,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:01,815 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290255 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:02,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:02,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:02,116 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:05,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:05,961 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2048020 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:08,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:08,130 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290255 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:08,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:08,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:08,283 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:15,983 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:15,985 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4557517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:18,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:18,182 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290255 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:18,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:18,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:18,393 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:26,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:26,257 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4557517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:26,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:26,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:26,454 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:26,546 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:35,046 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:35,048 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4557517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:37,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:37,591 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:37,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:37,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:37,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:45,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:45,461 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:47,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:47,885 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:48,006 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:48,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:48,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:56,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:56,033 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:58,486 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:25:58,487 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544605 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:25:58,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:58,753 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:25:58,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:06,756 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:06,757 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3666984 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:08,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:08,934 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:09,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:09,077 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:09,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:18,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:18,126 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4546901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:20,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:20,300 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:20,409 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:20,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:20,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:29,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:29,387 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4546901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:31,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:31,578 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1289416 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:31,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:31,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:38,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:38,946 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4546901 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:39,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:41,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:41,716 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:41,814 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:41,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:48,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:48,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4211914 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:48,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:50,701 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:50,703 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:50,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:26:57,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:26:57,412 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4211914 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:26:57,560 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:00,052 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:00,054 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1544599 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:00,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:06,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:06,420 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4211914 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:06,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:08,768 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:08,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:08,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:16,007 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:16,008 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:16,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:18,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:18,299 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:18,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:18,449 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:25,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:25,483 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:25,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:27,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:27,560 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1290269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:27,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:27,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:34,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:34,847 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4556148 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:34,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:37,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:37,496 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:37,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:38,635 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:38,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:40,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:40,558 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:40,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:40,744 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:41,552 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:41,633 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:44,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:44,881 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1545262 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:45,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:45,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:50,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:50,828 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:51,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:51,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:51,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:51,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:58,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:27:58,102 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:27:58,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:58,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:58,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:27:58,320 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:05,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:05,506 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3667579 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:05,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:05,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:05,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:05,677 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:13,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:13,953 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:14,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:14,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:14,109 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:14,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:22,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:22,994 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:23,167 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:23,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:23,170 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:23,285 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:32,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:32,100 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4386937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:32,426 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:32,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:32,428 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:32,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:41,525 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:41,526 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4495772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:41,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:41,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:41,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:41,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:50,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:28:50,927 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4495772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:28:51,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:51,300 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:51,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:28:51,314 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:00,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:00,290 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4495772 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:00,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:00,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:00,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:00,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:09,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:09,889 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4558747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:10,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:10,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:10,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:10,598 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:19,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:19,479 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4558747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:20,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:20,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:20,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:20,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:29,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:29,062 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4558747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:29,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:29,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:30,333 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:30,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:38,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:38,382 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4281318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:38,554 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:38,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:39,645 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:40,176 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:47,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:47,537 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4281318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:47,696 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:47,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:48,809 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:51,727 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:56,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:29:56,586 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4281318 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:29:58,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:58,088 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:29:58,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:00,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:06,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:06,129 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:06,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:06,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:06,395 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:06,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:15,615 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:15,616 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:15,789 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:15,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:15,854 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:15,856 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:25,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:25,279 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4667367 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:25,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:25,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:25,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:25,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:27,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:27,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:27,787 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:27,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:28,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:28,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:28,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:28,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:28,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,180 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,239 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,532 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,534 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:29,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:41,247 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:41,248 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8389349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:41,474 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:41,475 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:41,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:41,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:54,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:30:54,673 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8389349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:30:55,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:56,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:56,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:30:56,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:07,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:07,374 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 8389349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:07,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:07,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:07,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:07,613 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83879 input tokens (60000 > 129024 - 83879). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:07,720 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:18,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:18,859 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6889337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:19,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:19,089 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83879 input tokens (60000 > 129024 - 83879). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:19,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:19,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:19,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:31,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:31,502 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6889337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:31,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:31,719 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 83879 input tokens (60000 > 129024 - 83879). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:31,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:31,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:31,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:44,777 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:44,778 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6889337 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:45,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:31:45,222 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:31:45,362 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:45,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:31:45,413 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:02,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:02,904 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:03,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:03,349 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:03,469 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:03,518 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:03,555 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:20,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:20,852 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:21,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:21,719 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 247486 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:21,841 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:21,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:21,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:40,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:40,765 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889747 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:41,974 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:41,975 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:42,101 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:42,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:42,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:55,691 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:55,692 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6500458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:56,906 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:32:56,907 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:32:57,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:57,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:32:57,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:10,476 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:10,477 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6500458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:11,588 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:11,589 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 360523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:12,381 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:12,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:12,531 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:25,451 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:25,452 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6500458 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:26,712 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:26,713 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:26,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:26,798 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:43,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:43,544 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:43,805 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:44,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:44,592 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:44,676 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:33:59,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:33:59,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:33:59,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:00,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:00,197 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400425 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:00,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:14,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:14,622 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889464 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:14,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:14,861 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:15,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:15,798 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 458249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:15,912 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:23,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:23,544 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116385 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:23,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:23,709 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:24,482 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:24,484 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 458249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:24,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:31,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:31,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:31,759 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116385 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:31,806 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113461 input tokens (60000 > 129024 - 113461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:31,909 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:32,788 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:32,789 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 458249 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:32,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:40,356 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:40,357 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:40,359 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5116385 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:40,434 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113461 input tokens (60000 > 129024 - 113461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:40,539 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:34:47,755 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:34:47,756 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:34:47,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:02,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:02,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:02,097 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9888985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:02,172 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 113461 input tokens (60000 > 129024 - 113461). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:02,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:10,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:10,293 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:10,452 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:24,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:24,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:24,501 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9888985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:24,577 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 174495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:32,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:32,575 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262473 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:32,736 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:33,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:33,808 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 174495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:33,922 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:48,591 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:48,593 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:48,594 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9888985 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:48,651 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:48,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:49,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:49,914 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 174495 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:50,032 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:35:59,704 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:59,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:35:59,706 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6598498 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:35:59,762 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:00,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:00,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:00,547 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:11,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:11,415 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262523 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:21,172 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:21,173 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6598498 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:21,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:21,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:21,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:32,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:32,064 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262863 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:41,910 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:41,911 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6598498 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:36:42,147 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:42,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:42,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:36:54,686 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:36:54,687 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262863 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:10,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:10,267 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9411445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:10,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:10,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:10,548 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:22,771 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:22,772 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5262863 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:38,012 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:38,013 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9411445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:37:38,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:38,290 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:38,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:37:51,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:37:51,504 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5589125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:07,577 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:07,578 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9411445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:08,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:08,277 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:08,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:18,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:18,896 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6778237 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:18,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:18,967 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5589125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:19,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:19,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:20,825 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:32,035 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:32,036 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6778237 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:32,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:32,105 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5589125 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:32,882 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:32,883 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:32,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:32,988 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:43,192 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:43,193 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6778237 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:43,385 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:43,386 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:43,396 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:43,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:53,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:38:53,385 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6263875 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:38:53,843 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:53,845 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:53,913 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:38:53,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:05,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:05,058 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6263875 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:06,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:06,036 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:06,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:06,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:14,699 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:14,700 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6263875 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:15,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:15,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:15,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:15,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:30,597 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:30,599 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:30,893 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:30,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:30,970 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:30,971 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:46,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:39:46,624 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:39:47,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:47,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:47,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:39:47,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:04,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:04,967 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889243 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:05,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:05,653 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:06,380 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:06,381 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475408 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:06,478 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:21,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:21,743 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:22,015 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:23,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:23,216 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475408 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:23,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:38,596 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:38,597 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:38,860 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:40,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:40,103 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 475408 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:40,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:40,294 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:55,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:40:55,179 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889463 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:40:55,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:56,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:40:56,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:11,459 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:11,460 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:11,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:12,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:12,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:12,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:12,579 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:33,812 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:33,814 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:34,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:34,621 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:34,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:34,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:34,859 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:54,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:54,676 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889849 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:55,739 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:55,741 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:55,747 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:41:55,748 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:41:55,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:41:55,889 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:16,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:16,269 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889312 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:17,351 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:17,353 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:17,354 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:17,360 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191204 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:17,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:17,558 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:39,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:39,041 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889312 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:40,057 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:40,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:42:40,060 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 176954 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:40,064 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189316 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:42:40,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:42:40,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:02,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:02,624 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889312 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:03,626 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:03,627 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189316 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:03,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:03,862 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:03,867 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:18,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:18,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:18,281 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9430613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:18,353 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189316 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:19,599 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:19,649 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:20,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:20,701 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:20,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:34,930 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:34,932 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9430613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:35,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:37,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:37,087 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:37,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:52,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:52,327 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9430613 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:52,463 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:54,580 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:54,581 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 942628 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:54,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:56,091 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:56,092 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 706799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:56,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:56,255 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:56,487 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:56,488 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189716 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:56,601 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:57,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:57,851 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 706799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:57,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:58,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:58,251 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189716 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:58,364 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:59,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:43:59,682 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 706799 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:43:59,863 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:43:59,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:00,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:00,082 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 189716 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:00,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:15,877 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:15,878 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:16,130 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:18,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:18,196 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:18,285 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:33,853 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:33,854 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:34,112 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:34,114 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:36,022 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:36,023 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116026 input tokens (60000 > 129024 - 116026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:36,026 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:36,026 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:51,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:51,966 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 9889258 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:52,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:44:52,542 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:44:52,543 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116026 input tokens (60000 > 129024 - 116026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:44:52,650 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:02,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:02,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:02,611 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6427447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:02,665 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 946516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:03,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:03,179 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 116026 input tokens (60000 > 129024 - 116026). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:03,291 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:04,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:04,832 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:04,944 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:14,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:14,683 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6427447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:14,871 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:16,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:16,766 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:16,917 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:16,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:26,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:26,205 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6427447 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:26,289 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:28,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:28,235 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944624 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:28,397 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,954 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,955 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:29,972 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:30,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:30,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:30,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:30,936 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,023 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,059 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,137 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,345 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:31,346 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:31,514 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,563 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,574 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:31,761 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:31,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:31,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:32,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 173516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:32,705 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,710 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,790 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,796 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:32,897 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,313 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,354 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,465 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,498 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:33,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:33,626 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118237 input tokens (60000 > 129024 - 118237). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:33,888 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:33,889 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118237 input tokens (60000 > 129024 - 118237). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:34,191 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:34,192 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 118237 input tokens (60000 > 129024 - 118237). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:34,502 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:34,503 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121269 input tokens (60000 > 129024 - 121269). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:34,818 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:34,819 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121269 input tokens (60000 > 129024 - 121269). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:35,143 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:35,145 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 121269 input tokens (60000 > 129024 - 121269). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:35,497 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:35,498 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 135231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:35,852 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:35,854 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 135231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:36,222 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:36,224 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 135231 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:36,594 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:36,595 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:36,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:36,983 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:37,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:37,367 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 138306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:41,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:41,647 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178631 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:44,197 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:44,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:44,273 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:44,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:45,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:45,408 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178631 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:45,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:45,739 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187836 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:45,868 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:45,918 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:45,920 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:49,641 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:49,643 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2178631 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:49,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:49,982 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187836 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:50,142 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:50,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:50,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:50,458 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:50,668 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:50,669 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 187836 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:51,110 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,117 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,223 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,399 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,406 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:51,622 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185484 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:51,799 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,804 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,811 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,813 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:51,950 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:51,951 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185484 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:52,111 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,127 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,199 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:52,278 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:52,280 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 185484 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:53,016 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,066 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,198 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,315 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:53,987 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:53,989 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:54,121 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,171 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,177 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,611 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:54,612 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:54,752 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:54,857 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:55,279 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:55,281 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 402060 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:55,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:55,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:55,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:55,471 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:56,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:56,107 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:56,262 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:56,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:56,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:56,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:56,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:56,965 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:57,271 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:57,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:57,325 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:57,326 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:57,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:57,904 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 472516 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:58,093 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,095 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,208 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,337 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:58,340 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:58,519 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,524 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,530 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:58,742 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:58,929 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:58,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,037 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,139 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:45:59,141 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 162444 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:45:59,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,378 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,384 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,443 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,444 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,585 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:45:59,714 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,193 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,375 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,429 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,430 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,494 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:00,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,360 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,509 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,573 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:01,625 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:02,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:02,504 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:02,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:02,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:02,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:03,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,251 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,256 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,607 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,613 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,692 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,931 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:04,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,159 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,161 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,162 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,288 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:05,367 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,837 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,932 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:06,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,014 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,609 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,660 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,662 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,797 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,890 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,891 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:07,993 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,242 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,610 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,775 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:08,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,056 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,086 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,214 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,268 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,387 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,508 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,828 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,831 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:09,887 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:10,061 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:10,847 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,009 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,391 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,441 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:11,503 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:12,372 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,103 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,734 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:13,803 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:14,087 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:14,089 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191787 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:14,521 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:14,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:14,522 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191787 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:14,526 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106505 input tokens (60000 > 129024 - 106505). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:14,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:14,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:14,664 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,003 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,004 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 191787 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:15,008 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106505 input tokens (60000 > 129024 - 106505). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:15,246 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,248 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106505 input tokens (60000 > 129024 - 106505). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:15,464 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,466 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107831 input tokens (60000 > 129024 - 107831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:15,685 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,686 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107831 input tokens (60000 > 129024 - 107831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:15,876 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,938 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,941 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:15,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:15,951 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 107831 input tokens (60000 > 129024 - 107831). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:16,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:16,231 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108395 input tokens (60000 > 129024 - 108395). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:16,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:16,422 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:16,423 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:16,495 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:16,497 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108395 input tokens (60000 > 129024 - 108395). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:16,549 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:16,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:16,896 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:18,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:18,002 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:18,018 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:18,019 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108395 input tokens (60000 > 129024 - 108395). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:18,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:18,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:18,304 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:19,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:19,365 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:19,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:19,383 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108299 input tokens (60000 > 129024 - 108299). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:19,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:19,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:19,943 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:21,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:21,121 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762073 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:21,125 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:21,127 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108299 input tokens (60000 > 129024 - 108299). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:21,450 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:21,452 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108299 input tokens (60000 > 129024 - 108299). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:21,505 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:21,506 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:21,522 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:21,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:22,342 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:22,343 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381783 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:23,105 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:23,106 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381783 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:23,886 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:23,888 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 381783 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:24,894 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:24,900 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:24,981 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:24,982 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:25,622 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:25,623 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762026 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:25,821 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:25,826 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106179 input tokens (60000 > 129024 - 106179). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:26,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:26,966 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:27,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:27,538 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:27,539 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762026 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:27,672 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:27,673 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106179 input tokens (60000 > 129024 - 106179). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:27,774 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:27,838 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:28,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:28,938 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762026 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:29,050 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:29,165 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:29,167 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 106179 input tokens (60000 > 129024 - 106179). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:29,254 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:30,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:30,484 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761818 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:30,703 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:30,704 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108283 input tokens (60000 > 129024 - 108283). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:30,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:30,921 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:33,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:33,131 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:33,132 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761818 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:33,149 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:33,414 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:33,415 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108283 input tokens (60000 > 129024 - 108283). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:33,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:34,967 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:34,968 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761818 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:35,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:37,468 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:37,470 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:37,471 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:37,493 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "'max_tokens' or 'max_completion_tokens' is too large: 60000. This model's maximum context length is 129024 tokens and your request has 108283 input tokens (60000 > 129024 - 108283). None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:39,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:39,064 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:39,149 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:41,557 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:41,559 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:41,560 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1274269 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:41,586 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:41,960 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:43,082 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:43,084 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:43,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:46,261 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:46,264 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:46,265 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1576611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:46,289 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:48,024 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:48,025 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761846 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:48,403 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:50,717 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:50,718 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:50,719 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1576611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:50,745 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 172526 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:50,880 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:51,060 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:51,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:51,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:51,419 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:54,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:54,099 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1576611 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:54,370 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:54,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:54,419 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:54,643 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:54,644 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:56,089 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:56,090 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:56,091 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:56,106 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 951333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:56,212 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:56,213 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:56,590 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:56,592 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 250077 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:58,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:58,435 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:58,437 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:58,453 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 951333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:46:58,675 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:58,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:46:59,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:46:59,034 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 249992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:00,806 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:00,807 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:00,809 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761824 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:00,813 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 951333 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:00,962 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:00,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:01,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:01,317 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 249992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:03,925 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:03,926 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1571937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:04,063 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:04,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:04,108 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:04,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:04,428 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 249992 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:07,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:07,023 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1571937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:07,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:07,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:07,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:07,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:07,673 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:09,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:09,831 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:09,848 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:09,849 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1571937 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:10,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:10,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:10,382 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:10,383 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:13,039 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:13,041 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:13,058 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:13,060 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1569357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:13,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:13,603 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400157 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:14,150 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:15,305 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:16,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:16,236 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:16,237 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:16,253 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1569357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:16,819 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:16,820 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:16,905 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:19,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:19,759 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:19,760 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:19,776 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1569357 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:20,062 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:20,287 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:20,289 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:20,373 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:23,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:23,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:23,261 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:23,278 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:23,940 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:23,941 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 400158 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:25,301 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:26,123 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:26,124 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:26,126 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 762166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:26,142 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:26,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:26,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:26,706 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:27,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:27,851 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:30,543 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:30,545 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1572958 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:30,681 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:30,726 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:30,728 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:32,224 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:32,226 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:37,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:37,528 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:37,529 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1529745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:37,555 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3297831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:37,698 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:37,700 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:39,346 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:39,347 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:44,417 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:44,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:44,419 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1529745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:44,445 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3297831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:44,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:44,957 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:46,128 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:47,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:47,630 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1529745 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:52,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:52,211 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3297831 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:52,347 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:53,904 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:53,905 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:53,991 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:47:58,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:58,621 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:47:58,623 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 934190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:58,641 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2835467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:47:58,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:00,492 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:00,494 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:00,581 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:05,963 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:05,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:05,966 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 934190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:05,985 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2835467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:07,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:07,734 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761650 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:07,973 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:08,021 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:11,914 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:11,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:11,916 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 934190 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:11,939 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2835467 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:13,769 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:13,770 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:13,942 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:16,309 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:16,310 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:16,312 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:16,326 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1235907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:17,810 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:17,961 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:17,963 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761756 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:18,234 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:18,284 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:19,743 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:19,745 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1235907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:19,832 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:21,053 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:21,055 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:21,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:26,764 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:26,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:26,767 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1235907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:26,784 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3566532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:27,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:28,240 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:28,241 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:28,327 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:33,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:33,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:33,482 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:33,499 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3566532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:35,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:35,180 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761967 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:37,249 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:40,927 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:40,928 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:40,930 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:40,947 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3566532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:41,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:42,655 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:42,656 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761893 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:42,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:47,527 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:47,529 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:47,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:47,551 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3031284 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:49,250 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:49,251 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761893 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:49,336 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:50,619 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:50,620 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:50,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:54,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:54,851 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:54,852 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 761893 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:54,853 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3031284 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:57,067 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:48:57,068 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:48:57,182 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:48:57,229 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:01,259 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:01,260 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3031284 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:01,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:03,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:03,006 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 944765 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:04,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:05,869 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:07,879 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:07,881 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2923019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:08,011 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:09,535 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:09,536 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:09,620 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:14,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:14,269 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2923019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:15,642 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:15,643 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:17,001 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:17,002 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:17,079 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:21,140 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:21,141 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2923019 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:22,754 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:22,756 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 815445 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:23,244 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:23,245 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:23,321 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:27,659 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:27,660 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:29,260 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:29,261 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:29,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:29,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:29,749 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:34,608 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:34,609 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:36,258 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:36,260 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:36,600 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:36,651 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:36,652 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:42,152 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:42,153 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978150 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:43,758 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:43,759 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 801461 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:44,297 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:44,298 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:44,299 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:44,434 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:46,330 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:46,332 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1565841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:47,556 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:48,829 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:51,358 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:51,360 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:51,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:54,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:54,406 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1565841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:49:54,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:49:59,924 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:49:59,925 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:00,119 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:00,168 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:02,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:02,275 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1565841 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:02,363 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:07,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:07,482 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978306 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:07,606 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:09,185 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:09,187 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 802521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:09,338 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:14,421 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:14,423 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:14,551 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:15,990 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:15,992 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 802521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:16,074 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:20,980 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:20,981 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:21,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:22,572 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:22,573 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 802521 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:22,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:27,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:27,182 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780995 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:27,296 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:29,937 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:29,939 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1409859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:30,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:30,148 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:32,484 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:32,485 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:32,567 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:34,933 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:34,935 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1409859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:35,235 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:36,895 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:36,896 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:37,008 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:39,540 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:39,542 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1409859 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:39,835 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:39,884 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:41,513 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:41,514 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1275450 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:41,623 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:44,203 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:44,205 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1585201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:44,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:44,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:48,952 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:48,953 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:49,081 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:52,120 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:50:52,121 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1585201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:50:52,232 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:50:52,237 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:02,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:02,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:02,905 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:02,945 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6800388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:06,667 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:06,668 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1585201 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:07,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:07,181 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:16,186 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:16,188 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:16,189 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2780976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:16,233 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6800388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:19,872 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:19,873 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1537517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:20,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:20,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:29,318 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:29,319 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:29,321 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3497871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:29,361 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6800388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:32,766 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:32,767 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1537517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:32,858 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:38,447 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:38,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3497871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:38,689 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:38,737 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:40,976 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:40,977 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1537517 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:41,418 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:46,561 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:46,562 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3497871 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:46,870 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:51:55,410 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:51:55,412 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5898957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:51:55,592 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:01,010 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:01,012 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3350608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:01,151 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:09,830 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:09,832 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5898957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:10,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:16,389 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:16,391 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3350608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:16,536 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:16,602 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:25,392 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:25,394 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 5898957 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:25,489 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:31,133 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:31,134 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:31,136 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3350608 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:31,177 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:33,945 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:34,041 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:37,164 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:37,165 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2833341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:37,204 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:37,205 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:37,499 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:37,733 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:37,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:40,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:40,780 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000178 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:40,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:45,257 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:45,258 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2833341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:45,394 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:51,781 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:51,782 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:51,784 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999678 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:51,807 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3974907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:51,951 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:52:56,757 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:52:56,759 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2833341 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:52:56,901 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:03,083 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:03,084 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:03,085 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999678 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:03,112 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3974907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:09,529 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:09,530 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3402360 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:09,684 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:13,126 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:13,128 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999678 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:13,215 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:21,005 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:21,006 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3402360 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:21,049 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:21,050 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3974907 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:21,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:24,411 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:24,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:24,413 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:24,439 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1183884 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:30,644 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:30,645 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3402360 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:30,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:40,365 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:40,366 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:40,368 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:40,422 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1183884 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:45,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:45,407 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2832913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:45,448 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:45,448 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:47,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:47,928 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1183884 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:48,025 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:53:56,425 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:56,427 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:53:56,428 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2000038 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:53:56,452 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:01,201 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:01,202 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:01,204 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2832913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:01,245 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:01,344 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:10,328 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:10,329 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:10,331 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:10,356 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100388 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:15,480 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:15,481 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:15,483 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2832913 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:15,520 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:16,507 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:16,508 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 471681 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:16,618 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:19,307 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:19,308 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:19,309 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:19,335 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184169 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:24,029 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:24,031 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:24,032 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:24,068 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 471681 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:24,153 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:27,141 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:27,142 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:27,233 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:32,098 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:32,099 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:32,101 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:32,139 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 471681 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:32,275 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:35,076 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:35,078 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:35,079 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:35,104 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:46,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:46,045 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:46,046 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2978881 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:46,088 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:49,578 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:49,579 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:49,687 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:54:55,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:55,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:54:55,159 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3354976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:54:55,199 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:03,671 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:03,673 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:03,674 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999708 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:03,700 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:06,708 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:06,710 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176349 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:06,826 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:15,628 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:15,629 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:15,630 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:15,631 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3354976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:15,672 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:15,684 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6100389 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:18,666 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:18,667 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:18,776 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:24,096 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:24,097 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:24,098 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3354976 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:24,138 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:33,500 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:33,501 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:33,502 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6024532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:33,557 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:37,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:37,030 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999888 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:37,115 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:41,194 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:41,195 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:41,197 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2781100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:41,235 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184028 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:50,322 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:50,323 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:50,324 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6024532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:50,380 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1442045 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:52,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:52,213 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 876250 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:52,324 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:55:56,368 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:56,369 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:55:56,370 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2781100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:55:56,410 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1442045 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:05,311 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:05,312 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:05,314 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6024532 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:05,358 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 876250 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:08,144 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:08,145 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1442045 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:08,228 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:12,157 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:12,158 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:12,160 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2781100 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:12,199 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 876250 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:18,750 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:18,751 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4055993 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:18,898 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:18,899 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:23,510 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:23,512 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2975166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:23,779 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:29,483 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:29,485 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:29,486 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4055993 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:29,530 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:34,243 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:34,245 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2975166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:34,412 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:41,069 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:41,070 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:41,071 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:41,072 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:41,096 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4055993 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:41,123 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:46,230 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:46,231 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2975166 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:46,359 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:56:52,104 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:52,106 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:52,107 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:52,109 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999478 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:52,112 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797838 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:52,148 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:57,440 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:56:57,441 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2925912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:56:57,565 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:02,915 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:02,916 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:02,918 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797838 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:02,962 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176351 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:03,072 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:07,873 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:07,875 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2925912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:08,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:14,783 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:14,784 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:14,785 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 3797838 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:14,843 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:20,178 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:20,179 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:20,181 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2925912 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:20,220 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:22,760 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:22,761 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:22,840 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:32,042 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:32,044 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6110402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:32,102 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:32,103 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:36,902 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:36,903 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:36,905 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:36,943 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1184184 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:37,033 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:46,765 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:46,767 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:46,769 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6110402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:46,828 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999428 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:51,865 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:51,867 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:52,000 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:57:55,839 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:57:55,840 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:57:55,926 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:07,154 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:07,155 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:07,156 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:07,157 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2977773 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:07,198 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6110402 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:07,213 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:11,388 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:11,389 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:11,477 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:16,209 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:16,210 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:16,211 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:16,212 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2862592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:16,252 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2001165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:16,266 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:19,956 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:19,957 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999918 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:20,043 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:24,217 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:24,218 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:24,219 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2862592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:24,260 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2001165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:24,274 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:24,274 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1176213 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:28,661 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:28,662 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:28,748 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:31,453 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:31,454 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2001165 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:31,550 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:35,849 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:35,850 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:35,851 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2862592 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:35,896 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:36,316 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:36,317 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:38,352 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:38,354 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 955670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:48,027 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:48,028 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:48,030 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:48,031 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2962690 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:48,073 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6055809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:48,115 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999597 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:58:48,129 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:58:50,656 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:58:50,658 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 955670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:00,265 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:00,266 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:00,267 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:00,268 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2962690 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:00,306 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999898 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:00,318 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6055809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:00,361 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:03,576 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:03,577 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 955670 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:12,603 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:12,604 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:12,605 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:12,606 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 2962690 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:12,607 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 6055809 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:12,652 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999898 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:12,866 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:15,715 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:15,717 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 972382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:21,741 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:21,742 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:21,743 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4132601 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:21,790 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999898 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:21,964 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:21,965 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:24,638 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:24,640 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 972382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:31,562 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:31,564 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:31,565 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4132601 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:31,609 [ERROR] gen_code exception (attempt 1): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:32,169 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:32,220 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:35,253 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:35,254 [ERROR] gen_code exception (attempt 2): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 1999298 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:35,281 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:35,282 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 972382 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:41,405 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-12-10 18:59:41,407 [ERROR] gen_code exception (attempt 3): Error code: 400 - {'error': {'message': "This model's maximum context length is 129024 tokens. However, your request has 4132601 input tokens. Please reduce the length of the input messages. None", 'type': 'BadRequestError', 'param': None, 'code': 400}}
2025-12-10 18:59:41,730 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:41,827 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-10 18:59:41,875 [INFO] HTTP Request: POST http://localhost:8085/v1/chat/completions "HTTP/1.1 200 OK"
