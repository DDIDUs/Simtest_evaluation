{"id": "BigCodeBench/99", "raw_responses": {"test_figure_type": "```plaintext\n[Explanation]\nThe function loads sklearn's iris, builds a pandas DataFrame, sets matplotlib font, creates a seaborn.pairplot (which returns a PairGrid with a .fig attribute that is a matplotlib.figure.Figure), sets a title, and returns g.fig. The unittest only checks that the returned object is an instance of plt.Figure. None of the operations should raise an exception in a typical test environment (rcParams assignment is harmless if Arial isn't present, and PairGrid.fig is a Figure), so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_axes_existence": "```plaintext\n[Explanation]\nThe function loads the iris dataset, builds a DataFrame, and calls sns.pairplot which returns a PairGrid with a .fig attribute. The code sets a suptitle and returns g.fig. Pair plots create multiple axes, so len(fig.axes) > 0. There are no operations that should raise errors in a typical headless test environment (rcParams font fallback is safe). Therefore the test's assertion should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_figure_children": "```plaintext\n[Explanation]\nThe function creates a seaborn pairplot from the iris dataset and returns g.fig (a matplotlib Figure). A pairplot produces multiple Axes and other artists, so fig.get_children() will have length > 0. The code uses standard libraries (matplotlib, pandas, seaborn, sklearn) and does not raise exceptions in normal environments. Therefore the test's assertion should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_plot_labels": "```plaintext\n[Explanation]\nThe function builds a DataFrame with the iris.feature_names (which include \"sepal length (cm)\") and creates a seaborn.pairplot. PairGrid axes include variable names as axis labels; the leftmost/top-left subplot's ylabel will contain \"sepal length (cm)\". The test checks for the substring \"sepal length\" in the concatenated xlabel+ylabel, which will be present. No exceptions are expected from setting the font or using numeric hue, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_plot_title": "```plaintext\n[Explanation]\nThe function creates a seaborn PairGrid via sns.pairplot and assigns a suptitle with g.fig.suptitle(...). That sets the Figure._suptitle Text object, which the test checks for presence and exact text. There are no obvious errors in the code path used by the single test (the hue can be numeric, setting font to Arial does not affect title creation). Thus the figure will have a _suptitle with the expected text.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/227", "raw_responses": {"test_nonexistent_audio_file": "```plaintext\n[Explanation]\nThe function uses os.path.exists(...) to check the audio file's presence and raises FileNotFoundError if it does not exist. The test patches os.path.isfile (not os.path.exists) but calls task_func with 'nonexistent_audio.wav', which very likely does not exist in the test environment, so os.path.exists returns False and the function raises FileNotFoundError as expected. The test only asserts that the exception is raised, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_list_input": "```plaintext\n[Explanation]\nThe test patches os.path.isfile but the code calls os.path.exists to check the audio file. Because exists is not mocked, os.path.exists('audio.wav') returns False and the function raises FileNotFoundError before reaching the reshape that would raise ValueError for the empty list. Thus the test expecting a ValueError will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_successful_matrix_creation": "```plaintext\n[Explanation]\nThe test patches os.path.isfile, but the code checks os.path.exists, so the file-existence check will fail (FileNotFoundError) and the test will not proceed. Additionally, the test expects librosa.display.specshow to be called, but the function never calls it. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_docstring_examples": "```plaintext\n[Explanation]\nThe function checks os.path.exists(audio_file) but the test patches os.path.isfile (not exists), so the file-existence check is not mocked and will likely raise FileNotFoundError (audio.wav is not provided). Other patches (soundfile.read, plt.colorbar, librosa.display.specshow) would be fine, but the incorrect patched function causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_spl_calculation": "```plaintext\n[Explanation]\nThe test patches os.path.isfile but the code checks os.path.exists, so the file-existence check will fail and raise FileNotFoundError (the patch does not intercept os.path.exists). Even if the file check passed, the implementation does not apply the SPL scaling or convert values to dB as the test expects; the function simply normalizes by the matrix max producing values in [0,1], not the negative dB values asserted. Therefore the test will not pass.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/367", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function counts activities by weekday and plots a bar for each day. For the input dates (2023-10-23 Monday, 2023-10-24 Tuesday) the counts list becomes [1, 1, 0, 0, 0, 0, 0]. The created Axes has the expected title and labels, and ax.patches contains the 7 bar rectangles whose heights are 1.0, 1.0, then zeros (1.0 == 1 compares equal). The required datetime name is imported at module level, so the test's datetime references resolve. Therefore the test assertions should all pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function validates datetime objects, counts weekday occurrences (Monday=0..Sunday=6), and plots bars in the order Monday..Sunday. The test input dates correspond to Monday(1), Tuesday(2), Wednesday(0), Thursday(1), and the rest zero \u2014 matching expected heights. Labels and title are set as asserted. Potential float vs int heights compare equal (1 == 1.0). Therefore the code meets the test's checks.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function counts activities by datetime.weekday() (0=Monday,...,6=Sunday). 2023-10-29 is a Sunday so both activities map to index 6, producing counts [0,0,0,0,0,0,2]. The bar chart has seven bars in Monday->Sunday order; bars[0..5] heights 0 and bars[6] height 2, matching the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function accepts an empty list (no TypeError raised), builds counts = [0]*7, and creates a bar chart with seven bars of height 0. ax.patches will contain those rectangles whose get_height() returns 0. Calling task_func([1,2,3]) triggers the isinstance(datetime) check and raises TypeError. Thus the test's assertions and exception check will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function validates datetime objects, counts weekday occurrences, and plots bars for Monday\u2013Sunday. The test provides one date for each weekday (2023-10-23 through 2023-10-29), so each bar's height will be 1 (as float 1.0 equals 1). No other errors occur in this context, so the assertions should all pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/501", "raw_responses": {"test_valid_json": "```plaintext\n[Explanation]\nThe function parses the JSON array of dictionaries, writes headers and two data rows to an .xls file using xlwt, saves it under the given filename, and returns the absolute path. The test checks that the returned path exists and then removes the file. There are no logic errors for this input, so the file will be created and os.path.exists(file_path) will be True (assuming xlwt is available in the environment).\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_json": "```plaintext\n[Explanation]\nThe input JSON has a trailing comma, so json.loads(json_str) raises json.JSONDecodeError. The code catches JSONDecodeError and re-raises it as ValueError, which matches the test's expectation that ValueError is raised.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_json": "```plaintext\n[Explanation]\nThe function parses '[]' into an empty list, creates an xlwt Workbook with a sheet and (intentionally) writes no cells, then saves 'test_empty.xls' and returns its path. The saved .xls exists, pandas.read_excel will read the (empty) sheet into an empty DataFrame, so df.empty is True. Therefore the assertions in the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_string_json": "```plaintext\n[Explanation]\nThe function begins by checking if json_str is an instance of (str, bytes, bytearray) and immediately raises TypeError if not. Calling task_func(12345, ...) passes an int, so the TypeError is raised as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_custom_sheet_name": "```plaintext\n[Explanation]\nThe function uses xlwt to create and save an .xls file with the provided sheet name. Given the input JSON list of one dict, it writes headers and one data row and calls workbook.save(filename). The test only checks that the returned absolute path exists; the code returns os.path.abspath and should create the file successfully. No exceptions are raised for this input.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_content": "```plaintext\n[Explanation]\nThe function writes an .xls file correctly with the expected headers and values, but pandas.read_excel typically reads numeric Excel cells as float64 while pd.read_json produces int64 for the \"Age\" column. pd.testing.assert_frame_equal checks dtypes by default, so the dtype mismatch (int64 vs float64) will cause the test to fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/579", "raw_responses": {"test_valid_csv_file": "```plaintext\n[Explanation]\nThe patched open provides CSV text \"word1,word2\\nword3,word4\". The function reads cells, strips non-alpha characters so \"word1\"/\"word2\"/\"word3\"/\"word4\" all become \"word\", yielding Counter([('word', 4)]) and returns a matplotlib Axes and a list of most common words. mock_open supports iteration and accepts encoding kwarg; matplotlib axes object type matches the isinstance check. Therefore the assertions (Axes instance and list) will succeed.\n\n[Result]\nPASS\n```", "test_file_not_found_error": "```plaintext\n[Explanation]\nThe function attempts to open the given CSV file inside a try block; if the file does not exist the open() call raises FileNotFoundError which the function catches and re-raises as FileNotFoundError. The test expects a FileNotFoundError when calling task_func('non_existent.csv'), so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```", "test_io_error": "```plaintext\n[Explanation]\nThe test patches builtins.open to raise an IOError when called. In task_func the open call is inside a try/except that catches IOError and re-raises an IOError. Therefore task_func will raise IOError as the test expects, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_plot_output": "```plaintext\n[Explanation]\nThe mocked CSV contains \"word1,word1\\nword2,word2\". The function strips non-alphabetic characters, turning \"word1\" and \"word2\" into \"word\". All four cells become \"word\", so Counter yields one entry ('word', 4). The code therefore plots a single bar (len(ax.patches) == 1), but the test expects 2 bars. Hence the test will fail.\n\n[Result]\nFAIL\n```", "test_normalized_text": "```plaintext\n[Explanation]\nThe test mocks the CSV content as 'Caf\u00e9,Caf\u00e9\\nNi\u00f1o,Ni\u00f1o' and asserts that ('Cafe', 2) is in the most_common_words. The function normalizes accents correctly to 'Cafe' and 'Nino' but then lowercases all text (full_text.lower()), so the returned tuples are ('cafe', 2) and ('nino', 2). Because of the lowercase conversion, ('Cafe', 2) is not present, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/760", "raw_responses": {"test_dataframe_structure": "```plaintext\n[Explanation]\nThe function constructs a pandas DataFrame with 100 rows and columns 'ID', 'Name', 'Date of Birth', 'Email'. With rng_seed=1 the random operations run without error. All columns are created with matching lengths and the dict insertion order yields the expected column order. No exceptions are raised for the provided names when encoding/decoding. Therefore the test assertions (instance, columns, length) will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_randomness_and_encoding": "```plaintext\n[Explanation]\nThe function selects names from the provided latin_names + other_names via numpy.choice, so all Name values will be from the given lists. Emails are constructed with the default email_domain 'example.com' (the test does not override it), so every email contains '@example.com'. No other behavior in the code prevents these conditions.\n\n[Result]\nPASS\n```", "test_custom_parameters": "```plaintext\n[Explanation]\nThe function generates 100 records with emails formatted as \"{name}{year}@{email_domain}\" and years chosen via np.random.randint(start_year, end_year + 1). Given email_domain='test.com', every email will end with '@test.com'. Date of Birth values are datetime.date objects with year drawn between start_year and end_year inclusive. The code runs without raising exceptions for the provided inputs, so both assertions in the test will be True.\n\n[Result]\nPASS\n```", "test_invalid_year_range": "```plaintext\n[Explanation]\nThe function calls np.random.randint(start_year, end_year + 1) when generating dates. If start_year > end_year (e.g., 2005 > 2000), numpy raises a ValueError for an invalid range. Thus task_func(start_year=2005, end_year=2000, rng_seed=1) will raise ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```", "test_empty_name_lists": "```plaintext\n[Explanation]\nCalling task_func with both latin_names and other_names empty makes all_names an empty list. The code then calls np.random.choice(all_names, size=100), which raises a ValueError for an empty 'a'. This matches the test's expected exception.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_rng": "```plaintext\n[Explanation]\nBoth calls set the numpy RNG seed to the same value at the start of task_func, and all random draws (np.random.choice and np.random.randint) come solely from NumPy's RNG. The rest of the function is deterministic string/date manipulation. There are no other sources of non-determinism between the two calls, so df1 and df2 will be identical.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/999", "raw_responses": {"test_count_categories_data1": "```plaintext\n[Explanation]\nThe function downloads (mocked), opens (mocked with appropriate CSV content), uses csv.DictReader to get fieldnames and iterate rows, counts values in the 'category' column, and removes the file (os.remove is mocked). The provided mock_open read_data contains exactly 2 cat1, 2 cat2, and 1 cat3 lines, so the returned dict matches the expected result. No exceptions are raised, and mocks cover external operations.\n\n[Result]\nPASS\n```", "test_count_names_data2": "```plaintext\n[Explanation]\nThe test patches urlretrieve, os.remove, and open (with mock_open providing CSV content). The function reads the CSV with csv.DictReader, checks the header, counts values in the 'name' column, and calls os.remove. The mocks prevent network and file deletion side effects. mock_open's behavior works with csv.DictReader here, so the function will return {'Alice': 2, 'Bob': 1, 'Charlie': 1}. The test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_count_categories_data3": "```plaintext\n[Explanation]\nThe test patches urlretrieve, open (with CSV data), and os.remove, so task_func will read the mocked CSV, find the 'category' header, count rows using csv.DictReader, and return {'cat1':2,'cat2':1,'cat3':2}. No exceptions occur and removals are mocked. Therefore the code will produce the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_count_names_data3": "```plaintext\n[Explanation]\nThe test patches urlretrieve, open (with CSV read_data), and os.remove, so no real network or file I/O occurs. task_func calls urlretrieve (mocked), opens the mocked file (mock_open provides the specified CSV text), DictReader finds the 'name' column, and counts rows: Alice 3, Bob 1, Charlie 1. os.remove is mocked so deletion is harmless. The function returns the expected dict, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_existent_column": "```plaintext\n[Explanation]\nThe test patches urlretrieve, open (with CSV content lacking the requested column), and os.remove. task_func reads the mocked CSV, detects the missing column, calls os.remove (mocked) and raises ValueError. The exception propagates (caught and re-raised by the outer except), so the test's assertRaises(ValueError) will succeed. The mocked open provides the read_data needed by csv.DictReader, so the code will behave as expected.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1085", "raw_responses": {"test_empty_text": "```plaintext\n[Explanation]\nFor an empty input string: cleaned_text is '', split() yields an empty list, Counter produces an empty Counter, and most_common(10) returns []. The function returns that empty list and a matplotlib Axes (ax.bar accepts empty data). The test asserts that common_words == [], so the function meets the expectation.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_word": "```plaintext\n[Explanation]\nThe function lowercases, removes punctuation, splits, counts words with Counter, and returns the most common words list and the matplotlib Axes. For input 'test test test' it returns [('test', 3)]. The test's tearDown calls plt.close(); plt is imported at module level in the provided code, so it's available. Therefore the assertion matches and no exceptions are raised.\n\n[Result]\nPASS\n```", "test_punctuation": "```plaintext\n[Explanation]\nThe function lowercases, removes punctuation with the given regex, splits into words, counts them, and returns the most common list and an Axes. For the input 'hello! hello, world.' it yields [('hello', 2), ('world', 1)]. matplotlib.pyplot is imported as plt so the test's tearDown can call plt.close(). Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_sensitivity": "```plaintext\n[Explanation]\nThe function lowercases the input, strips punctuation, splits into words, and counts frequencies. For 'Hello hello HeLLo' it produces cleaned words ['hello','hello','hello'] and returns [('hello', 3)]. The plotting code returns an Axes but doesn't affect the result; plt is imported at top-level so the test's tearDown can call plt.close(). No errors expected.\n\n[Result]\nPASS\n```", "test_common_scenario": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation, splits words, and uses Counter.most_common(10). For the input \"This is a test. This is only a test.\" the cleaned words sequence is ['this','is','a','test','this','is','only','a','test'], producing counts this/is/a/test = 2 and only = 1. Counter preserves first-seen order for ties, so most_common() yields [('this',2),('is',2),('a',2),('test',2),('only',1)] which matches the expected result. matplotlib.pyplot is imported as plt in the module, so the test's tearDown plt.close() will work.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/100", "raw_responses": {"test_plot_attributes": "```plaintext\n[Explanation]\nThe function computes start_date with end_date.replace(day=end_date.day - 29). With the current date 2025-12-16 (day = 16), this attempts replace(day = -13), which raises a ValueError. The function therefore errors before returning the Axes, so the unittest assertions are never reached.\n\n[Result]\nFAIL\n```", "test_reproducibility": "```plaintext\n[Explanation]\nThe function will usually raise a ValueError when computing start_date because it uses datetime.replace(day=end_date.day - 29). For most days of the month (e.g., day=16), end_date.day - 29 is negative and invalid for replace, causing an exception before any plotting or random number generation. The test will therefore fail (it never reaches the reproducible y-data comparison).\n\nEven if the date arithmetic happened to be valid, the random values themselves are seeded each call so they would match; but the date-related errors prevent the test from running to that point.\n\n[Result]\nFAIL\n```", "test_random_seed_effect": "```plaintext\n[Explanation]\nThe function computes start_date with end_date.replace(day=end_date.day - 29). For most days (including the provided current date 2025-12-16 where day=16) end_date.day - 29 is <=0, causing datetime.replace to raise ValueError (\"day is out of range for month\") before any data is returned. Thus the test will error out and not reach the comparison of generated sequences.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_data_range": "```plaintext\n[Explanation]\nThe function computes start_date = end_date.replace(day=end_date.day - 29). If today's day-of-month is <=29 (e.g. current date 2025-12-16 has day 16), end_date.day - 29 is <=0 which is invalid for datetime.replace and raises ValueError. Even ignoring that, the subsequent use of replace(day=...) to build the date list can overflow month days. Therefore the function will raise an exception and the test will not pass.\n\n[Result]\nFAIL\n```", "test_value_range": "```plaintext\n[Explanation]\nThe code uses end_date.replace(day=end_date.day - 29) to compute start_date. datetime.replace requires a valid day (1-31); subtracting 29 from the current day can produce zero or negative values and raise a ValueError. Given the current date (2025-12-16), end_date.day - 29 = -13, so start_date.replace(...) will raise an exception before any plotting or value generation occurs. Although the random values themselves would be within 1..100 (which satisfies the test's 0..100 check), the function will not complete due to the invalid date calculation, causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_value": "```plaintext\n[Explanation]\nThe function calls end_date = datetime.now() and then start_date = end_date.replace(day=end_date.day - 29). For current dates where end_date.day <= 29 (e.g., today 16 Dec), end_date.day - 29 is non-positive, and datetime.replace will raise a ValueError (day out of range). Thus task_func() raises an exception before generating any random data, so the test cannot compare the expected y-values. Therefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/239", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function shadows the imported scipy.stats by assigning a local variable named stats (a dict). Later it calls stats.norm.pdf(...), which raises AttributeError because stats is a dict, not the scipy.stats module. Additionally, the plot title is never set (ax.get_title() == ''), so the test's final assertion would also fail even if the previous error didn't occur. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function creates a local variable named \"stats\" (a dict), which shadows the imported \"scipy.stats\" module. Later the code calls stats.norm.pdf(...), but \"stats\" is a dict and has no attribute \"norm\", causing an AttributeError and the test to error out. Additionally, the code never sets the Axes title to \"Histogram with PDF\", so even if the name collision were fixed the final title assertion would fail. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function defines a local variable named stats (a dict) that shadows the imported scipy.stats module. Later the code calls stats.norm.pdf(...) which will raise AttributeError ('dict' object has no attribute 'norm') before returning any values. (Even if that were fixed, the test's final assertion misuses assertTrue and would also fail, but the primary failure is the AttributeError.)\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function shadows the imported scipy.stats by assigning a local variable named stats (a dict). Later the code calls stats.norm.pdf, which fails because the local dict has no attribute 'norm', raising AttributeError before the test assertions. (Additionally, the plot title isn't set to \"Histogram with PDF\", but the AttributeError is the primary failure.)\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function overwrites the imported scipy.stats module by assigning the name \"stats\" to a dict of statistics. When it later calls stats.norm.pdf(...), this raises AttributeError ('dict' has no attribute 'norm'), so the function errors before returning. (Also, the code never sets the Axes title to \"Histogram with PDF\", which would cause the test assertion to fail even if the stats name collision were fixed.)\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/368", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists files in the source directory, seeds the RNG, selects a random file, moves it to the destination, and returns the filename. The test only checks that the returned name is one of the original filenames, that the file exists in the destination after the call, and that it ends with \".txt\". Given the created files and using seed=1, the function will return one of those filenames, move it to the dest directory, and satisfy all assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function lists files in the source dir, seeds the RNG with the default seed (100), selects a random file, moves it to the destination, and returns the filename. The test creates five files in src_test_dir_2 and calls task_func with the default seed. The returned filename will be one of those created files, and after shutil.move the file will exist in the destination. The module-level imports (os, shutil) are present for the test code to use. No error conditions are triggered by this test, so the assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function lists files in the source directory and explicitly raises FileNotFoundError when the list is empty. The test calls task_func on an empty source directory and expects FileNotFoundError, so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function lists files in the source directory, seeds the random generator, chooses one file, moves it to the destination directory with shutil.move, and returns the filename. The test sets up src_test_dir_4 with files file11.txt..file15.txt and an existing destination directory, calls task_func with seed=2, and only asserts that the returned name is one of the original filenames, that the file now exists in the destination, and that the name ends with \".txt\". The implementation satisfies these conditions (and the necessary modules are imported at top), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lists files in the provided source directory, seeds the RNG (default 100), selects one file via random.choice, moves it with shutil.move, and returns the filename. The test creates src_test_dir_5 with files file16.txt..file20.txt and dest dir, then calls task_func with default seed. The returned name must be one of the created filenames (true) and the file must exist in the destination after move (shutil.move does this). The test code refers to os/shutil which are imported by the tested module, so names are available. Therefore the implementation satisfies the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/502", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates 7 distinct dates (today and the previous 6 days), creates one random Duration (0\u2013120) per activity per date, and returns a seaborn lineplot Axes and the DataFrame. The DataFrame columns match the expected set, durations are in range, and there are 7 unique dates. seaborn.lineplot returns a matplotlib Axes and the legend contains one entry per activity, so all assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function generates the expected DataFrame with columns {'Date','Activity','Duration'}, durations in 0..120, and produces one entry per activity per date so len(df['Date'].unique()) == days_in_past. seaborn.lineplot returns a matplotlib Axes and creates a legend with one entry per activity, so the legend checks pass. Types and values match the test's assertions.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function builds a DataFrame with one row per activity per day for the specified number of past days. It generates 'dates' as a list of length days_in_past and appends one row per activity for each date, so df['Date'].nunique() == days_in_past for any ndays in [1,5,10,100,500]. The test only checks unique date counts (and does not inspect the plot), so no exceptions are expected. Therefore the code will satisfy the test loop.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function seeds Python's random module at the start of each call and then generates durations with random.randint in a fixed loop order, so repeated calls with the same random_seed produce identical DataFrames. Different seeds produce different random values, so df1 == df2 and df2 != df3 as the test expects. The use of datetime.now().date() for dates is consistent across quick sequential calls (no midnight boundary in the test), and pandas DataFrame construction yields comparable objects for pd.testing.assert_frame_equal. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test expects task_func to raise ValueError when days_in_past is 0 or negative. The provided task_func does not validate days_in_past and will silently proceed (producing an empty dates list, empty DataFrame, and a plot) instead of raising ValueError. Therefore the assertRaises checks in the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/582", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates 1000 normal samples and plots a histogram with 30 bins (so ax.patches will be ~30, >=5) and then plots a single PDF line (one entry in ax.lines). The assertions check only these counts, so the code meets them.\n\n[Result]\n```plaintext\nPASS\n```", "test_standard_functionality": "```plaintext\n[Explanation]\nThe function creates a matplotlib Figure via plt.subplots() and returns it. The test calls task_func() and checks isinstance(fig, plt.Figure). Given the code imports matplotlib.pyplot as plt and returns the Figure object, this isinstance check will pass (assuming the required libraries are available). There are no other failing conditions in the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_varying_sizes": "```plaintext\n[Explanation]\nThe function returns a matplotlib Figure as required, so logic is fine. However, the code imports scipy.stats at the top. If the test environment does not have SciPy installed, importing will raise ModuleNotFoundError before the tests run. Because the provided test only asserts the returned object is a plt.Figure and does not avoid using SciPy, the presence (or absence) of SciPy is critical. In typical minimal test environments SciPy is often not available, so this is likely to cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_histogram_pdf_overlay": "```plaintext\n[Explanation]\nThe test computes expected_bins using numpy's 'auto' binning on the seeded data, then reseeds and calls task_func. task_func uses np.random.normal with the same seed (so data match) but always uses bins=30 for ax.hist. Therefore len(ax.patches) will be 30, which will not equal expected_bins (typically ~22 for these 1000 normal samples). The PDF line check would pass, but the bin-count assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_return_type": "```plaintext\n[Explanation]\nThe function creates a matplotlib Figure via plt.subplots() and returns it. The test asserts that the returned object is an instance of plt.Figure. Given the function returns the Figure object created by matplotlib, the isinstance check will succeed, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/763", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function reads the JSON list [{'key1':10},{'key1':20}], collects key1, computes mean and median using numpy (15.0 each), writes them to CSV, and the test reads and compares floats. There are no mismatches or edge issues for this input, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function reads the JSON, computes mean and median using numpy, writes them to CSV, and returns results. For test2.json the single key \"key2\" yields mean and median 35.0; the CSV will contain that row and the test's float comparisons will succeed. Numpy is used but available in the test. No mismatches or edge cases affect this single-case test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function reads the JSON file with two records [{'key3':50}, {'key3':60}], computes mean and median using numpy (55.0 each), writes a CSV with that single key and values. The test reads the CSV and compares floats with expected 55.0 \u2014 all operations and types match, so the assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function reads the JSON file with [{'key4': 70}, {'key4': 80}], computes mean and median using numpy (75.0 for both), writes a CSV with one row, and the test reads that CSV and compares numeric values. There are no mismatches, key names align, and single-key ordering doesn't matter. Assuming numpy is available (the test also imports numpy), the code will produce the expected results.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function reads the JSON [{'key5':90}, {'key5':100}], computes mean and median using numpy (95.0 each), writes them to temp_output.csv, and the test reads and compares the float values. There are no mismatches or edge cases for this single-key input, so the produced values match the expected results.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/915", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function computes Z-scores with scipy.stats.zscore (ddof=0). For the input [100,101,102,103,104,150], the Z-score of 150 is about 2.23 (>2), so it's correctly identified as the sole outlier. The function sets the axis title and labels as required and returns (outliers, ax). Therefore all assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function correctly identifies the outlier value (100) and computes the expected Z-score, but it does not add a 'Z_score' column to the returned outliers DataFrame. The test tries to access outliers2['Z_score'], which will raise a KeyError (or fail the assertion), so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns a DataFrame of outliers but never adds the computed Z-scores as a 'Z_score' column. While the outliers DataFrame will include the closing_price 112 (so the first assertion would pass), the test also expects a 'Z_score' column with the value 4.309576782241563. Accessing outliers3['Z_score'] will raise a KeyError, so the test will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects the returned outliers DataFrame to include a 'Z_score' column with computed z-scores. The provided task_func computes z_scores and uses them to mask outliers, but it never adds a 'Z_score' column to the outliers DataFrame. Although using z_threshold=-1 makes the mask include all rows (so the closing_price list matches), accessing outliers3['Z_score'] will raise a KeyError (or at least not match the expected values). Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function does not add a 'Z_score' column to the returned outliers DataFrame. The test accesses outliers3['Z_score'], which will raise a KeyError (or fail) for the empty DataFrame. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/1003", "raw_responses": {"test_valid_xml": "```plaintext\n[Explanation]\nThe test mocks urlopen to return XML bytes only when used as a context manager (mock_urlopen.return_value.__enter__.return_value.read.return_value). The function calls urllib.request.urlopen(url) without a with-statement, so response.read() returns a MagicMock (not the bytes). Passing that to etree.fromstring triggers an exception, causing task_func to raise ValueError instead of returning the expected DataFrame. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_xml": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but sets the mock for a context-manager return value (.__enter__().read()). The function calls urlopen(url) and then response.read() without using a with-statement, so response.read() is an unconfigured MagicMock (not the bytes set in the test). Passing that to lxml.etree.fromstring will raise an exception (XMLSyntaxError or TypeError), which the code catches and re-raises as a ValueError. The test expects a ValueError, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_different_structure_xml": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen so that only the context-manager-returned object's read() returns bytes. The function calls urlopen(url) and then response.read() without using a with-statement, so response.read() returns an unconfigured MagicMock (not bytes). etree.fromstring on that raises an exception, which is caught by the generic except and converted to a ValueError. The test expects a ValueError, so the code will raise ValueError and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise a generic Exception. task_func wraps the call in a try/except and has a broad \"except Exception\" clause that converts any exception into a ValueError. The test only asserts that a ValueError is raised, so the function's behavior satisfies the test.\n\n[Result]\nPASS\n```", "test_non_xml_data": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but configures the mock for use as a context manager (setting __enter__().read()). The function calls urlopen(...) directly (no with) and calls response.read(), so the mocked read() wasn't set and returns a Mock object. etree.fromstring(mock) raises a TypeError (not XMLSyntaxError), which is caught by the broad except Exception and converted to ValueError. The test expects a ValueError, so the function will raise ValueError and the test passes.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1124", "raw_responses": {"test_valid_url_with_title": "```plaintext\n[Explanation]\nThe input string contains \"https://www.google.com\", which the regex matches. The test patches requests.get to return a mock response that (in the test harness) provides .content with an HTML title \"Google\" and a raise_for_status method, so the function parses the HTML and returns the title \"Google\". No errors in URL extraction, request handling, or HTML parsing prevent this, so the function will produce the expected output.\n\n[Result]\nPASS\n```", "test_non_existent_website": "```plaintext\n[Explanation]\nThe regex extracts \"https://nonexistentwebsite12345.com\" from the input. The test patches requests.get to raise requests.RequestException, which the function catches and returns \"Unable to fetch the content of the URL: {url}\". The returned string matches the expected output exactly.\n\n[Result]\nPASS\n```", "test_string_without_urls": "```plaintext\n[Explanation]\nThe input string contains no \"http://\" or \"https://\" pattern, so re.findall returns an empty list and the function returns \"No valid URL found in the provided string.\" This exactly matches the expected test assertion. No network calls or other code paths are exercised.\n\n[Result]\nPASS\n```", "test_multiple_urls_in_string": "```plaintext\n[Explanation]\nThe function extracts URLs with a regex, takes the first match, fetches it via requests.get, and returns the <title> text. In the test the requests.get is patched to return a mock response whose content presumably contains the \"Google\" title; the code selects the first URL (\"https://www.google.com\") from the input string, parses the mocked response with BeautifulSoup, and returns \"Google\". There are no obvious errors (regex returns both URLs, first is chosen, response.raise_for_status() is expected to be a no-op on the mock, and .content is parsed). Therefore the function should produce the expected \"Google\".\n\n[Result]\n```plaintext\nPASS\n```", "test_url_with_no_title_tag": "```plaintext\n[Explanation]\nThe test patches requests.get to return a Mock and sets mock_get.return_value.text (but not .content). In the code, response.raise_for_status() will be a callable Mock (no exception), and BeautifulSoup is called with response.content. Since .content was not set, it's another Mock whose string representation contains no HTML <title> tag, so soup.find('title') returns None and the function returns \"No title tag found in the webpage.\" This matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_malformed_url": "```plaintext\n[Explanation]\nThe input string uses the scheme \"ht://\", not \"http://\" or \"https://\". The regex in task_func only matches http/https URLs, so no URLs are found and the function returns \"No valid URL found in the provided string.\" which exactly matches the expected result. The patched requests.get is never invoked because no URL is extracted, so the mock's behavior doesn't affect the outcome.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/101", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nThe function generates a deterministic synthetic DataFrame (np.random.seed is set) and creates a Seaborn heatmap, returning the Axes object from sns.heatmap. For the fixed seed (42) the constructed covariance matrix allows np.linalg.cholesky to succeed, so no exceptions are raised, and sns.heatmap returns a matplotlib AxesSubplot (an instance of plt.Axes). Therefore the unittest's isinstance check will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_heatmap_features": "```plaintext\n[Explanation]\nThe code builds a DataFrame with 13 synthetic features plus a 'MEDV' target, so df.corr() is a 14x14 matrix. The heatmap image array therefore contains 196 values (14*14). The test expects 169 values (13*13), so the asserted shape (169,) will not match (196,). Hence the test will fail.\n\n[Result]\nFAIL\n```", "test_heatmap_values": "```plaintext\n[Explanation]\nThe test compares the heatmap's correlation values to a fixed list of numbers (the real Boston dataset correlations). The function creates a synthetic, randomly generated dataset (even though seeded) and thus produces different correlation values, so the arrays will not match the expected list. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_plot_appearance": "```plaintext\n[Explanation]\nThe function sets a non-empty title via plt.title('Correlation Heatmap of Boston Housing Dataset'), so ax.get_title() will not be ''. get_xlabel() and get_ylabel() are empty, but the title assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/241", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test expects L2-normalized values [0.18257419, 0.36514837, 0.54772256, 0.73029674] (vector divided by its L2 norm). The code uses sklearn.preprocessing.StandardScaler which produces standardized (zero-mean, unit-variance) values instead, so norm_arr will not match the expected array. Other assertions (original array, title, labels) would pass, but the normalization check fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function correctly extracts arr = [-1, 0, 3] and sets up the plot/title/labels as expected. However, it uses sklearn.preprocessing.StandardScaler to produce normalized_array (z-score standardization), while the test expects L2 normalization manual_norm = arr / np.linalg.norm(arr). These two normalizations produce different values, so np.testing.assert_allclose(norm_arr, manual_norm) will fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function uses sklearn.preprocessing.StandardScaler to normalize the input. For a single-value input [5], StandardScaler produces 0.0 (since the feature has zero variance), but the test expects 1.0 (L2-normalized). The original array, title, and labels would match, but the normalized value assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function uses sklearn.preprocessing.StandardScaler which performs z-score scaling (subtract mean, divide by std). For the constant input [4,4,4,4] the standard deviation is zero, so StandardScaler produces NaNs (or otherwise not the L2-normalized result). The test expects L2 normalization (arr / np.linalg.norm(arr)) = [0.5, 0.5, 0.5, 0.5], so the assertion comparing norm_arr to manual_norm will fail. Other assertions (array equality, title, labels) would pass, but the normalization check fails.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function builds an empty numpy array for original; but calling preprocessing.StandardScaler().fit_transform on an array with shape (0, 1) causes sklearn to raise a ValueError (fit requires at least one sample). Therefore task_func(original=[]) will error before returning the expected empty normalized array and axes, so the test will not pass.\n\n[Result]\n```plaintext\n[FAIL]\n```"}}
{"id": "BigCodeBench/374", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function searches the given directory for .xlsx files, loads each workbook, converts any cell value containing \" to a string with quotes replaced by \\\" (backslash + quote), writes the modified value back, saves the workbook, and counts processed files. openpyxl preserves backslashes in string cell values when saving/loading, and the test's files and assertions match the transformation. No operations here should raise exceptions for the provided test data, so the function will process all 3 files and the asserted cell values will match.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe module's top-level import uses \"import regex as re\". The third-party \"regex\" package is not guaranteed to be available, causing a ModuleNotFoundError at import time and preventing the tests from running. (If the import succeeded, the test itself doesn't call task_func and would pass because the workbook values remain unchanged.)\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe module's top-level imports use \"import regex as re\" (the third-party \"regex\" package) instead of the standard library \"re\". If the execution environment does not have the \"regex\" package installed, importing the module will raise ModuleNotFoundError and the test suite cannot run. The single test itself does not call task_func and would pass logically, but the faulty import prevents the tests from executing successfully.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects task_func(directory_path='/invalid/directory/') to raise FileNotFoundError. The implementation uses glob.glob on the pattern (which returns an empty list for a non-existent directory) and then iterates over that empty list, catching exceptions per-file but never raising any error. It simply returns 0. Therefore no FileNotFoundError is raised and the test's assertRaises fails.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function globs for \"*.xlsx\" files in the given directory and counts processed files. The test creates an empty subdirectory and calls task_func on that empty directory. glob will find no files, processed_count remains 0 and is returned. No exceptions or side effects affect this case.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/583", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function generates an RSA key pair with rsa.newkeys(2048), creates a valid 16-byte AES key and 12-byte GCM nonce, encrypts the private key bytes, writes a file with a deterministic filename, and returns (pub_key, filename, password, nonce). The test only checks that pub_key is an rsa.PublicKey instance and filename is a str. All operations use correct APIs and types, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_creation": "```plaintext\n[Explanation]\nThe function generates RSA keys, encrypts the private key with AES-GCM using a 16-byte key and 12-byte nonce, writes the base64-encoded nonce, tag, and ciphertext to a file named \"private_key_<hex>.txt\", and returns the filename. The file is created with open(..., 'wb') before returning, so the test's os.path.exists(filename) check will succeed. There are no obvious runtime errors in the code logic that would prevent file creation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_content": "```plaintext\n[Explanation]\nThe function writes an AES-GCM-encrypted private key as base64-encoded bytes separated by ':' into a file opened in binary mode. The test calls task_func(), then opens that file in text mode and reads it, asserting the content is non-empty. Base64 output is ASCII, so reading as text succeeds and the content is non-empty. Key and nonce sizes are valid for AES-GCM and rsa.newkeys is used correctly. There are no other mismatches with the test, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_key_size": "```plaintext\n[Explanation]\nThe function calls rsa.newkeys(2048) so pub_key.n.bit_length() will be 2048, but the test asserts it equals 512. This mismatch causes the test to fail. (File creation/cleanup is fine and not relevant to this assertion.)\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_unique_file_per_call": "```plaintext\n[Explanation]\nThe function creates a filename using 8 cryptographically-random bytes (get_random_bytes(8)) and embeds their hex representation into the filename (\"private_key_<hex>.txt\"). Two independent calls will almost certainly produce different 8-byte values, so filename1 != filename2. Other parts of the function (AES key/nonce sizes, RSA key handling, file writing) are valid and do not force identical filenames.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_encryption_decryption": "```plaintext\n[Explanation]\nThe test reads the file and calls b64decode on the entire file content, but the code writes three base64-encoded parts separated by ':' (nonce:tag:encrypted). This is not a single base64 string and b64decode will raise an error (\"Non-base64 digit found\"). Additionally, the implementation uses AES GCM and stores the tag separately while the test uses AES EAX and a different decrypt flow, so even if decoding matched it would still be incompatible. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/765", "raw_responses": {"test_files_with_content": "```plaintext\n[Explanation]\nThe function creates the target directory, iterates over the provided full-path keys, checks content is not None and that the file exists, then copies each file into the target directory and returns the destination paths. The test writes two files in 'test_dir', passes their full paths with non-None content, and uses 'target_dir' (existing) as destination. Both files exist and will be copied, producing two file paths that exist and start with 'target_dir'. Therefore the test conditions are met.\n\n[Result]\nPASS\n```", "test_files_with_no_content": "```plaintext\n[Explanation]\nThe function creates the target directory, iterates the provided dict of full paths->content, and copies only entries whose content is not None and whose source file exists. In the test, both files are created (one empty), but only file1 has non-None content, so exactly one file is copied to the given relative target_dir. All assertions (count, existence, and start of path) will be satisfied.\n\n[Result]\nPASS\n```", "test_files_do_not_exist": "```plaintext\n[Explanation]\nThe function only copies when content is not None and the source file actually exists. The test creates the directories but not the files, so os.path.exists(...) is False for both paths and nothing is copied. The returned list is empty, matching the asserted length 0.\n\n[Result]\nPASS\n```", "test_mixed_case": "```plaintext\n[Explanation]\nThe function copies only entries whose value is not None and whose source file exists. The test creates all three files (file2.txt is created empty but has value None so is skipped), so file1 and file3 are copied into target_dir. The test's assertions (count == 2, files exist, and paths start with target_dir) will all be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_dict": "```plaintext\n[Explanation]\nCalling task_func({}, self.target_dir) passes an empty dict as kwargs, so the for-loop never runs and copied_files remains []. The function returns an empty list without errors, satisfying the test assertion that its length is 0.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/916", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns matplotlib Axes and sets the correct titles, so the title and type assertions pass. However, the test expects the histogram x-label to be 'closing_price' and its y-label to contain 'Count'. The code sets x-label to 'Closing Price' and y-label to 'Frequency', so those assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_df": "```plaintext\n[Explanation]\nThe function calls ax1.boxplot(df['closing_price']). When 'closing_price' is an empty Series, matplotlib's boxplot computation uses numpy.percentile on the empty array, which raises a ValueError (cannot compute percentiles of empty data). This causes task_func to raise an exception instead of returning the two Axes objects required by the test. Thus the test that provides an empty DataFrame will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_column": "```plaintext\n[Explanation]\nThe function accesses df['closing_price']; with the provided DataFrame that only has 'price', this lookup raises a KeyError. The test expects a KeyError using assertRaises, so the function's behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_value_df": "```plaintext\n[Explanation]\nThe function creates two matplotlib Axes via plt.subplots and returns them, so isinstance(..., plt.Axes) will be True. ax1.boxplot on a pandas Series and ax2.hist with a single value both add artists (lines/patches) to the axes, so Axes.has_data() will return True for each. No backend-dependent operations (like plt.show()) are required. Therefore the test assertions should all pass.\n\n[Result]\nPASS\n```", "test_large_values_df": "```plaintext\n[Explanation]\nThe function returns two matplotlib Axes objects from plt.subplots and draws a boxplot and histogram on them. Both ax1.boxplot(...) and ax2.hist(...) add artists/data to the axes, so ax.has_data() will be True. The returned axes are instances of matplotlib's Axes (plt.Axes), so the isinstance checks pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1129", "raw_responses": {"test_download_with_direct_key": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that only defines .content. The code calls response.raise_for_status(), which does not exist on the mock object, causing an AttributeError and the test to fail. Other parts (JSON parsing, file writing) are fine.\n\n[Result]\nFAIL\n```", "test_download_with_incorrect_key": "```plaintext\n[Explanation]\nThe function looks up data[unknown_key] immediately after parsing JSON. In the test the key 'nonexistent' is not present, so a KeyError is raised at that line before any call to requests.get or response.raise_for_status is reached. The test expects a KeyError, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_download_with_specified_directory": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that only has a content attribute. The code calls response.raise_for_status() after requests.get, but MockResponse does not implement raise_for_status, causing an AttributeError during the test. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_download_to_default_directory": "```plaintext\n[Explanation]\nThe test patches requests.get with mock_requests_get which returns a MockResponse that only has a 'content' attribute. The code calls response.raise_for_status(), but the mock object does not implement raise_for_status, causing an AttributeError. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_multiple_downloads": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that defines only .content. The code calls response.raise_for_status(), but MockResponse has no such method, causing an AttributeError. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/1004", "raw_responses": {"test_word_frequencies": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen and sets the mocked object's __enter__().read() return value, expecting urlopen to be used as a context manager. The code calls urllib.request.urlopen(url) without using \"with\", so response.read() uses mock_urlopen.return_value.read, which was not configured and thus returns a MagicMock. Calling .decode on that yields another MagicMock, and re.findall receives a MagicMock instead of a str/bytes, causing a TypeError. Therefore the function will raise an exception and the test will not pass.\n\n[Result]\nFAIL\n```", "test_empty_file": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen and sets the mocked object's __enter__().read() to return bytes (it assumes the code uses a context manager). But the function calls urllib.request.urlopen(url) without a with-statement, so response is mock_urlopen.return_value whose read() was not configured. response.read() thus returns a Mock, and .decode() yields another Mock, causing re.findall(...) to raise a TypeError (expects a string/bytes). Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_non_text_file": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise Exception. task_func calls urllib.request.urlopen directly without catching exceptions, so the exception will propagate and satisfy self.assertRaises(Exception). The test's tearDown calls plt.clf(); plt is available because the provided code imports matplotlib.pyplot as plt at top-level. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_special_characters": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen and configures the mock's context-manager return value (mock_urlopen.return_value.__enter__.return_value.read.return_value = b'1234567890'), i.e., it expects the code to use \"with urllib.request.urlopen(...) as response:\". But task_func calls urllib.request.urlopen(url) directly and then response.read().decode(...). Because the mock's non-__enter__ return_value.read was not set, response.read() returns a MagicMock, and subsequent .decode(...) yields a MagicMock. Passing that into re.findall causes a TypeError (\"expected string or bytes-like object\"). Thus the function will raise an error and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_large_input": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but configures the mock for use as a context manager (setting return_value.__enter__.return_value.read.return_value). The function calls urlopen() without using a with-statement, so response is mock_urlopen.return_value and its read.return_value was not set by the test. As a result response.read().decode(...) yields a MagicMock (not the expected bytes string), so the regex/counting will not see the 10000 \"OpenAI\" tokens and the assertion word_freq['OpenAI'] == 10000 fails (or a TypeError may occur). Therefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/108", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function's input checks pass for the test DataFrame (has required columns, numeric 'value', 'freq' is a string, valid model). seasonal_decompose is called with period=7 on 30 observations (30 > 2*7), and the data contains no zeros (safe for 'multiplicative'). The function returns the DecomposeResult produced by seasonal_decompose, so the instance check in the test will succeed. No ValueError is raised.\n\n[Result]\nPASS\n```", "test_invalid_input_data": "```plaintext\n[Explanation]\nThe function first verifies the DataFrame contains the required columns ['group','date','value'] and raises ValueError if not. The test passes a DataFrame with columns 'a' and 'b', so task_func will raise the expected ValueError and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe function immediately checks \"if not isinstance(df, pd.DataFrame): raise ValueError(...)\". Passing the string 'not a dataframe' triggers that ValueError, which matches the test's expectation.\n\n[Result]\nPASS\n```", "test_invalid_model": "```plaintext\n[Explanation]\nThe function explicitly validates decomposition_model and raises ValueError if it's not 'additive' or 'multiplicative'. The test calls task_func with 'invalid_model' and expects a ValueError, so the behavior matches the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_frequency": "```plaintext\n[Explanation]\nThe test expects a ValueError when freq='invalid_freq', but the function only checks that freq is a string (it does not validate the string's content). Therefore no ValueError is raised and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_insufficient_data": "```plaintext\n[Explanation]\nThe function performs minimal validation and then calls statsmodels.tsa.seasonal.seasonal_decompose with period=7. For the provided small_df (5 observations), seasonal_decompose raises a ValueError because the number of observations is less than 2 * period (5 < 14). That matches the test's assertRaises(ValueError).\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_components_existence": "```plaintext\n[Explanation]\nThe function accepts the provided DataFrame (has 'group','date','value'), 'value' is numeric, and defaults to multiplicative decomposition. seasonal_decompose is called with period=7; with 30 daily observations (>= 2*7) and positive values (1..30) multiplicative decomposition is valid. The function returns a DecomposeResult which includes trend, seasonal, and resid attributes, so the test's assertions will pass.\n\n[Result]\nPASS\n```", "test_component_shapes": "```plaintext\n[Explanation]\nThe function's input validations pass for the provided DataFrame. seasonal_decompose is called with period=7 on a 30-day daily series of positive integers (satisfying requirements for multiplicative decomposition and 2*period <= nobs). The decomposition result's trend, seasonal, and resid arrays/series have the same length as the input series (30), so the test's shape assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_additive_model": "```plaintext\n[Explanation]\nThe function validates inputs, sets the date index, and calls statsmodels.tsa.seasonal.seasonal_decompose with model='additive' and period=7 on the provided numeric Series. It returns the DecomposeResult and axes; the test checks the DecomposeResult.seasonal string against the expected small periodic floating-point values. Given the deterministic series (1..30), period=7, and standard statsmodels behavior, the seasonal component will match the expected repeating tiny values and the return type matches DecomposeResult. No validation or index issues prevent the call, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_numeric_values": "```plaintext\n[Explanation]\nThe test inserts a string into the 'value' column, causing the column dtype to become object. The function checks pd.api.types.is_numeric_dtype(df['value']) and will raise ValueError(\"Column 'value' must contain numeric data\") in that case. That matches the test's assertRaises expectation.\n\n[Result]\n```plaintext\nPASS\n```", "test_missing_values": "```plaintext\n[Explanation]\nThe test injects a None into the numeric 'value' column, which pandas upcasts to float (so the is_numeric_dtype check passes). The code then calls statsmodels.tsa.seasonal.seasonal_decompose on the series containing NaN. seasonal_decompose raises a ValueError when the input contains missing values, matching the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/15", "raw_responses": {"test_successful_command_execution": "```plaintext\n[Explanation]\nThe CSV contains one command \"echo Hello\". task_func runs the command, writes stdout (\"Hello\\n\") to an output file in the given output directory, and returns the output file's path. The test's os.path.join(self.output_dir_path, result[0]) yields result[0] unchanged because it's an absolute path, so the file is opened correctly and contains \"Hello\". All assertions should pass.\n\n[Result]\nPASS\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe function immediately checks os.path.exists(commands_file_path) and raises FileNotFoundError if the file is missing. The test passes a nonexistent CSV path, so the expected FileNotFoundError will be raised. No other code runs before this check to interfere.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_command": "```plaintext\n[Explanation]\nThe function reads the CSV, runs the invalid command via subprocess with shell=True, and writes the command and the shell error output (stderr) into the output file when returncode != 0. On a typical Unix environment the shell stderr includes \"not found\", and the file also contains the \"Command: invalid_command_xyz\" line, so both required substrings will be present. The returned path is absolute, and os.path.join in the test still resolves to that path, so the file is read correctly. Therefore the test should pass on the expected Unix-like runner.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_csv_file": "```plaintext\n[Explanation]\nThe function returns an empty list when the CSV has no rows because csv.reader yields no rows and output_files remains empty. The test creates an existing empty file and expects length 0, so the code will return [] and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_mixed_commands": "```plaintext\n[Explanation]\nThe function reads the two commands, runs them with shell=True and writes per-command output files. For the invalid command the code writes the command string and any stderr into the file. On a typical Unix shell the stderr contains \"not found\", so the file will include both 'invalid_command_abc' and 'not found'. The returned paths are absolute, so opening the second result works as used in the test. Therefore the test assertions should be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_command_failure_with_specific_exit_code": "```plaintext\n[Explanation]\nThe test writes a CSV with a failing command (\"exit 1\") and then expects the output file to contain the string \"Error executing command\". The task_func on failure writes \"Command failed with exit code X\", \"Command: ...\", and optionally \"Error output:\" / \"Standard output:\". It never writes the specific phrase \"Error executing command\", so the assertIn will fail. (The returned path handling does not prevent this failure.)\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/267", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function does add data[\"a\"]=1 and returns a numpy ndarray for the FFT, so those assertions would pass. However the plot text does not match the test exactly: the code sets title 'FFT of the signal' (lowercase 's'), xlabel 'Frequency (Hz)', and ylabel 'Magnitude', while the test expects 'FFT of the Signal', 'Frequency [Hz]', and 'Frequency Spectrum Magnitude'. These string mismatches cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function will pass the checks that 'a' is in data (it sets data[\"a\"]=1) and that fft is a numpy.ndarray. However the matplotlib Axes title and labels do not match the test's expected strings: the code sets title \"FFT of the signal\" (lowercase 's'), xlabel \"Frequency (Hz)\", and ylabel \"Magnitude\", while the test expects \"FFT of the Signal\", \"Frequency [Hz]\", and \"Frequency Spectrum Magnitude\". These mismatches cause the test assertions to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function does add key 'a' and returns an ndarray FFT, so those assertions would pass. However the test expects specific plot labels:\n- Expected title 'FFT of the Signal' but code sets 'FFT of the signal' (case mismatch).\n- Expected xlabel 'Frequency [Hz]' but code sets 'Frequency (Hz)'.\n- Expected ylabel 'Frequency Spectrum Magnitude' but code sets 'Magnitude'.\nBecause these label strings do not match the expected values, the test will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe code adds key \"a\" and returns a numpy fft array and an Axes object, so the first two assertions pass. However the test expects specific plot text that does not match the code:\n- Expected title \"FFT of the Signal\" vs code sets \"FFT of the signal\" (case mismatch).\n- Expected xlabel \"Frequency [Hz]\" vs code sets \"Frequency (Hz)\".\n- Expected ylabel \"Frequency Spectrum Magnitude\" vs code sets \"Magnitude\".\nThese mismatches cause the test assertions to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function does add key 'a' and returns a numpy ndarray for the FFT, so those assertions would pass. However the test expects specific plot labels that do not match the code:\n- Expected title 'FFT of the Signal' vs actual 'FFT of the signal' (case/text mismatch).\n- Expected xlabel 'Frequency [Hz]' vs actual 'Frequency (Hz)'.\n- Expected ylabel 'Frequency Spectrum Magnitude' vs actual 'Magnitude'.\n\nBecause these label strings differ, the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/503", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function builds the index as formatted date strings (via strftime), so df.index entries are strings. The test calls df.index[-1].date(), which will raise AttributeError because a string has no .date() method. Therefore the test will not pass. (Additionally, values are numpy.float64, which wouldn't satisfy isinstance(..., (int, float)), but the index issue already causes failure.)\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function builds the DataFrame index as formatted strings (strftime('%Y-%m-%d')), so df.index elements are Python strings, not datetime or pandas Timestamps. The test calls df.index[-1].date(), which will raise an AttributeError because str has no .date() method. Other assertions (row count, columns, numeric values) would pass, but the .date() check fails, causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe test will fail. The function builds the index as strings of dates in descending order (today first), so df.index[-1] is an older date string (no .date() method) rather than today's datetime \u2014 this causes an AttributeError or wrong comparison. Additionally, DataFrame entries are numpy.float64, so isinstance(x, (int, float)) will be False for those values. \n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects ValueError for days_in_past = -1, 0, and 2.5.\n\n- days_in_past = -1: np.random.random(-1) raises a ValueError (negative dimensions), so this case passes.\n- days_in_past = 0: the function runs without error (creates an empty DataFrame); no ValueError is raised, so this case fails.\n- days_in_past = 2.5: range(2.5) raises a TypeError, not a ValueError, so it does not satisfy the expected ValueError.\n\nBecause two of the three checks do not raise ValueError as expected, the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test expects task_func to raise ValueError when stock_names is empty or contains non-string items. The provided function does no validation: it accepts an empty list (creating a DataFrame with no columns) and accepts non-string items as column keys (ints/None are allowed), so no ValueError is raised. Therefore the test assertions will fail.\n\n[Result]\nFAIL\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function sets numpy's random seed at the start, so two calls with the same random_seed produce identical random arrays and thus identical DataFrames. The index is based on datetime.now() formatted to '%Y-%m-%d' (date-only), so two calls in the same test run will produce the same index values. A different random_seed yields different numeric values, so equality checks against df2 will be False. Therefore the test's assertions should all hold.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function builds 'dates' with a list comprehension over range(days_in_past), which yields 366 date strings when days_in_past=366. Each stock's prices are generated with np.random.random(days_in_past), producing arrays of length 366. These are assembled into a DataFrame with an index of length 366, so df.shape[0] will be 366.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function builds a DataFrame using the provided stock_names as keys for the data dict. Calling task_func(stock_names=['ABC']) will create a column 'ABC' (with 7 random rows by default). No errors are raised, so the assertion that 'ABC' is in df.columns will be true.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/587", "raw_responses": {"test_file_encryption": "```plaintext\n[Explanation]\nThe module requires third-party packages (rsa and cryptography). In a typical isolated test environment these are not available, causing an ImportError at import time so the test cannot run. The minor filename mismatch for the saved AES key does not affect the single assertion (existence of the encrypted file).\n\n[Result]\nFAIL\n```", "test_encrypted_key_file_creation": "```plaintext\n[Explanation]\nThe code correctly creates the encrypted AES key file (returned path is test_file.txt.aes_key.encrypted). However, the provided unittest's tearDown references the name os without importing os in the test module. That will raise a NameError during tearDown, causing the test run to report an error (not a pass). Therefore the single test case will not pass.\n\n[Result]\nFAIL\n```", "test_public_key_type": "```plaintext\n[Explanation]\nThe function calls rsa.newkeys and returns the generated public key as pub_key. In the python-rsa library the public key object is an instance of rsa.PublicKey, so the test's isinstance check will succeed. The rest of the function (file I/O, AES encryption, RSA encryption of the AES key) does not affect the type check and contains no errors that would raise during the test. The tearDown looks for a differently named AES-key file but that only affects cleanup and not the asserted outcome.\n\n[Result]\nPASS\n```", "test_encrypted_file_content": "", "test_aes_key_file_content": "", "test_non_existent_file": "```plaintext\n[Explanation]\ntask_func immediately tries to open the given path with open(..., 'rb') and does not catch exceptions, so calling task_func('non_existent_file.txt') will raise FileNotFoundError as the test expects. The test's tearDown refers to os, which is imported by the provided code, so no NameError occurs. Therefore the test assertion is satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/771", "raw_responses": {"test_file_creation_and_content": "```plaintext\n[Explanation]\nThe test patches os.listdir but the function uses Path.glob to find files, so the mocked list is never used and no files will be processed (glob will likely yield nothing for the test directory). Even if files were found, the test expects open to be called as mock_file.assert_any_call(expected_path, 'w') but the code calls open with additional keyword arguments (newline and encoding), so the call signatures would not match. Also the function writes the entire file content in a single write() call, while the test asserts individual row writes. Therefore the code will not satisfy the test expectations.\n\n[Result]\nFAIL\n```", "test_empty_directory": "```plaintext\n[Explanation]\nThe test patches os.listdir to return an empty list. pathlib.Path.glob internally uses os.listdir to enumerate directory entries, so glob(\"*.csv\") will yield no files. The function will not enter the loop and returns an empty list, matching the expected result.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_matching_files": "```plaintext\n[Explanation]\nThe test patches os.listdir, but task_func uses pathlib.Path(directory).glob(\"*.csv\") to find files, so the patch has no effect. If the directory has no CSV files (typical in the test environment), glob yields nothing, new_files stays empty, and the function returns [], matching the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_mixed_file_types": "```plaintext\n[Explanation]\nThe test patches os.listdir to simulate files, but the function uses Path(directory).glob(\"*.csv\") instead of os.listdir; therefore no files will be found and the function returns an empty list instead of ['Shan.csv']. Additionally, even if files were found, the test's mock_file.assert_any_call checks for a call with only (Path(...), 'r') while the function calls open with additional keyword args (newline and encoding), which would also cause the assertion to fail. Thus the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches os.listdir to return ['Shan-1.csv'] and patches builtins.open to raise IOError. pathlib.Path.glob internally uses os.listdir, so the function will see the Shan-1.csv entry and then attempt to open it. Because open is patched to raise IOError and the function doesn't catch it, task_func will propagate the IOError, matching the test's assertRaises expectation.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/917", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function fits an ARIMA(1,1,1) to the 7-point series, forecasts 7 steps, and returns a list and an Axes object. The forecast values match the expected numbers within 2 decimal places for this deterministic model. The returned ax is an Axes instance. The test's final check uses assertTrue with the plotted y-data (a non-empty list) which is truthy, so it passes. No type or indexing errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function fits an ARIMA(1,1,1) on the provided 7-day increasing series, forecasts 7 steps, and returns the forecast list and a matplotlib Axes. The forecast values from statsmodels' ARIMA on this deterministic input match the expected numbers within two decimal places. The returned Axes is a matplotlib Axes instance, and the plotted historical y-data equals [200, 201, 202, 203, 204, 205, 206] (float equality matches ints). Therefore the test assertions should pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns a list of forecasts and a matplotlib Axes as required, and the ARIMA forecasts are likely to match the expected values within 2 decimal places. However, the test's final assertion calls unittest.TestCase.assertAlmostEqual on two lists (the plotted y-data list vs. the expected list). assertAlmostEqual expects numeric scalars and will attempt subtraction; passing lists causes a TypeError (unsupported operand types for -) and the test will fail. Therefore the code will not pass the provided test case.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function sets the date index, fits ARIMA(1,1,1) and returns the 7-step forecast as a list and the matplotlib Axes. For the provided input (closing prices 400..406) the ARIMA forecast values match the expected numbers within the 2-decimal tolerance. The returned ax is an Axes instance, and the plotted historical line's y-data equals [400,401,...,406] (element-wise equality holds between floats and ints), so the final list and plot checks pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function sets the 'date' index, fits an ARIMA(1,1,1) on the provided increasing series, forecasts 7 steps, returns a list of floats and the Axes object from plt.subplots(). For the provided linear input (500..506) the ARIMA forecast is deterministic and matches the expected values within 2 decimal places. The returned types match the test assertions (list and Axes). The final plot-related assertion uses assertTrue with the y-data list as the first argument, which will pass because it's a non-empty list. Therefore the code will pass the test.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1006", "raw_responses": {"test_non_zip_content": "```plaintext\n[Explanation]\nThe function checks response.status_code == 200 and then verifies the content type using response.headers.get('content-type', '') and the URL suffix. The test's mocked headers use 'Content-Type': 'text/plain' (capitalized), so get('content-type') returns the default '' and the code correctly treats it as not a ZIP. The URL ends with .txt, so the function returns \"Error: The URL does not point to a ZIP file.\" which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.RequestException. task_func calls requests.get inside a try block and has an except requests.exceptions.RequestException that returns the exact message \"Error: Unable to download the file from the provided URL.\" Thus the raised exception is caught and the returned string matches the test assertion. The test's use of os is satisfied because the module already imported os.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_download_invald_zip_file": "```plaintext\n[Explanation]\nThe mocked response has status_code 200 and content for an invalid zip. The code checks content-type using a lowercase key (the mock uses 'Content-Type') but still allows the URL check because the URL ends with '.zip', so it proceeds to write the file. Opening the written file with ZipFile raises BadZipFile, which the code catches and returns \"Error: The downloaded file is not a valid ZIP file.\" exactly as the test expects. The test's teardown uses os, which is available because the submitted code imports os. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_general_error": "```plaintext\n[Explanation]\nrequests.get is patched to raise RuntimeError('Unexpected error'). task_func wraps the call in a try/except and has a general \"except Exception as e\" that returns \"Error: {str(e)}\". Thus the function will return \"Error: Unexpected error\", satisfying the test's startswith assertion.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1137", "raw_responses": {"test_phone_numbers_page_1": "```plaintext\n[Explanation]\nThe input file contains \"Phone: +1234567890\". The regex and group-joining logic strip the leading '+' because the capturing groups only include digits (the optional '+' is outside the digit groups). re.findall will return groups that concatenate to \"1234567890\", so the function returns and saves '1234567890' while the test expects '+1234567890'. Therefore the output will not match the expected value.\n\n[Result]\nFAIL\n```", "test_phone_numbers_page_2": "", "test_phone_numbers_page_3": "```plaintext\n[Explanation]\nThe test file contains the text \"No phone number here!\" with no digits, so the regex finds no matches. The function will read the local file (url starts with 'file://'), parse text with BeautifulSoup (no effect on content), produce an empty list, write [] to the output JSON, and return []. This matches the expected empty list, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_phone_numbers_page_4": "```plaintext\n[Explanation]\nThe regex captures only digit groups and does not include the leading '+' in any capture group. For \"+919876543210\" the function will return \"919876543210\" (and save that to JSON), but the test expects \"+919876543210\". Therefore the result will not match the expected output.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_phone_numbers_page_5": "```plaintext\n[Explanation]\nThe regex and group handling drop the leading '+' and (in the first alternative) the leading country '1' because the country code is in a non-capturing group and only the area/other groups are captured. For input \"+14151234567, +13171234567\" the function will return ['4151234567', '3171234567'] (or similar) rather than the expected ['+14151234567', '+13171234567'], so the test assertion will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/120", "raw_responses": {"test_reproducibility_with_seed": "", "test_series_length": "```plaintext\n[Explanation]\nThe function builds a Series with total_days = (end_date - start_date).days + 1 and appends that many dates. For start_date=2020-01-01 and end_date=2020-01-10, total_days = 10, so len(dates) == 10. The test expects len(dates) == (end_date - start_date).days which is 9. The lengths mismatch, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_date_types": "```plaintext\n[Explanation]\nThe function checks types with isinstance(..., datetime) for both start_date and end_date and raises ValueError when they are not datetime instances. Both test calls pass strings for one parameter each, so both raise ValueError as expected.\n\n[Result]\nPASS\n```", "test_start_date_after_end_date": "```plaintext\n[Explanation]\nThe function explicitly checks \"if start_date > end_date: raise ValueError(...)\". The test calls task_func(datetime(2020, 12, 31), datetime(2020, 1, 1)) so that condition is true and a ValueError is raised, satisfying the test's assertRaises.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_return_type": "```plaintext\n[Explanation]\nThe function returns pd.Series(random_dates) and pandas is imported as pd. With default datetime arguments the input validation passes and no exception is raised, so the returned object is a pandas.Series. Thus the test asserting isinstance(dates, pd.Series) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_date_within_range": "```plaintext\n[Explanation]\nFor the given start_date (2020-01-01) and end_date (2020-01-05) the function computes total_days = 5 and generates 5 random offsets in the range 0..4. Each produced date is start_date + offset days, so every element is guaranteed to be between 2020-01-01 and 2020-01-05 inclusive. The returned pandas Series yields datetime-like objects (pd.Timestamp) that compare correctly with datetime.datetime, so all assertions in the test will succeed.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/273", "raw_responses": {"test_invalid_content_type": "```plaintext\n[Explanation]\nThe test expects send_error(400, 'Content-Type header is not application/json') to be called. The handler checks the header using the exact key 'Content-Type' (case-sensitive) while the test provides 'content-type' \u2014 this still leads to the invalid content-type branch, but the code calls send_response/send_header/end_headers and writes to wfile instead of calling send_error. Therefore send_error is never invoked and the assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_class_properties": "```plaintext\n[Explanation]\ntask_func defines and returns PostHandler as \"class PostHandler(http.server.BaseHTTPRequestHandler):\", so handler_class is a type and is a subclass of http.server.BaseHTTPRequestHandler. There are no runtime errors in defining the class, and the test only checks these properties.\n\n[Result]\nPASS\n```", "test_valid_json_data": "```plaintext\n[Explanation]\nThe handler checks headers using capitalized keys ('Content-Type', 'Content-Length'), but the test sets headers as lowercase ('content-type', 'content-length') on a plain dict. Because dict lookups are case-sensitive, content_type becomes None and the code takes the 400 error branch (Content-Type header is not application/json). The test expects a 200 response for valid JSON, so the assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_invalid_json": "```plaintext\n[Explanation]\nThe test expects the handler to call send_error(400, 'Invalid JSON') when given invalid JSON. The handler code never calls send_error; it uses send_response/send_header/end_headers and writes responses directly. Additionally, the test supplies headers with lowercase keys ('content-type', 'content-length') while the handler looks up 'Content-Type' and 'Content-Length', causing the content-type check to fail early and return a different 400 message. Therefore the mocked send_error will not be called as asserted.\n\n[Result]\n```plaintext\nFAIL\n```", "test_missing_data_key": "```plaintext\n[Explanation]\nThe test sets headers as a plain dict with lowercase keys ('content-type', 'content-length'), but the handler code looks up 'Content-Type' and 'Content-Length' (case-sensitive), so content_type becomes None and the handler immediately sends a 400 response with message \"Content-Type header is not application/json\" using send_response/send_header, not send_error. The test expects send_error to be called with 400 and \"No data key in request\" (and the JSON body lacks 'data'), which does not occur. Therefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/399", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe first assertion (len(ax.lines) == 2) passes because two lines are plotted. The second assertion expects labels exactly 'sin' and 'cos', but the code sets labels to 'sin(1\u03c0t)' and 'cos(1\u03c0t)'. The membership check compares full labels for equality, so 'sin' and 'cos' are not found and the assertion fails.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nFor frequency=0 the code generates t = linspace(...), sine_wave = sin(0) -> all zeros, cosine_wave = cos(0) -> all ones. The function plots sine first then cosine, so ax.lines[0] is the sine (all 0s) and ax.lines[1] is the cosine (all 1s). Input validation and defaults are fine, and no operations cause errors for this test. Therefore the assertions in the test will be True.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function raises ValueError when frequency < 0 and when sample_size <= 0. The test calls task_func(-1) (frequency negative) and task_func(5, -1) (sample_size negative), so both calls will raise ValueError as expected. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function generates sine and cosine arrays over t = linspace(0,2,5000) with frequency 10 and plots sine first then cosine. For t=0 the sine value is 0 and cosine is 1, and the waves cross positive and negative values within the interval, so np.any(... >= 0) and np.any(... <= 0) are both true for each line. The plotting order matches the test's indexing (sine at lines[0], cos at lines[1]). Therefore the test assertions will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function validates inputs, creates sine and cosine plots with labels, calls ax.legend(), and returns fig and ax. For task_func(1) the legend is created, so ax.get_legend() will not be None. No errors are triggered by the provided inputs.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/509", "raw_responses": {"test_difference_report": "```plaintext\n[Explanation]\nThe function compares raw file lines using difflib.ndiff and returns the line content as plain strings like \"name,age\". The test expects the Content column to contain Python tuple-like string representations \"('name', 'age')\" (including parentheses and quotes). Because the produced Content values (\"name,age\", \"Alice,30\", etc.) do not match the expected \"('name', 'age')\" formatting, the produced df_list will differ from expect. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func with two nonexistent filenames and uses assertRaises(FileNotFoundError). At the start of task_func it checks os.path.exists for each file and raises FileNotFoundError if missing. This matches the test expectation, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_custom_delimiter": "```plaintext\n[Explanation]\nThe test creates two non-empty files (test3.csv, test4.csv) with semicolon-separated content and calls task_func(..., delimiter=';'). task_func reads the files as text, they exist and are non-empty, ndiff produces line comparisons, and the function returns a pandas DataFrame. The unused delimiter parameter does not affect this test, which only asserts the return type. No exceptions are raised.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_file_path": "```plaintext\n[Explanation]\ntask_func is called with integers 123 and 456. The function calls os.path.exists on these arguments, which raises a TypeError (\"expected str, bytes or os.PathLike object, not int\"). The unit test uses with self.assertRaises(Exception):, which accepts any Exception subclass (including TypeError). Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_empty_files": "```plaintext\n[Explanation]\nBoth empty files are created by the test. task_func reads the files, detects both have zero lines and raises ValueError(\"Both files are empty\"). The test expects a ValueError, so the function behavior matches the test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/590", "raw_responses": {"test_valid_url": "```plaintext\n[Explanation]\nThe test calls task_func with a real external URL and expects a DataFrame. task_func requires network access (urllib.request.urlopen) and external packages (pyquery, pandas). In typical CI/sandbox environments network access is blocked (or those packages may be unavailable), causing urlopen to raise URLError and the test to fail. Therefore the code will not pass this test case.\n\n[Result]\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test calls task_func with an invalid hostname. urllib.request.urlopen on such a URL will raise urllib.error.URLError. The function catches urllib.error.URLError and re-raises an urllib.error.URLError, so the test's with self.assertRaises(urllib.error.URLError) will be satisfied. No earlier checks or other exceptions interfere.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_network_error": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise urllib.error.URLError. task_func calls urllib.request.urlopen inside a try block, catches urllib.error.URLError and re-raises urllib.error.URLError, so the test's with self.assertRaises(urllib.error.URLError) will succeed. The URL validity check does not trigger for the provided non-empty string.\n\n[Result]\nPASS\n```", "test_empty_url": "```plaintext\n[Explanation]\nThe test calls task_func('') and expects a ValueError. The function begins with \"if not url or not isinstance(url, str): raise ValueError(...)\", so passing an empty string triggers that check and raises ValueError. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_dynamic_comparison": "```plaintext\n[Explanation]\nThe function replaces missing href attributes with an empty string and also filters out anchors where both text and href are empty. The test's helper keeps all anchors and leaves missing hrefs as None. This leads to mismatches ('' vs None) and potential row-count differences, so the DataFrame comparison will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_fetch_time_format": "```plaintext\n[Explanation]\nThe test calls task_func with a real URL and does not mock network access. The function performs urllib.request.urlopen and will raise urllib.error.URLError if the execution environment blocks outbound HTTP(s) calls. In typical isolated test environments there is no internet, so task_func will raise URLError and the test will error/fail. Aside from networking, the function sets fetch_time using datetime.now().strftime('%Y-%m-%d %H:%M:%S'), which matches the tested format, so the only likely failure is lack of network access.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/928", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates two-letter combinations using itertools.permutations(letters, 2), which excludes repeated-letter pairs like 'zz'. The result dictionary therefore has no 'zz' key, so result['zz'] in the test raises a KeyError (the test expects value 0). Other asserted keys exist, but this missing key causes the test to fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe code builds two-letter combinations using itertools.permutations(..., 2), which only produces pairs of distinct letters (no 'aa', 'bb', 'cc', ...). As a result keys like 'aa' are not present in the returned dict, but the test accesses result['aa'] expecting 1. This will raise a KeyError / fail the assertion.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe code generates two-letter keys using itertools.permutations(letters, 2), which excludes repeated-letter pairs like \"zz\". The test accesses result['zz'] expecting 0, but 'zz' is not in the result dict, causing a KeyError (test failure). Other checked pairs (fe, ef, dc, ba) would be present/counted correctly, but the missing 'zz' key makes the test fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function builds keys from itertools.permutations(letters, 2), which excludes repeated-letter pairs like 'zz'. The test accesses result['zz'], but that key is never created, so the test will raise a KeyError (and thus fail) even though the other pair counts would be correct.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function generates two-letter keys using itertools.permutations(letters, 2), which excludes repeated-letter pairs like 'zz'. For an empty input it returns counts for only those permutation keys. The test accesses result['zz'], which is not present in the dictionary, causing a KeyError and the test to fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/777", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists the three mocked zip filenames, the regex matches each name and extracts the correct prefixes, creates the expected extraction directories, opens each zip via zipfile.ZipFile with the correct path and mode, and returns the directories in the same order. The test's mocked calls align with these operations, so all assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function iterates os.listdir, filters '.zip', matches the provided regex which correctly captures 'test_data' from 'test_data-123.zip', creates the expected extraction directory, and calls zipfile.ZipFile with the correct path and mode. The test patches os.listdir, os.makedirs, and zipfile.ZipFile and asserts those calls; the code's behavior matches the assertions exactly. \n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe code iterates os.listdir, checks .zip suffix, matches the regex r'^(.*?)-\\d+\\.zip$' which captures \"data_test\" from \"data_test-321.zip\", creates the extraction dir with os.makedirs(extraction_dir, exist_ok=True) and calls zipfile.ZipFile(os.path.join(test_dir, 'data_test-321.zip'), 'r') then extractall. The mocks in the test correspond to these calls, so both asserted calls will occur.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nos.listdir is patched to return an empty list, so the for-loop in task_func never runs. Therefore os.makedirs and zipfile.ZipFile are never invoked. The test's assert_not_called() expectations are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function lists the directory, matches \"test-456.zip\" with the regex (prefix \"test\"), creates the expected extraction directory, calls zipfile.ZipFile(expected_zip_path, 'r') and calls extractall once. The test patches os.listdir, zipfile.ZipFile, and os.makedirs and asserts those calls; the code performs them exactly as asserted, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/123", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function appends 12 to my_list so sum_of_list becomes 18 for input [1,2,3]. The test setUp creates only 3 CSV files. After selecting files, the code checks if len(files_to_use) < sum_of_list (3 < 18) and raises FileNotFoundError. The test expects a concatenated DataFrame instead, so the test will fail.\n\n[Result]\nFAIL\n```", "test_list_modification": "```plaintext\n[Explanation]\nThe function appends 12 to my_list (making sum 18) then requires that many CSV files. The test setup only creates 3 files in the provided directory, so after selecting files the function detects len(files_to_use) < sum_of_list and raises FileNotFoundError. Because task_func raises before returning, the unittest will error/fail rather than reach the assertIn. Therefore the test does not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_input": "```plaintext\n[Explanation]\nThe test calls task_func('not a list', file_dir=self.test_dir). The function immediately checks isinstance(my_list, list) and raises TypeError(\"'my_list' must be a list\") when the argument is not a list. This matches the test's assertRaises(TypeError), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func([1,2,3], file_dir='./non_existent_dir/'). The function appends 12 and then uses glob to find CSV files in that directory. Since the directory doesn't exist, glob.glob returns an empty list and the function raises FileNotFoundError as soon as it detects no files. The test expects a FileNotFoundError, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_correct_file_count": "```plaintext\n[Explanation]\nThe function appends 12 to the input list, so my_list [1] becomes [1, 12] and sum_of_list = 13. The test directory contains only 3 CSV files, so after globbing files_to_use has length 3 which is less than 13. The function then raises FileNotFoundError (\"Not enough files found...\") instead of returning a DataFrame of 9 rows. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/1008", "raw_responses": {"test_successful_scrape": "```plaintext\n[Explanation]\nThe test's mock response sets mock_response.text to the HTML but does not set mock_response.content. The function uses response.content when creating the BeautifulSoup object; since content is a MagicMock (not the HTML string/bytes), BeautifulSoup won't find the table, so table is None and the function raises ValueError. Thus the test, which expects a DataFrame with columns Name and Age, will fail.\n\n[Result]\nFAIL\n```", "test_table_not_found": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock response that only sets .text. The code calls response.raise_for_status() (present on MagicMock, does nothing) and uses response.content when building the BeautifulSoup object. response.content will be a MagicMock (not containing any <table>), so soup.find(...) returns None and the function raises the expected ValueError. Therefore the test's with self.assertRaises(ValueError) condition is satisfied.\n\n[Result]\nPASS\n```", "test_network_error": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.exceptions.ConnectionError. task_func calls requests.get(url) and does not catch this exception, so the ConnectionError will propagate. The test expects that exception, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_http_error": "```plaintext\n[Explanation]\nThe test patches requests.get so that the returned object's raise_for_status raises requests.exceptions.HTTPError. In task_func, requests.get(url) is called and then response.raise_for_status() is invoked; that exception will propagate (the function does not catch it). The test expects this HTTPError to be raised. Since the module-level import requests exists in the provided code, the test's reference to requests.exceptions.HTTPError resolves correctly. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_table": "```plaintext\n[Explanation]\nThe test's mock sets mock_response.text but not mock_response.content. The function uses response.content with BeautifulSoup; since mock_response.content is a MagicMock (not the provided HTML string), BeautifulSoup will not find the <table id=\"table0\"> and table will be None, causing the function to raise ValueError. The test expects an empty DataFrame, so the behavior does not match.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/274", "raw_responses": {"test_invalid_content_type": "```plaintext\n[Explanation]\nThe handler code accesses self.headers['Content-Length'] (capitalized header name), but the test sets headers as a plain dict with lowercase keys ('content-length'). This causes a KeyError before any response logic runs. The code does not catch KeyError, so it will raise an exception instead of sending a 400 response; therefore the test's assertions (send_response called with 400 and end_headers called once) will not be met.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_missing_key_in_json_data": "```plaintext\n[Explanation]\nThe handler code reads content_length using self.headers['Content-Length'] (capitalized). The test sets self.handler.headers to a plain dict with lowercase keys ('content-length'), so accessing 'Content-Length' raises a KeyError before the JSON validation branch runs. That exception is unhandled, so send_response(400) is never called and the test will error rather than pass.\n\n[Result]\nFAIL\n```", "test_valid_json_request": "```plaintext\n[Explanation]\nThe handler code reads self.headers['Content-Length'] (capitalized) but the test provides headers with 'content-length' (lowercase) in a plain dict, causing a KeyError and the request handling to crash. Additionally, even if header lookup matched, the code calls server.send_message(msg) while the test asserts instance.sendmail(...) \u2014 a mismatch that would also cause the test to fail.\n\n[Result]\nFAIL\n```", "test_invalid_json_format": "```plaintext\n[Explanation]\nThe test assigns handler.headers as a plain dict with lowercase keys ('content-length'), but the code accesses self.headers['Content-Length'] (capitalized). That causes a KeyError before the JSON parsing logic runs, so the handler never calls send_response(400) as the test expects.\n\n[Result]\nFAIL\n```", "test_empty_json_data": "```plaintext\n[Explanation]\nThe handler code does int(self.headers['Content-Length']) using a capitalized header name. The test supplies a plain dict with lowercase 'content-length' and 'content-type' keys, so self.headers['Content-Length'] raises KeyError before any response is sent. That exception causes the test to error instead of asserting send_response(400), so the test will fail.\n\n[Result]\nFAIL\n```", "test_email_sending_exception": "```plaintext\n[Explanation]\nThe handler accesses self.headers['Content-Length'] (capitalized) but the test sets headers with lower-case keys ('content-length'), so a KeyError occurs before the SMTP mocking is reached. Therefore the test will not observe the intended 535 response. The code would fail the test.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/401", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function sets MAIL_SERVER to 'localhost', MAIL_PORT to int(25), MAIL_USE_TLS to False (since default 'False'.lower() == 'true' is False), and MAIL_USERNAME and MAIL_PASSWORD to None when no environment variables are present. Instantiating Mail(app) is valid. Therefore the assertions in the test match the produced config values and the test should pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function reads environment variables (patched by the test) and correctly sets:\n- MAIL_SERVER to 'test_server'\n- MAIL_PORT to int('2525') \u2192 2525\n- MAIL_USE_TLS to 'True'.lower() == 'true' \u2192 True\n- MAIL_USERNAME and MAIL_PASSWORD to the provided strings\nIt creates a Flask app named 'test_case_2' and initializes Flask-Mail with that app, so mail_instance.app.name matches. Therefore all assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function reads MAIL_SERVER from the patched environment and sets other defaults. MAIL_SERVER becomes 'another_server'. MAIL_PORT uses the default integer 25 and is converted to int -> 25. MAIL_USE_TLS uses default 'False' -> boolean False. MAIL_USERNAME and MAIL_PASSWORD remain None. Initializing Flask and Flask-Mail with these configs will succeed. The test assertions match these values.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function reads MAIL_PORT and MAIL_USE_TLS from the environment (with defaults), converts MAIL_PORT to int and MAIL_USE_TLS to a boolean, leaves MAIL_USERNAME and MAIL_PASSWORD as None, and initializes Flask-Mail after setting app.config. With the patched environment values ('3030' and 'False') the resulting configs are MAIL_SERVER='localhost', MAIL_PORT=3030, MAIL_USE_TLS=False, MAIL_USERNAME=None, MAIL_PASSWORD=None \u2014 matching the test assertions. Initialization of Mail(app) does not interfere with these config values.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function reads environment variables with correct defaults and types. With MAIL_USERNAME patched to 'username' and others absent, MAIL_SERVER becomes 'localhost', MAIL_PORT int 25, MAIL_USE_TLS False, MAIL_USERNAME 'username', and MAIL_PASSWORD None. Initializing Mail(app) does not affect these config values. Therefore the assertions in the test match the function's behavior.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/511", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function computes statistics using pandas on the 'Salary' column of the provided data and returns a dict with sum, mean, min, and max. For the given input the calculated values are 500000, 100000.0, 50000, and 150000 which match the expected dict. The returned numeric types (numpy scalars) compare equal to the Python literals used in the test, and plotting does not affect the assertion. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function builds a DataFrame correctly, extracts the 'Experience' column [3,6,8,11,13], and computes sum 41, mean 8.2, min 3, max 13. The returned dict matches the expected values (numpy numeric types compare equal to Python ints/floats). Creating the pie chart does not raise an error in typical test environments. Therefore the test assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function computes statistics from the 'Age' column correctly: it builds a DataFrame from the provided data and uses sum(), mean(), min(), and max(), producing the values 185, 37.0, 27, and 47 respectively. The numpy/pandas scalar types compare equal to Python numeric literals used in the test, and creating the pie chart does not affect returned stats. Therefore the asserted dict matches exactly.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a dict with np.nan values when data is empty. The test uses self.assertEqual to compare that dict with another dict containing np.nan. In Python/NumPy, NaN != NaN, so the two dictionaries are not equal and the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function computes statistics from the 'Age' column of the DataFrame and returns a matplotlib Axes. For the input data [[30, 75000, 5]] the computed stats are sum=30, mean=30.0, min=30, max=30 (numpy integer values compare equal to Python ints), and the returned ax is a valid matplotlib Axes instance. No errors occur when plotting a single value.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe function checks if the provided column is in the valid_columns list and raises a KeyError when it's not. Calling task_func('InvalidColumn', data) triggers that branch, so the test expecting a KeyError will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function creates a pie chart with one wedge per data point using column_data (Salary) and labels set to the Age values. For the provided data of 5 rows, ax.patches will contain 5 wedges and ax.texts will include the Age labels (as strings) alongside percentage texts. The column name is valid and data is non-empty, so no exceptions occur. Therefore the assertions in the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/592", "raw_responses": {"test_csv_file_creation": "```plaintext\n[Explanation]\nThe test expects a file named \"sensor_data.csv\" in OUTPUT_DIR. The task_func creates a file named \"sensor_data_{timestamp}.csv\" (with a timestamp suffix), so the exact path FILE_PATH = os.path.join(OUTPUT_DIR, 'sensor_data.csv') will not exist after calling task_func(1). Therefore the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_csv_file_rows": "```plaintext\n[Explanation]\nThe test expects a file named './output/sensor_data.csv' with 25 lines (header + 24 rows). The function creates a file named with a timestamp (e.g. 'sensor_data_YYYYMMDD_HHMMSS.csv'), so the test's FILE_PATH won't exist. Additionally, for hours=24 the code uses start_time.replace(hour=start_time.hour + i), which will raise ValueError once start_time.hour + i > 23 (very likely unless run exactly at midnight), causing the function to error before completing. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_csv_file_header": "```plaintext\n[Explanation]\nThe test expects a file named './output/sensor_data.csv'. The function creates a file with a timestamped name like 'sensor_data_YYYYMMDD_HHMMSS.csv' (not 'sensor_data.csv'), so './output/sensor_data.csv' will not exist. Although the header is written correctly, the mismatched filename causes the test to fail when attempting to open the expected path.\n\n[Result]\n```plaintext\n[FAIL]\n```", "test_file_path_return": "```plaintext\n[Explanation]\nThe function constructs a filename with a timestamp (e.g. \"sensor_data_20251216_123456.csv\") and returns that path. The test expects the fixed filename \"sensor_data.csv\" in OUTPUT_DIR. These will not match, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_no_hours_data": "```plaintext\n[Explanation]\nThe test expects a file named './output/sensor_data.csv', but task_func writes a timestamped filename like './output/sensor_data_YYYYMMDD_HHMMSS.csv'. For hours=0 the function does create a CSV with only the header (one line), so content would satisfy the test if the filename matched. However, because the test opens a fixed filename that is never created, it will raise FileNotFoundError and the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/785", "raw_responses": {"test_archive_txt_files": "```plaintext\n[Explanation]\nThe function will find the two .txt files under ./test_files, create /tmp/archive if needed, run tar to create an archive (e.g. /tmp/archive/archive_.txt.tar.gz), delete the original .txt files, and return the archive path. The test only asserts the archive file exists and that no .txt files remain (the latter is checked via glob which yields no entries after deletion). Assuming a standard environment with the tar command available, the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_archive_image_files": "```plaintext\n[Explanation]\nThe function will find the two .jpg files in ./test_files, create /tmp/archive if needed, run tar to create the archive, remove the original jpg files, and return the archive path. The test checks that the archive file exists and that .jpg files are gone. Assuming the test environment provides the 'tar' executable (so subprocess.run succeeds), the code meets the test's expectations.\n\n[Result]\nPASS\n```", "test_no_matching_files": "```plaintext\n[Explanation]\nThe test calls task_func with a pattern that matches no files and expects the string 'No files found matching the pattern.' The implementation returns None when no files are found. This mismatch causes the assertion to fail. (Additionally, tearDown will attempt to remove ARCHIVE_DIR which was never created, possibly raising an error, but the primary cause is the differing return value.)\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_multiple_archiving_unique_naming": "```plaintext\n[Explanation]\nThe function archives matching files and then deletes them. The test calls task_func three times: the first call creates the archive and removes the .txt files, so the second and third calls find no files and return None. Therefore archive_path1 and archive_path2 are both None and equal, so the assertion passes.\n\n[Result]\nPASS\n```", "test_archiving_in_nonexistent_directory": "```plaintext\n[Explanation]\nThe test calls task_func with pattern '/tmp/archive/*.txt' after ensuring ARCHIVE_DIR does not exist. task_func creates /tmp/archive and then glob.glob finds no files, so it returns None. The test then does os.path.isfile(archive_path) where archive_path is None, causing a TypeError (os.path.isfile expects a path-like), so the test will error/fail. Therefore the code does not pass the test.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/942", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe DataFrame and columns match and there are 5 unique categories, but the test expects the plot title 'Category-wise Sales Trends' while the function sets 'Sales Report by Category', so the final assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function uses pandas.date_range with freq='WOM-2FRI' and periods=7 starting at '2021-01-01', which yields 7 distinct dates on/after 2021-01-01. The DataFrame is constructed as the Cartesian product of those 7 dates and the 5 default categories, giving 35 rows. All test assertions (min date >= start, 7 unique dates, 35 rows) will be satisfied.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function generates a DataFrame combining the generated dates with the provided categories. When called with freq='W-TUE' and categories=['Books','Games'], the DataFrame will contain exactly those two unique category values. The code sets the random seed and populates Sales, and the plotting calls should not affect the DataFrame contents. Therefore both assertions in the test (two unique categories and they are 'Books' and 'Games') will hold.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function builds a DataFrame with all combinations of the provided dates and categories. With start_date='2019-06-01', periods=10, freq='W-WED', and categories=['Food','Clothing'], the DataFrame will contain both categories for each date (20 rows total). The test checks that there are exactly 2 unique categories and they match the provided list \u2014 both conditions hold. No errors from plotting or date generation are expected.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function generates a date range of length 13 and builds one row per (date, category). Passing categories=['Electronics'] produces 13 rows all with Category == 'Electronics'. The sales column and plotting do not affect these checks. pd.date_range with freq='WOM-2FRI' is supported, so both assertions (all categories equal and length is 13) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1012", "raw_responses": {"test_successful_download_and_extraction": "```plaintext\n[Explanation]\nThe test calls task_func with a real Google Drive URL and does not mock requests. In typical restricted test environments there is no network access (or the Drive link may not return a direct zip), so requests.get will fail or the downloaded content won't be a valid zip. The function catches the exception and returns an error tuple, causing the test assertions (which expect a successful download and non-empty extracted files) to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with status_code = 404. MagicMock.raise_for_status does not automatically raise, so response.raise_for_status() does not raise and execution continues. Writing response.content (a MagicMock) in binary mode raises a TypeError; the function catches it and returns an error message prefixed with \"Error: ...\". The test expects the message to contain \"Download failed\", so the assertion will fail (though the returned list is []). Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_non_200_http_response": "```plaintext\n[Explanation]\nThe test patches requests.get to return a mock with status_code=404 but does not configure raise_for_status to raise. The code calls response.raise_for_status() (which does nothing on the mock) and then attempts to write response.content (also a MagicMock) to a file, causing a TypeError. The function catches the exception and returns a message starting with \"Error:\", not containing \"Download failed\" as the test expects. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```", "test_network_error": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.exceptions.ConnectionError. task_func calls requests.get inside a try/except Exception block, so the exception will be caught and it will return a tuple whose first element contains \"Error\" and whose second element is an empty list. That satisfies both assertions in the test. Directory creation and cleanup do not interfere with this behavior.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_corrupted_zip_file": "```plaintext\n[Explanation]\nThe test patches requests.get, builtins.open, and zipfile.ZipFile. The mocked requests.get returns a MagicMock response (raise_for_status exists as a MagicMock and does nothing), and open is mocked so writing is fine. zipfile.ZipFile is set to raise zipfile.BadZipFile when called, which the function catches and returns ('Error: ...', []). The test asserts 'Error' in the message and that the returned list is empty \u2014 both are satisfied.\n\n[Result]\nPASS\n```", "test_request_exception": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.exceptions.RequestException. The function calls requests.get inside a try block, so the raised exception is caught by the except Exception as e branch, which returns a tuple whose first element contains 'Error' and whose second element is an empty list. These satisfy the test assertions.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/124", "raw_responses": {"test_output_types": "```plaintext\n[Explanation]\nThe test expects 20 histogram bar heights, but the function creates bins=range(1,102) which yields 100 bins (edges 1..101) so ax.containers[0] will contain 100 bar rectangles (heights), not 20. This length/count mismatch causes the equality assertion to fail. The other type checks would pass, but the histogram data does not match the expected list.\n\n[Result]\nFAIL\n```", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe function immediately checks isinstance(my_list, list) and raises TypeError(\"'my_list' must be a list\") if false. Calling task_func('not a list') will trigger that TypeError before any other code runs, so the unittest expecting a TypeError will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_list_elements": "```plaintext\n[Explanation]\nThe function checks that my_list is a list and iterates over its items, raising ValueError if any item is not an int or float. Passing [1, 2, 'three'] triggers this check for 'three' (a str), so a ValueError is raised as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_plot_title": "```plaintext\n[Explanation]\nThe function sets ax.set_title('Histogram of Random Numbers') and returns the Axes. For my_list = [1, 2, 3] the code runs without raising exceptions and produces an Axes whose title matches the expected string.\n\n[Result]\nPASS\n```", "test_time_measurement": "```plaintext\n[Explanation]\nThe function accepts the input list [1,2,3], appends 12 to make total_sum 18, so it generates 18 random integers. time_taken is computed as end_time - start_time using time.time(), which will be a non-negative float in normal operation. No type/value errors occur for this test, and matplotlib usage doesn't interfere with returning time. Therefore the assertion time_taken >= 0 will hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_size_limit": "```plaintext\n[Explanation]\nThe function appends 12 to [30,40,50], so total_sum = 132 and actual_size = min(132, 100) = 100. task_func generates 100 random ints and returns (time_taken, ax). The test then computes np.histogram over a list of 100 randint(1,100) values with bins=20; np.histogram returns an array of counts of length 20, so len(n) == 20. The top-level import \"from random import randint\" in the submitted code makes randint available for the test code block. Creating the matplotlib Axes does not interfere with the assertion. Therefore the test's assertion will hold.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/19", "raw_responses": {"test_single_file_zip": "```plaintext\n[Explanation]\nThe function creates a zip of all files in the given directory. The test creates one file in a temporary directory, so files list is non-empty. The code then creates files.zip in that directory and returns its path. The test only asserts that the returned path exists, which will be true. No errors or edge cases affect this single test.\n\n[Result]\nPASS\n```", "test_multiple_files_zip": "```plaintext\n[Explanation]\nThe function checks the directory exists, collects all files (non-recursively), creates 'files.zip' inside the directory, writes each file by basename, and returns the zip path. The test creates 5 files in a temporary directory, calls task_func and then asserts the returned zip_path exists. The implementation will create that zip file, so the assertion will be True.\n\n[Result]\nPASS\n```", "test_empty_directory": "```plaintext\n[Explanation]\nThe function checks that the directory exists (it does), collects non-directory files with glob (none in an empty tempdir), and returns None when no files are found. The test creates an empty temporary directory and asserts the return is None, so the code behavior matches the test.\n\n[Result]\nPASS\n```", "test_non_existent_directory": "```plaintext\n[Explanation]\nThe test calls task_func('/non/existent/directory') and asserts a FileNotFoundError is raised. The function checks os.path.exists(directory) and raises FileNotFoundError with a message if the path doesn't exist, so it will raise the expected exception for a non-existent path.\n\n[Result]\nPASS\n```", "test_exclusion_of_subdirectories": "```plaintext\n[Explanation]\nThe function lists only files in the given directory (glob '*' filtered by os.path.isfile), so subdirectories and their contents are excluded. It collects the file list before creating files.zip, so the new zip is not included in the list. Given one top-level file 'testfile.txt' in the test, exactly one entry will be written to the zip. The test's assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_integrity_in_zip": "```plaintext\n[Explanation]\nThe function zips all regular files in the given directory into 'files.zip' using each file's basename. The test creates 'testfile.txt' with specific content, calls task_func on the temp directory, then opens the produced archive and reads 'testfile.txt'. The implementation correctly finds the file, writes it into the ZIP as 'testfile.txt', and the content will match when read back. No exceptions or timing/closing issues occur.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/287", "raw_responses": {"test_single_file_few_words": "", "test_multiple_files": "```plaintext\n[Explanation]\nThe function writes per-file word counts into ./testdir/test_output.json, but the test opens 'test_output.json' from the current working directory (mismatch in path). Even if the file path matched, the JSON content the function writes is a mapping of filenames to per-file counters, while the test expects a single aggregated dictionary of word -> total count. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_empty_files": "```plaintext\n[Explanation]\nThe test creates an empty file in ./testdir/ and then opens 'test_output.json' in the current working directory. The function writes the JSON output to os.path.join(directory, filename) (i.e. ./testdir/test_output.json), so the test's open('test_output.json') will fail with FileNotFoundError. Even if the test opened the file in the directory, the function would write {'empty_file.txt': {}} whereas the test expects {} \u2014 so the test will not pass.\n\n[Result]\nFAIL\n```", "test_files_with_special_characters": "```plaintext\n[Explanation]\nThe test expects a JSON file named 'test_output.json' in the current working directory containing a flat mapping of words to counts for the single file. The implementation writes the JSON to os.path.join(directory, filename) (i.e., './testdir/test_output.json') instead of the CWD, so the test's open('test_output.json', 'r') will fail with FileNotFoundError. Even if the path matched, the code writes a dict keyed by the text filename ({'special_chars.txt': {...}}) rather than the expected flat word-count dict, so the contents would not match the expected result.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_nested_directories": "```plaintext\n[Explanation]\nThe test writes a .txt file into nested_dir and calls task_func('test_output.json', nested_dir). The function writes the JSON to nested_dir/test_output.json, but the test attempts to open 'test_output.json' in the current working directory, causing a FileNotFoundError. Even if the file path matched, the function writes a dict keyed by filenames (e.g. {\"nested_file.txt\": {\"hello\": 2, \"world\": 1}}) whereas the test expects just {\"hello\": 2, \"world\": 1}. Therefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/409", "raw_responses": {"test_normal_functionality": "```plaintext\n[Explanation]\nThe function reads the Excel file, selects the 'Sales' column and computes mean/median/std using NumPy (population std). For the test data [100,200,150,300,250] the mean and median are 200 (200.0 == 200 is True) and the population std is sqrt(5000)=70.71067811865476, matching the test. Reading/writing Excel with pandas should work in the test environment. No mismatches in types or formulas that would cause the assertions to fail.\n\n[Result]\nPASS\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test calls task_func with a filename that does not exist in the provided directory. The implementation constructs the full path and explicitly checks os.path.exists(full_file_path); if the file is missing it raises FileNotFoundError. This matches the test's expectation, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_column_name": "```plaintext\n[Explanation]\nThe test calls task_func with an existing Excel file ('test_file.xlsx') and a non-existent column name. The function builds the correct file path, reads the Excel file with pandas, checks for the column, and raises ValueError when the column is missing. That matches the test's expected behavior.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_excel_file": "```plaintext\n[Explanation]\nThe test creates an empty Excel file (no columns) and calls task_func on it for column 'Sales'. task_func reads the file and checks for the column; since it's absent it raises a ValueError as expected by the test. File paths and setup/teardown are consistent, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_with_different_data_types": "```plaintext\n[Explanation]\nThe test writes an Excel file where the 'Sales' column contains mixed ints and strings. task_func reads the file and calls numpy functions (np.mean, np.median, np.std) on the pandas Series. Converting to an object array, np.mean will attempt to add ints and strings, causing a TypeError which propagates out. The test expects a TypeError via assertRaises, so the code behavior matches the test.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/593", "raw_responses": {"test_dataframe_content": "```plaintext\n[Explanation]\nThe test expects the function to write a file named exactly './output/traffic_data.csv', call pandas.read_csv on that path, and call matplotlib.pyplot.show(). The implementation instead writes a timestamped filename 'traffic_data_{timestamp}.csv', does not call pandas.read_csv, and does not call plt.show(). Although randint is patched and would be called, the mismatches (filename, missing read_csv call, and missing plt.show()) cause the assertions to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_empty_dataframe_on_zero_hours": "```plaintext\n[Explanation]\nWhen task_func(0) is called it creates an empty DataFrame and still constructs a matplotlib figure and axes (fig, ax = plt.subplots(...)) and returns that ax. The test expects ax to be None for zero hours. The patched pd.read_csv is unused by task_func, so it doesn't affect behavior. Therefore the returned ax will not be None and the assertion will fail.\n\n[Result]\nFAIL\n```", "test_directory_creation": "```plaintext\n[Explanation]\nThe test patches os.makedirs and os.path.exists and then calls task_func(1). task_func calls os.makedirs(output_dir, exist_ok=True). The test asserts mock_makedirs.assert_called_with(os.path.dirname(FILE_PATH)), which expects no keyword arguments. Because the actual call includes the keyword argument exist_ok=True, the mock assertion will fail due to the unexpected kwarg.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_plot_generation": "```plaintext\n[Explanation]\nThe test patches plt.show and then calls task_func(1) and asserts that plt.show was called. The provided task_func creates and configures a Matplotlib plot but never calls plt.show(), so the patched mock is never invoked. Therefore the test's assert_called() will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_task_func_runs_without_error": "```plaintext\n[Explanation]\nThe test only checks that task_func(1) runs without raising an exception. The function creates the output directory, builds a small DataFrame, writes a CSV, and constructs a matplotlib figure/axes. All required modules (os, plt, pandas, etc.) are imported at top-level so the test's references (e.g., to os and plt for patching) are available. The code does not call plt.show(), and creating the figure/axes typically works in a headless environment. Therefore task_func(1) should complete without error.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/800", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function checks os.path.exists for the default 'match_data.csv'. In the test no CSV is created, so the existence check is False, it skips reading, and with empty input dicts the totals remain 0. It returns Counter({'goals': 0, 'penalties': 0}) which matches the expected result. No errors or side effects occur.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test calls task_func with goals totaling 5 and penalties totaling 3 and does not create or reference 'match_data.csv'. Since os.path.exists('match_data.csv') will be False, the CSV-reading branch is skipped and only the provided dictionaries are summed. The function returns Counter({'goals': 5, 'penalties': 3}), which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function uses the default CSV path 'match_data.csv'. The test does not create or mock any file, so os.path.exists(csv_file_path) will be False and the file-reading branch is skipped. With empty input dictionaries, totals remain zero and the function returns Counter({'goals': 0, 'penalties': 0}), which matches the expected result.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function uses os.path.exists on the default 'match_data.csv'; in the test no CSV is created so the file branch is skipped. It sums goals (2+3=5) and penalties (1+2=3) and returns Counter({'goals':5,'penalties':3}). Comparing a Counter to an equivalent dict is equal, so the assertion passes.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function checks for the default CSV file 'match_data.csv'. In this test no CSV is created, so the file read branch is skipped and totals start at 0. Summing the provided dictionaries gives goals = -2 + 3 = 1 and penalties = 1 + (-2) = -1. The returned Counter matches the expected Counter({'goals': 1, 'penalties': -1}).\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/1013", "raw_responses": {"test_empty_page": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock (with text='<html></html>'). The code uses response.content (which becomes a MagicMock), but BeautifulSoup will parse it without finding any <a> tags, so links list is empty. The function returns len(absolute_links) == 0 and writes a CSV (which the tearDown removes). No exception is raised, so the assertion result == 0 will pass.\n\n[Result]\nPASS\n```", "test_single_link": "```plaintext\n[Explanation]\nThe test's mock sets requests.get.return_value to a MagicMock with only the text attribute containing the HTML. The function uses response.content when calling BeautifulSoup. Because the MagicMock has no content attribute set, response.content is another MagicMock (not the HTML string), so BeautifulSoup does not parse the expected anchor tag. No links are found and the function returns 0, but the test expects 1. Therefore the code fails this test.\n\n[Result]\nFAIL\n```", "test_multiple_links": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with its text attribute set to the HTML string, but the code uses response.content when calling BeautifulSoup. The mocked response does not provide the expected .content HTML, so BeautifulSoup won't find the two <a> tags and the function will return 0 instead of the expected 2. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_duplicate_links": "```plaintext\n[Explanation]\nThe test mocks requests.get.return_value with a MagicMock that has .text set to the HTML, but the code uses response.content when creating the BeautifulSoup object. Accessing .content on that MagicMock yields another MagicMock (not the HTML string), so BeautifulSoup does not receive the HTML and finds no <a> tags. The function returns 0 instead of the expected 1, so the test will fail.\n\n[Result]\nFAIL\n```", "test_external_links": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with its text attribute set to the HTML string, but the function uses response.content. The MagicMock has no .content set, so response.content will be another MagicMock (not the HTML bytes). BeautifulSoup will not parse the intended HTML, no <a> tag will be found, and the function will return 0 instead of the expected 1. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/129", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe test patches requests.get and sets only mock_get.return_value.text, not response.content. The function calls response.raise_for_status() (a harmless Mock) and then passes response.content to BeautifulSoup. Because .content is a Mock (not the HTML string), BeautifulSoup parses the Mock's string representation which does not contain the <table>, so table = None and the function raises ValueError(\"No table found on the page\"). The test expects a DataFrame and does not handle an exception, so the test will fail.\n\n[Result]\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.ConnectionError. task_func does not catch or convert that exception, so the requests.exceptions.ConnectionError will propagate. The test, however, uses self.assertRaises(ConnectionError) referring to the built-in ConnectionError, which is a different exception class. Because requests.ConnectionError is not the same as (nor a subclass of) the built-in ConnectionError, the raised exception will not match the expected type and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_empty_page": "```plaintext\n[Explanation]\nThe test patches requests.get to return a Mock whose .text is '' (and .content is a Mock). task_func calls response.raise_for_status() (a Mock, so no exception) and passes response.content to BeautifulSoup. BeautifulSoup will convert the mocked content to a string and find no <table>, causing the function to raise ValueError(\"No table found on the page\"). The test expects a ValueError, so the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_html_content": "```plaintext\n[Explanation]\nThe test patches requests.get to return a Mock with .text set to non-HTML. task_func calls response.raise_for_status() (the Mock method does not raise) and then uses BeautifulSoup on response.content (which is a Mock and parses to no <table>), so table is None and the function raises ValueError(\"No table found on the page\"). That matches the test's assertRaises(ValueError), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_http_error": "```plaintext\n[Explanation]\nThe test patches requests.get to return a mock whose raise_for_status raises requests.HTTPError. task_func calls requests.get(...) then immediately calls response.raise_for_status(), so the requests.HTTPError will be raised and propagate. That matches the test's assertRaises expectation. No other code paths are executed before the exception, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_return_type_with_complex_data": "```plaintext\n[Explanation]\nThe test patches requests.get and sets the mock response's .text to the HTML, but the function uses response.content (and calls response.raise_for_status()). The mocked response has no .content containing the HTML bytes, so BeautifulSoup receives a Mock object (or empty content) and fails to find a <table>, causing a ValueError instead of returning a DataFrame. Therefore the code will not pass the test.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/17", "raw_responses": {"test_process_not_found_starts_process": "```plaintext\n[Explanation]\nThe test patches psutil.process_iter to return an empty list, so the code sets process_found = False and enters the \"not found\" branch. It calls subprocess.Popen once (patched) with the given process name and returns the exact expected string. The mock assertions match this behavior, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_process_found_restarts_process": "```plaintext\n[Explanation]\nThe test patches psutil.process_iter to return a MagicMock whose name() is 'notepad', but the code checks proc.info['name']. The MagicMock does not provide proc.info['name']=='notepad', so process_found stays False. The function will return \"Process not found. Starting notepad.\" and will not call process.terminate(), which mismatches the test expectations.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_process_terminates_and_restarts_multiple_instances": "```plaintext\n[Explanation]\nThe code checks proc.info['name'] when iterating psutil.process_iter, but the test supplies MagicMock process objects with a .name() method (and no .info dict). proc.info['name'] therefore evaluates to a MagicMock not equal to the string 'multi_instance', so process_found stays False. The function will take the \"not found\" branch, return \"Process not found. Starting multi_instance.\", and will not call terminate on the mocked processes. The test expects \"Process found. Restarting multi_instance.\" and that both process.terminate() were called, so the assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/302", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nThe function computes the Pearson correlation of the three columns formed from the lists [[8,10,12], [7,9,11]]. These columns are perfectly linearly related, so corr_matrix is a 3x3 matrix of 1.0s (index/columns 0,1,2). The expected_result is a 3x3 matrix of integer 1s; DataFrame.equals considers 1.0 equal to 1, so result.equals(expected_result) returns True. The test uses assertFalse(result.equals(expected_result)), which therefore fails.\n\n[Result]\nFAIL\n```", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test creates an empty DataFrame (no rows). task_func immediately checks if df.empty and raises ValueError(\"DataFrame is empty\"). This matches the test's expectation that a ValueError is raised, so the test will pass.\n\n[Result]\nPASS\n```", "test_plot_generation": "```plaintext\n[Explanation]\nThe function accepts the provided DataFrame (not empty, 'Value' entries are lists), creates a correlation matrix, and when plot=True it draws a seaborn heatmap, sets the title to 'Correlation Heatmap', and returns (corr_matrix, ax). The returned ax is a matplotlib Axes instance, so both assertions (instance check and title match) succeed.\n\n[Result]\nPASS\n```", "test_invalid_data": "```plaintext\n[Explanation]\nThe test supplies a DataFrame whose 'Value' entry is a list of strings. The function only checks that df is non-empty and that df['Value'].iloc[0] is a list (which it is), so it proceeds to build value_df and calls value_df.corr(). pandas.corr() on non-numeric (object) columns does not raise ValueError \u2014 it yields an empty DataFrame \u2014 and the function returns that. Therefore no ValueError is raised and the test, which expects a ValueError, will fail.\n\n[Result]\nFAIL\n```", "test_plot_data_correlation": "```plaintext\n[Explanation]\nThe function creates a correlation matrix from the provided lists, plots a seaborn heatmap with annotations, and returns (corr_matrix, ax). For the test DataFrame the correlation matrix is all ones; seaborn's annot=True produces text annotations that are convertible to floats (e.g., \"1\" or \"1.00\"). The test reads the text annotations from ax, reshapes them to the correlation shape, converts to float, and compares with corr_df values\u2014these will match. The input validation and plotting/return behavior align with the test, so the assertion should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/417", "raw_responses": {"test_model_type": "```plaintext\n[Explanation]\nThe function constructs and returns a keras.models.Sequential instance (model) and the test only asserts that the returned model is an instance of Sequential. The code imports and uses Sequential from keras, and the test imports the same, so the instance check will succeed. No runtime errors are expected for this small dataset and training call.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_axes_type": "```plaintext\n[Explanation]\nThe function creates a matplotlib Axes with fig, ax = plt.subplots() and returns that ax. The unittest checks isinstance(ax, plt.Axes), which will be True for the Axes object produced. The Keras model training and plotting should run without affecting the Axes type. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_axes_title": "```plaintext\n[Explanation]\nThe function explicitly sets the Axes title with ax.set_title('Model loss') and returns the Axes object. The test only checks ax.get_title() equals 'Model loss', which will be true. There are no issues in the code relevant to this assertion.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_axes_xlabel": "```plaintext\n[Explanation]\nThe function explicitly sets ax.set_xlabel('Epoch') and returns the Axes object. The test only checks ax.get_xlabel() == 'Epoch'. The model training and plotting should run without affecting the xlabel, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_axes_ylabel": "```plaintext\n[Explanation]\nThe function sets the y-axis label with ax.set_ylabel('Loss'), so ax.get_ylabel() will return 'Loss' for the plotted Axes. The model training and plotting code do not alter the label afterward. Therefore the test asserting the y-label equals 'Loss' should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_model_output_shape": "```plaintext\n[Explanation]\nThe function builds a Sequential model with Dense(1, input_dim=2), so the model's output shape is (None, 1). The provided X and Y shapes are compatible, train_test_split works, and training on the small dataset will complete quickly. The function returns the model, and model.output_shape will equal (None, 1) as asserted by the test. No errors expected from optimizer/loss/plotting in typical headless test environments.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_model_weights": "```plaintext\n[Explanation]\nThe model created contains a single Dense layer with 1 unit and input_dim=2, so model.get_weights() returns two arrays (kernel shape (2,1) and bias shape (1,)), making len(weights) == 2. The function returns the model as expected and no operations in the code should raise errors for the provided 4-sample XOR-like input. Therefore the test's assertion passes.\n\n[Result]\nPASS\n```", "test_model_loss": "```plaintext\n[Explanation]\nThe function compiles the model with loss='binary_crossentropy' and returns the model. The test checks that the string 'binary_crossentropy' is present in model.loss. In Keras the compiled loss is stored as that name string in model.loss when provided as a string, so the membership test succeeds. The rest of the function (data split, fit, plotting) does not interfere with this check.\n\n[Result]\nPASS\n```", "test_model_optimizer": "```plaintext\n[Explanation]\nThe function creates an SGD optimizer via \"sgd = SGD(...)\" using the same keras.optimizers.SGD imported in the test and passes it to model.compile. After compile, model.optimizer is the optimizer instance created (of class SGD). Therefore isinstance(model.optimizer, SGD) will be True. No other errors affect this specific test.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/513", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly validates input, builds a DataFrame, checks non-negative numeric columns, computes sum/mean/min/max for 'Steps', and returns a matplotlib Axes with the expected title. Although pandas returns numpy numeric types, equality checks with Python ints/floats succeed (numpy types compare equal to native types). Given the provided data, the computed stats are {'sum': 16500, 'mean': 5500.0, 'min': 5000, 'max': 6000} and the axis title is 'Line Chart of Steps', so the test assertions will pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function builds a DataFrame from the given data, validates inputs (non-empty, valid column, non-negative numeric values), computes sum/mean/min/max for 'Calories Burned' correctly (250+275+300 = 825, mean 275.0) and creates a matplotlib Axes with the title 'Line Chart of Calories Burned'. The numeric types returned by pandas compare equal to Python ints/floats in unittest assertEqual. No errors occur for the provided test data.\n\n[Result]\nPASS\n```", "test_case_3": "", "test_case_4": "```plaintext\n[Explanation]\nThe function accepts the provided data, constructs a DataFrame, and the non-negative checks pass because all numeric entries are 0. For 'Steps' it computes sum=0, mean=0.0, min=0, max=0 (types may be numpy scalars but compare equal). It also returns an Axes whose title is set to 'Line Chart of Steps'. Therefore the assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function computes statistics from the 'Calories Burned' column and creates a plot with the exact title. The input data are non-negative so validation passes. The returned stats use numpy numeric types but compare equal to the expected Python ints/floats. The plot title matches. Therefore the test assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe test calls task_func with column 'Invalid Column'. The function checks valid_columns and will raise a KeyError for an invalid column. KeyError is a subclass of Exception, so the with self.assertRaises(Exception): will catch it. No earlier code paths raise errors for the provided non-empty data.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function validates numeric columns and raises ValueError if any value is negative. The test provides a single row with Steps = -5000 (negative), so the check (df['Steps'] < 0).any() will be True and task_func will raise ValueError as expected.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_8": "```plaintext\n[Explanation]\nThe input data is non-empty and 'Steps' is a valid column. The function builds a DataFrame, validates non-negative numeric values, and computes sum/mean/min/max for the 'Steps' series. For a single row [5000], these produce 5000, 5000.0, 5000, 5000 respectively. The matplotlib plot creation should not raise an error in typical test environments, and numpy scalar results compare equal to the expected Python numeric literals, so the dictionaries match.\n\n[Result]\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function computes statistics directly from the provided data without requiring sorting; for the 'Steps' column it sums 6000+5000+5500 = 16500, mean 5500.0, min 5000, max 6000. The returned numeric values (numpy scalars) compare equal to the expected Python ints/floats, and no validations raise errors for the given input. Plotting does not affect the returned stats. Therefore the test's expected dict matches the function output.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_10": "```plaintext\n[Explanation]\nThe function immediately checks \"if not data\" and raises ValueError(\"Data list cannot be empty\") for an empty list. The test expects any Exception when calling task_func('Steps', []) so the raised ValueError satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_11": "```plaintext\n[Explanation]\nThe function accepts the provided data and column 'Steps'. It creates a DataFrame, validates non-negative numeric values (all positive), computes stats, and plots Date vs Steps. The axis title, xlabel, and ylabel are explicitly set to 'Line Chart of Steps', 'Date', and 'Steps' respectively, which match the test assertions. No exceptions are raised for this input.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_12": "```plaintext\n[Explanation]\nThe function creates a DataFrame from the input data, validates non-negative numeric values, and plots the requested numeric column against the Date values. The test uses 'Distance Walked' with values [1.0, 2.0]; matplotlib's plotted line will contain y-data [1.0, 2.0]. The test refers to np, which is available because the provided code imports numpy as np at module scope. Therefore the array comparison will succeed.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/594", "raw_responses": {"test_task_func_checks_backup_directory_exists": "```plaintext\n[Explanation]\nThe test patches os.path.exists and then checks mock_exists.call_args to see what path was checked. The provided task_func never calls os.path.exists (it uses os.makedirs and shutil.copy2 directly), so mock_exists will not have been called and mock_exists.call_args will be None. Attempting to index call_args in the test will raise an error / cause the equality check to fail. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_task_func_copies_to_backup_directory": "```plaintext\n[Explanation]\nThe test patches shutil.copy and then checks mock_copy.call_args, but the code calls shutil.copy2 (not patched). As a result the mock is never called and mock_copy.call_args is None, causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_task_func_writes_correct_header": "", "test_backup_file_creation": "```plaintext\n[Explanation]\nThe test patches shutil.copy and asserts it was called with (FILE_PATH, BACKUP_PATH). The implementation calls shutil.copy2 (not patched) and copies to a timestamped backup file path (not the backup directory). As a result the mock_copy is never called and the assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```", "test_csv_writing": "```plaintext\n[Explanation]\nThe test patches csv.writer and asserts it was called once. task_func(1) creates a csv.DictWriter which internally calls csv.writer exactly once at instantiation. For hours=1 the datetime.replace call is valid (no hour overflow). Patching csv.writer globally will intercept that internal call. The rest (file creation, backup) does not raise errors for this input. Therefore the mock will be called once and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/826", "raw_responses": {"test_files_moved": "```plaintext\n[Explanation]\nThe regex pattern r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b' matches the provided filenames ('file1.txt', 'document1.doc', 'notes.docx'), and the function moves files matching this pattern. Non-matching files remain in the source. The function creates the target directory, checks isfile, moves matching files, and returns the correct count (3). Therefore the test's assertions should all pass.\n\n[Result]\nPASS\n```", "test_no_files_moved": "```plaintext\n[Explanation]\nThe test uses custom_pattern = '\\.pdf$'. The function uses pattern.match(item), which only matches at the start of the filename; a pattern that expects \".pdf\" at the end will not match any filenames that don't start with \".pdf\". Therefore no files are moved, moved_files_count stays 0, and the target directory remains empty \u2014 matching the test assertions.\n\n[Result]\nPASS\n```", "test_directory_does_not_exist": "```plaintext\n[Explanation]\nThe function calls os.listdir(source_dir) without catching exceptions. If the source directory has been removed, os.listdir raises FileNotFoundError, which matches the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_source_directory": "```plaintext\n[Explanation]\nThe test clears the source directory before calling task_func, so os.listdir(source_dir) is empty. The function iterates over no items, leaves moved_files_count at 0, ensures the target directory exists (already empty), and returns 0. This matches the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_target_directory_creation": "```plaintext\n[Explanation]\nThe regex pattern r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b' matches the three test filenames (file1.txt, document1.doc, notes.docx). task_func creates the target directory if missing and moves matching files from source to target. The test only requires the target to be recreated and at least one matched file present in the target; the implementation performs these actions. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1015", "raw_responses": {"test_valid_webpage_url": "```plaintext\n[Explanation]\nThe mocked requests.get returns a MagicMock with .content as bytes and .raise_for_status callable (no exception), so fetching succeeds. pandas.read_html accepts bytes HTML and will return a DataFrame with one row. The code writes the table to an SQLite DB and returns len(df) == 1. The test expects 1, so the code passes.\n\n[Result]\nPASS\n```", "test_local_file_url": "```plaintext\n[Explanation]\nThe mocked local file contains a single-row table with only <td> cells. pandas.read_html by default treats the first row as header (header=0), so that single row becomes the header and the resulting DataFrame has 0 data rows. The function then returns 0, but the test expects 1, so the test will fail.\n\n[Result]\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.RequestException. task_func calls requests.get for a non-file URL and catches RequestException only to re-raise it. Therefore calling task_func('http://invalid-url.com') will raise requests.RequestException as the test expects. No other code paths are executed before the exception, so the test will pass.\n\n[Result]\nPASS\n```", "test_empty_table": "```plaintext\n[Explanation]\nThe mocked requests.get returns a response with HTML containing an empty <table>. The function reads response.content, pandas.read_html either raises ValueError (no tables) or returns a DataFrame that's empty; the code handles both cases and returns 0 without creating the database. This matches the test expectation.\n\n[Result]\nPASS\n```", "test_database_error": "```plaintext\n[Explanation]\nThe test patches requests.get to return HTML containing a table and patches sqlite3.connect to raise sqlite3.DatabaseError when called. The function successfully fetches and parses the HTML and reaches the sqlite3.connect call, which raises DatabaseError; the function catches and re-raises that error. Therefore the test's assertRaises(sqlite3.DatabaseError) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/139", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function selects numeric columns A and B, creates a matplotlib figure and Axes for each, and appends the Axes objects to a list which it returns. The test asserts each returned element is an instance of plt.Axes. plt.subplots produces Axes (a subclass of matplotlib.axes.Axes), so the isinstance checks will succeed. There are no other error conditions for the provided DataFrame, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_input_empty_dataframe": "```plaintext\n[Explanation]\nThe function checks for isinstance(pd.DataFrame) and then raises ValueError if df.empty is True. Calling task_func(pd.DataFrame()) uses an empty DataFrame, so df.empty triggers the ValueError as the test expects. The plotting code is not reached, so no other errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe function immediately checks isinstance(df, pd.DataFrame) and raises ValueError when given a non-DataFrame. The test calls task_func('not a dataframe') and expects a ValueError, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_no_numeric_columns": "```plaintext\n[Explanation]\nThe test calls task_func on a DataFrame that contains only a non-numeric column 'C'. The function checks for numeric columns using df.select_dtypes(include=[np.number]) and raises ValueError(\"DataFrame must contain at least one numeric column\") when none are found. The earlier DataFrame checks pass, so the correct ValueError is raised and the test's assertRaises passes.\n\n[Result]\nPASS\n```", "test_histograms_count": "```plaintext\n[Explanation]\nThe function selects numeric columns using df.select_dtypes(include=[np.number]) which picks 'A' and 'B'. It creates one Axes per numeric column and returns them in a list. Given the test DataFrame has two numeric columns, the returned list length will be 2, satisfying the assertion. No other errors occur for this input.\n\n[Result]\nPASS\n```", "test_plot_labels": "```plaintext\n[Explanation]\nThe function selects numeric columns A and B, creates a histogram for each, and sets ax.set_xlabel('Value') and ax.set_ylabel('Frequency'). The test checks that 'Value' is in the x-label and 'Frequency' is in the y-label for each returned Axes. All conditions are met, so the test will pass.\n\n[Result]\nPASS\n```", "test_correctness_of_histogram_lines": "```plaintext\n[Explanation]\nThe function creates histograms using matplotlib/numpy's default bin edges which span exactly from the data minimum to maximum (numpy.linspace between min and max). The test checks the leftmost patch x and rightmost patch x+width against the column data min/max with a tolerance of 0.01; these match for both numeric columns A and B. No type or empty-data issues occur for the provided dataframe, so the assertions should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/34", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function removes URLs with the regex, splits remaining text into words, generates and returns a WordCloud instance. For the test input the URL \"https://www.example1.com\" will be removed, the returned object is a WordCloud, and that exact URL will not be a key in result.words_. plt.show() does not affect these assertions. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function removes the URL with the provided regex (it will also remove the trailing dot), builds a WordCloud, and returns it. The test checks that the return is a WordCloud instance and that the URL string is not a key in result.words_. Both conditions hold: task_func returns a WordCloud and the URL is removed, so it won't appear in the words_ dict. (Plotting via matplotlib.pyplot.show() is unlikely to break the test in the execution environment.)\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function removes URLs, splits the given sentence into words, ensures non-empty, generates and returns a WordCloud object. For the input 'There is no url in this sentence.' there are words left, so no ValueError. The function returns a WordCloud instance, matching the test's isinstance check. (Assumes the wordcloud and matplotlib packages are available, which the test expects.)\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe regex removes the entire URL leaving an empty string, so words becomes empty and the function raises ValueError with the exact expected message. The test checks for that exception and message, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function removes URLs with a regex, splits remaining text into words, generates and returns a WordCloud. For the test input the regex matches and removes both https://www.example51.com and https://www.example52.com, so the generated word cloud will not contain the URL as a token. The function returns a WordCloud instance, and calling plt.show() won't break the test in a headless test environment (matplotlib can use a non-interactive backend). Therefore the assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/418", "raw_responses": {"test_return_types": "```plaintext\n[Explanation]\nThe function trains a Keras Sequential model and returns the model and a matplotlib Axes. For the given 4-sample input and random_state=42, train_test_split with test_size=0.3 yields a test set of size 2 that contains both classes, so roc_curve can be computed. model.predict produces a (n_samples, 1) probability array which sklearn accepts. The returned objects are a keras.models.Sequential and a matplotlib Axes, matching the test assertions.\n\n[Result]\n```plaintext\nPASS\n```", "test_model_type": "", "test_model_output_shape": "```plaintext\n[Explanation]\nThe model is built with input_shape=(2,) and a single Dense(1) output, so model.output_shape will be (None, 1). The provided Y as a 1D array is acceptable to Keras for binary classification. The train_test_split with random_state=42 and test_size=0.3 on 4 samples yields 2 test samples that (with this seed) include both classes, so roc_curve won't error. The function returns the Keras Sequential model and an Axes object as required. Therefore the test that checks model.output_shape == (None, 1) will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_model_loss": "", "test_model_optimizer": "```plaintext\n[Explanation]\nThe function explicitly creates an SGD optimizer via keras.optimizers.SGD and passes that instance to model.compile, so model.optimizer will be an instance of keras.optimizers.SGD from the same tensorflow.keras import used in the test. The rest of task_func runs without errors for the provided small dataset (train_test_split with random_state=42 yields a test set containing both classes, roc_curve accepts the predicted probabilities shape), so the test's isinstance check should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_plot_axes": "```plaintext\n[Explanation]\ntrain_test_split(X, Y, test_size=0.3, random_state=42) on 4 samples yields a test set of size 1. With the given random_state that single test sample has label 1, so Y_test contains only one class. sklearn.metrics.roc_curve requires both positive and negative samples and raises a ValueError when only one class is present. The exception occurs before the function returns the Axes, so the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/526", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe input JSON for the tested case is [{'a':2,'b':3,'c':4}]. The function collects numeric values per key, computes mean and median, creates a DataFrame indexed by keys and sorted. For key 'a' it will produce mean=2.0 and median=2.0 and the index will be ['a','b','c']. No code paths prevent these results, so the assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe input file contains a single record {'a': 1}. The function collects numeric values per key, computes mean and median (both 1.0 for 'a'), and returns a DataFrame indexed by 'a'. The test checks the index and these values, so all assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe implementation only adds keys to key_values when a numeric value is encountered. For the test file [{'a': 1.5}, {'b': None}], 'b' has no numeric values and is never added, so the resulting DataFrame will contain only index ['a'] rather than ['a','b']. The test expects 'b' to appear with NaN statistics, so the assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe input JSON file contains an empty list. The function builds an empty defaultdict and results dict, so pd.DataFrame.from_dict({}, orient='index') produces an empty DataFrame with 0 rows. The test expects df.shape[0] == 0, so the function's return satisfies the assertion.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe input file contains [{'a':1.5,'c':4}, {'b': None}]. The code only adds a key to key_values when the value is numeric. Because 'b' has value None (non-numeric), 'b' is never added to key_values and therefore is missing from the resulting DataFrame index. The test expects index ['a','b','c'] with NaNs for 'b', so the index mismatch causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe function filters values by isinstance(value, (int, float)), so in the test data it keeps 5 for 'a', 4 for 'b', and 7 for 'c'. It computes mean/median per key, sorts the index to ['a','b','c'], and returns the expected numeric means. Therefore the assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function reads the JSON, collects numeric values per key (ignoring None), and computes mean/median. For the test data 'a' contains 1..100 giving mean 50.5; 'b' contains 2*i for even i -> [4,8,...,200] giving the mean expected by the test. The code uses numpy/pandas correctly, so the assertions will pass.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function only adds keys to key_values when a value is numeric. For the test file all values are strings, so key_values remains empty and the resulting DataFrame has no rows. The test expects df.loc['a','mean'] and df.loc['b','mean'] to be NaN, but those indices do not exist, causing a KeyError / test failure.\n\n[Result]\nFAIL\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function only collects values that are instances of int or float and not NaN, so strings and None are ignored. For the test data, 'a' collects 10 and 5 -> mean 7.5; 'b' collects 20 and 25 -> mean 22.5; 'c' collects 30 -> mean 30.0. These match the test's expected values.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/618", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a DataFrame with one row per team (TEAMS has 5 entries) and three columns in the order inserted ('Team','Goals','Penalty Cost'), so df.shape == (5,3) and columns match. Goals are generated with randint(0,5) so <=5, and penalties with randint(0,3) multiplied by 1000 so <=3000. The returned plots are the matplotlib Axes objects from plt.subplots, so isinstance checks against matplotlib.axes.Axes pass. No other test constraints are violated.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nCalling task_func(0, 5) uses randint(0, 0) for goals so every 'Goals' value is 0. Penalty cost is randint(0,5) * 1000, so every 'Penalty Cost' is between 0 and 5000 inclusive. The function returns a DataFrame with those columns and no other errors are raised, so both test assertions hold.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nCalling task_func(10, 0) will generate team_penalties using randint(0, 0) which is always 0, so 'Penalty Cost' (0 * PENALTY_COST) will be 0 for all teams. Goals are generated with randint(0, 10), so all 'Goals' values are <= 10. The function returns the DataFrame directly, and the test only inspects the DataFrame columns, so plotting side-effects do not affect the assertions. Therefore both assertions in the test will be satisfied.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nWith goals=0 and penalties=0, randint(0, 0) returns 0 for every team, and penalty_cost = team_penalties * 1000 is 0. The DataFrame columns 'Goals' and 'Penalty Cost' will therefore be all zeros, satisfying both assertions. The plotting calls don't affect the DataFrame and should run in typical test environments.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function uses randint(0, goals) and randint(0, penalties) so Goals values will be in [0,2]. Penalty Cost is team_penalties * 1000 where team_penalties is in [0,1], so Penalty Cost values are either 0 or 1000 (<=1000). The DataFrame columns checked by the test therefore always satisfy the assertions.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/845", "raw_responses": {"test_case_identical_strings": "```plaintext\n[Explanation]\nThe function logic returns (1.0, 1.0) for the inputs, but the module from which ratio is imported (\"Levenshtein\") is not part of Python's standard library and is commonly not installed in the test environment. This will raise an ImportError at import time and cause the test to fail before execution.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_different_strings": "", "test_case_empty_strings": "```plaintext\n[Explanation]\nFor input ('', ''), cleaning and splitting produce empty token lists, so the term-frequency vectors are empty and norms are zero, resulting in cosine_similarity = 0.0. The Levenshtein ratio for two identical empty strings is 1.0. Therefore the function returns (0.0, 1.0), matching the expected result.\n\n[Result]\nPASS\n```", "test_case_similar_strings": "", "test_case_numerical_strings": ""}}
{"id": "BigCodeBench/945", "raw_responses": {"test_with_default_parameters": "```plaintext\n[Explanation]\nThe function generates sales_data when none is provided (it sets np.random.seed(42) and creates 13 integers), fits a sklearn LinearRegression, and predicts 13 future values. model.predict returns a numpy.ndarray of length 13. The unittest only checks the return type is an ndarray and that its first dimension is 13, so the code will satisfy both assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_with_custom_parameters": "```plaintext\n[Explanation]\nThe function uses the provided sales_data (converted to a numpy array), fits a sklearn LinearRegression on X = 0..9, then predicts for indices 10..19 producing a numpy array of length 10. The test only checks the return type is np.ndarray and shape[0] == 10. There are no errors for the given inputs and parameters, so the assertions will hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_with_random_sales_data": "```plaintext\n[Explanation]\nThe function, when called with periods=5 and no sales_data, generates sales_data (np.array), fits sklearn LinearRegression on indices, and predicts 5 future values, returning a numpy.ndarray of length 5. The test only checks the return type is np.ndarray and its shape[0] == 5, which are satisfied. The internal reseeding does not affect these assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_forecasted_values_increasing": "```plaintext\n[Explanation]\nThe function fits a linear regression to provided sales_data [100,150,200,250,300], producing the exact line y = 100 + 50*x. It then predicts for indices 5..9 giving [350,400,450,500,550], which is strictly increasing. The test asserts that the forecast is not entirely non-decreasing (assertFalse(all(...))), so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_with_specific_sales_data": "```plaintext\n[Explanation]\nThe function receives the provided sales_data (not None), converts it to a numpy array of length 5, fits LinearRegression on indices 0..4 and predicts for indices 5..9, returning a numpy array of length 5. The test only checks that the return is an ndarray and has 5 rows \u2014 both conditions are satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1019", "raw_responses": {"test_successful_ocr_extraction_and_encoding": "```plaintext\n[Explanation]\nThe patched pytesseract.image_to_string returns a non-empty string ('Extracted Text in cp1251'), so the function takes the OCR branch, encodes/decodes using the provided encodings and returns the same string. The Image.open mock configuration for a context manager is irrelevant because the code does not use a with-statement. No exceptions occur, so the returned value matches the expected string.\n\n[Result]\n```plaintext\nPASS\n```", "test_ocr_fails_comment_extraction_succeeds": "```plaintext\n[Explanation]\nThe test patches Image.open to return a context-manager mock whose __enter__ returns the prepared mock image with info.get configured. But the code calls Image.open(...) without using a \"with\" context, so it receives the outer mock (mock_open.return_value) rather than the inner mock (self.mock_image). pytesseract.image_to_string is patched to raise an Exception, triggering the fallback path which again uses Image.open(...) (same outer mock). As a result img.info.get returns a Mock, not the expected bytes, so the code attempts to call .encode/.decode on a Mock and returns a Mock object instead of the expected string. The test compares to a string and will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_ocr_succeeds_encoding_fails": "```plaintext\n[Explanation]\nThe OCR mock returns a non-empty string, so the code attempts to encode it using the provided from_encoding ('invalid_encoding'). text.encode('invalid_encoding') raises a LookupError which the code catches and re-raises as ValueError. The test expects ValueError, so the function behavior matches the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_ocr_and_comment_extraction_fail": "```plaintext\n[Explanation]\nThe test patches pytesseract.image_to_string to raise Exception, so the code goes to the outer except branch and tries Image.open again. The test configures mock_open.return_value.__enter__.return_value = self.mock_image (as if used in a with-statement), but the implementation calls Image.open without using a context manager, so img is mock_open.return_value (not self.mock_image). That mock's img.info.get(...) returns another Mock (truthy) rather than the empty string set on self.mock_image. The code then tries to encode/decode that Mock and returns a Mock object. The test expects an empty string, so the returned Mock does not equal '' and the assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_ocr_extraction_succeeds_no_encoding_needed": "```plaintext\n[Explanation]\nThe patched pytesseract.image_to_string returns a non-empty UTF-8 string. The code takes this path and performs encoding/decoding using from_encoding='utf8' and to_encoding='utf8', which succeeds and returns the same string. The mocked PIL.Image.open return value does not need to supply .info because OCR branch is taken. No exceptions are raised, so the returned value matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/424", "raw_responses": {"test_normal_functionality": "```plaintext\n[Explanation]\nThe function reads the created JPEG, converts BGR->RGB, performs deterministic KMeans (random_state=42) on the 10x10 image and returns the original and segmented numpy arrays. The test creates the same JPEG and checks exact pixel arrays that match what cv2.imread + cvtColor would produce and what KMeans with the fixed seed would yield on those pixels. There are no mismatched filenames or parameter issues, and no file writes expected by the test. Therefore the implementation should produce the expected arrays and pass the assertions.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test expects task_func('non_existent.jpg') to raise FileNotFoundError. The function indeed checks os.path.exists and would raise FileNotFoundError for a missing file. However, the module contains top-level imports of cv2 and sklearn.cluster.KMeans. If those packages are not installed in the test environment, importing the module will raise ImportError before the test can call task_func, causing the test to fail. Because the presence of these non-standard libraries is not guaranteed, the code is not reliably going to pass the test.\n\n[Result]\nFAIL\n```", "test_invalid_n_clusters": "```plaintext\n[Explanation]\nThe test calls task_func('test_image.jpg', -1) expecting a ValueError. The function does raise ValueError for n_clusters <= 0, so the assertion in the test would pass. However, the provided test code's tearDown uses os and os.path without importing the os module, causing a NameError during tearDown (unittest treats this as an ERROR). Because the test suite will error in tearDown, the overall test does not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_n_clusters_as_non_integer": "```plaintext\n[Explanation]\nThe function checks \"if not isinstance(n_clusters, int) or n_clusters <= 0: raise ValueError\" before attempting to read or process the image. Passing the string 'three' causes that condition to be true, so a ValueError is raised as the test expects. The test's teardown uses os, but the submitted code imported os at module level, so os will be available when tests run in the same environment.\n\n[Result]\nPASS\n```", "test_single_cluster_returns_original_image": "```plaintext\n[Explanation]\nThe test expects that using n_clusters=1 returns the original image unchanged. However, the implementation runs KMeans with one cluster, which replaces all pixels with the single cluster centroid (the mean color), so the segmented image will generally differ from the original (the dummy image has distinct colored pixels). Therefore the assertion that original and segmented arrays are equal will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/528", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function reads the CSV (including header which is not duplicated), counts row tuples with Counter, and returns a dict of rows with count>1. The expected Counter in the test compares equal to a dict with the same items (Counter equality with dict succeeds). File extension and content match, and matplotlib/pandas imports won't affect the asserted return value. Therefore the test's assertion will pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test asserts the plot title must be 'Duplicate Entries', but the function sets the title to 'Duplicate Rows Count'. That mismatch causes the assertion to fail. Other aspects (ylabel, Axes type) match.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns an empty dict {} when there are no duplicates; {} == Counter() is True, so the first assertion would pass. However, when there are no duplicates the function still creates and returns a matplotlib Axes object (ax), not None. The test expects ax to be None, so the second assertion fails.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects task_func('sample_data.txt') to raise ValueError, which the code does (it checks the .csv extension). However, after the test the tearDown method calls plt.close('all') but the test module never defines or imports plt (it only imported matplotlib), so tearDown will raise a NameError, causing the test run to error out. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function checks the .csv extension then attempts to open the file with open(file_path, 'r'). If the file does not exist, open raises FileNotFoundError which is not caught in task_func, so the exception propagates as expected by the test's assertRaises. The test's tearDown calls plt.close('all'); plt is available because the tested code imported matplotlib.pyplot as plt at module level. Therefore the single test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/147", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe test patches socket.socket to return a MagicMock. The code calls socket.socket(), then methods like settimeout, connect_ex and close on that mock; MagicMock provides these attributes by default, so no AttributeError occurs. connect_ex will return a MagicMock (not equal to 0), producing boolean False entries, but the function still returns a dict. The test only asserts the return type is dict, so it will pass.\n\n[Result]\nPASS\n```", "test_open_port": "```plaintext\n[Explanation]\nThe test patches socket.socket so the code uses a MagicMock for each socket. The function calls sock.connect_ex which returns a MagicMock; the code compares that result to 0, producing a MagicMock (truthy) rather than a bool. any(result.values()) will therefore see truthy values and the assertion that at least one port is open will pass. Threads and IPv4Network.hosts() produce two IPs for /30, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_closed_port": "```plaintext\n[Explanation]\nThe test patches socket.socket and sets connect.side_effect to raise socket.error, but the code calls sock.connect_ex, not sock.connect. The mocked socket's connect_ex returns a MagicMock (no side effect), so (result == 0) yields a MagicMock object (truthy). Thus result.values() contain truthy MagicMocks, any(...) is True, so the assertion expecting all False fails.\n\n[Result]\nFAIL\n```", "test_all_ips_checked": "```plaintext\n[Explanation]\nThe function iterates over network.hosts(), which yields only usable host addresses (excludes network and broadcast). The test's expected_ips is built from IPv4Network(ip_range), which includes all addresses (network, hosts, broadcast). For '192.168.0.0/30' the sets differ (IPv4Network has 4 addresses; hosts() yields 2), so the key sets won't match and the assertion will fail.\n\n[Result]\nFAIL\n```", "test_return_value_structure": "```plaintext\n[Explanation]\nThe test patches socket.socket, so sock.connect_ex is a MagicMock whose call returns a MagicMock. The code stores (result == 0) directly; MagicMock.__eq__ returns another MagicMock, not a bool. Thus the returned dict values are MagicMock instances, causing isinstance(status, bool) assertions to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/82", "raw_responses": {"test_app": "```plaintext\n[Explanation]\nThe test's assertion uses \"Flask\" (self.assertIsInstance(app, Flask, ...)) but the test file never imports Flask into its namespace. This will raise a NameError (or cause the test to error/fail) when running the test. Therefore the provided code cannot pass the test as written.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_protected_route_access": "```plaintext\n[Explanation]\nThe test checks that accessing /protected when not authenticated does not display \"Logged in as:\". In this app, /protected is protected by @login_required, so an unauthenticated request will not render the protected page. Either Flask-Login returns a 401 or redirects to the login view; in either case the response content will not contain \"Logged in as:\". The test also provides a login.html template in the created templates directory, so rendering the login page (if redirected) works. Therefore the assertion in the test will succeed.\n\n[Result]\nPASS\n```", "test_secret_key": "```plaintext\n[Explanation]\nThe test only checks that task_func sets app.config['SECRET_KEY'] to the provided secret_key. task_func assigns app.config['SECRET_KEY'] = secret_key and returns the Flask app. No code paths in task_func raise during this assignment, so the assertion will be true and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_login_page_accessibility": "```plaintext\n[Explanation]\nThe test checks GET /login returns 200 and the test creates a templates/login.html in the working directory's templates folder. The app sets app.config['SECRET_KEY'] (needed by Flask-WTF) and renders 'login.html'. Although the code stores the passed template_folder into config under 'TEMPLATES_FOLDER' (not used by Flask), Flask's default template search (the \"templates\" directory under the app root) matches the folder created by the test, so render_template will find the file and return 200. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_login_manager_initialization": "", "test_logout_route_redirects_to_login": "```plaintext\n[Explanation]\nThe test accesses /protected and expects content \"Logged in as: testuser\", but the app's protected() returns render_template('protected.html') while the test only creates login.html (no protected.html or home.html). This will raise a TemplateNotFound / 500 error (or at least not contain the expected text). Also the test expects logout to redirect to a login page, but the app redirects to home(), which would require home.html (also not provided). Therefore the provided code will not satisfy the test assertions.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/636", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nCalling task_func(0) creates an empty DataFrame with five columns. non_zero_counts is a Series of five zeros (one per column), so ax.bar is called with five bars of height 0, which still creates five Rectangle patches. The test expects zero patches, so the second assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nFor rows=1 the DataFrame has length 1. The code always creates 5 bars (one per column); matplotlib adds one Rectangle patch per bar even if the height is zero. No other patches are added, so len(ax.patches) == 5 holds. Therefore both assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function creates a DataFrame with the requested number of rows (rows=10) and five columns, so len(df) == 10. It then creates a bar for each of the five columns, so the Axes contains five bar patches. Therefore both assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nCalling task_func(-5) leads to np.random.randint(..., size=( -5, 5 )) which raises a ValueError for negative dimensions. The function will error before creating a DataFrame or plot, so it cannot satisfy the test's expectations (an empty DataFrame and zero patches).\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a DataFrame with shape (rows, 5) using rows=1000, so len(df) == 1000. ax.bar(...) adds five Rectangle patches (one per column) to ax, and the annotations are Text objects (not patches), so len(ax.patches) == 5. Therefore the test assertions are satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/854", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nFor input [1, 2] the function validates the list, generates permutations [(1, 2), (2, 1)], and computes factorial sums [1+2, 2+1] => [3, 3]. The test checks result == [3,3] and uses assertCountEqual for permutations (order-insensitive), so the returned permutations match. No errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function accepts a list of non-negative integers. For input [1,2,3] it generates the six permutations in the expected order and computes the sum of factorials for each permutation: 1!+2!+3! = 1+2+6 = 9, producing [9,9,9,9,9,9]. The returned permutations are tuples matching the expected set. The test compares factorial sums for equality and uses assertCountEqual for permutations (order-insensitive), so the code will pass this test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nFor input [1], permutations is [(1,)]. The code computes factorials [1] and uses reduce on a single-element list which returns 1, yielding factorial_sums [1]. It returns ([1], [(1,)]) which matches expected result and permutation check.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function checks for an empty list and returns ([], []). The test calls task_func([]) and expects result == [] and perm == [] (verified via assertCountEqual), so both assertions succeed.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nAll test calls provide invalid inputs. The function raises TypeError for non-list inputs and for non-integer elements, and ValueError for negative integers. These are subclasses of Exception, so each assertRaises(Exception, ...) will succeed.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/952", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns a DataFrame with the required columns and number of rows. Task names already contain underscores, and the generator check passes. The test's use of datetime in setUp is fine because the module imported datetime at top-level. Although the function uses a datetime.date object for 'Due Date', pandas converts Python date objects to datetime64[ns], so comparing the Series to the YYYY-MM-DD string succeeds. Therefore the test assertions will all pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function returns a pandas DataFrame with columns 'Task Name', 'Assigned To', and 'Due Date' and length equal to n_tasks. For the test inputs (tasks list and n_tasks=2), no exceptions are raised, random.seed is set, and random.choices produces two selected tasks and two employees. The DataFrame produced will be an instance of pd.DataFrame, have the expected column set, and have length 2, satisfying all assertions in the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns a pandas DataFrame with the columns 'Task Name', 'Assigned To', and 'Due Date', uses random.choices so it can produce more tasks than provided (with replacement), and sets a deterministic seed. For the tested n_tasks values (2, 10, 20, 100) it produces a DataFrame instance with the correct columns and length. The test does not assert due date types or specific values, so the datetime.date entries are acceptable. No errors are expected.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function immediately raises ValueError when n_tasks < 0. The test calls task_func(..., -1, ... ) inside assertRaises(ValueError), so the expected exception will be raised and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nCalling task_func with n_tasks=0 returns a DataFrame constructed from empty lists for tasks, employees, and due dates. The function handles k=0 correctly (random.choices with k=0 yields empty lists), sets the expected three columns, and returns length 0. Therefore the assertions in the test (is instance of DataFrame, correct columns set, and zero length) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe test calls task_func([], 2, seed=123). The function sanitizes tasks to an empty list, then calls random.choices(sanitized_tasks, k=2). random.choices raises an IndexError (\"Cannot choose from an empty sequence\") when the population is empty, so the function will raise an exception instead of returning an empty DataFrame. Therefore the test (which expects a DataFrame of length 0) will fail.\n\n[Result]\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function uses random.choices with the provided employees argument (custom_employees), and the seed is set, so every entry in df['Assigned To'] will be one of ['Alice','Bob','Charlie']. The test only checks that all assigned employees are in that list, which will be True. No other issues affect this specific test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_8": "```plaintext\n[Explanation]\nThe function seeds the global random generator when seed is provided, so calling task_func(..., seed=0) twice produces identical random.choices results and thus identical DataFrames (pd.testing.assert_frame_equal will pass). Using a different seed (100) produces a different assignment, so df1.equals(df3) will be False. The necessary imports (including datetime) are present in the provided code block, so the test's setUp can access datetime. Therefore the test assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function replaces spaces with underscores, and with seed=42 random.choices will pick the two tasks in some order (first 'Task_Two', then 'Task_One'), so the DataFrame's 'Task Name' column contains both 'Task_One' and 'Task_Two'. The test's set comparison therefore succeeds.\n\n[Result]\nPASS\n```", "test_case_10": "```plaintext\n[Explanation]\nThe function sanitizes task names (no change for 'Task'), seeds the RNG, and selects 2 tasks from ['Task','Task'] with replacement, producing ['Task','Task']. The DataFrame length is 2 and set(df['Task Name']) == {'Task'}, so both assertions succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/1020", "raw_responses": {"test_get_data_with_default_parameters": "```plaintext\n[Explanation]\nThe mocked requests.get returns bytes encoded as cp1251 and chardet.detect is mocked to return {'encoding': 'cp1251'}. The function decodes using cp1251, re-encodes/decodes to utf8, and json.loads successfully returns {'key': 'value'}. The mock.patch argument order in the test matches the function parameters. No errors expected.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_get_data_with_custom_url_and_encodings": "```plaintext\n[Explanation]\nThe test patches requests.get to return .content containing the bytes b'{\"message\": \"success\"}' encoded in latin1. task_func is called with from_encoding='latin1' and to_encoding='utf8', so the function decodes the bytes using latin1, re-encodes to utf8, decodes back to a string, and json.loads parses it to {'message': 'success'}. No errors are raised, so the result matches the expected output.\n\n[Result]\nPASS\n```", "test_get_data_with_empty_response": "```plaintext\n[Explanation]\nThe test patches requests.get so task_func receives a response whose .content is b''. The function returns {} when content is empty, matching the test's expected_output.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_get_data_with_invalid_json": "```plaintext\n[Explanation]\nThe test expects task_func() to raise json.JSONDecodeError for invalid JSON bytes. The implementation calls json.loads inside a try/except that catches json.JSONDecodeError and returns an empty dict instead. Therefore no JSONDecodeError will be propagated and the test's assertRaises will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_get_data_with_different_valid_encoding": "```plaintext\n[Explanation]\nThe test provides UTF-8 encoded JSON bytes and calls task_func(from_encoding='utf8', to_encoding='utf8'). The code uses the given from_encoding path to decode the bytes to a string, re-encodes/decodes to the target encoding, and json.loads parses the JSON successfully. The mocked requests.get returns the .content bytes as expected, so the result matches the expected dictionary.\n\n[Result]\nPASS\n```", "test_get_data_with_undetectable_encoding": "```plaintext\n[Explanation]\nThe test patches requests.get to return non-empty bytes and chardet.detect to return {'encoding': None}. The function checks detected['encoding'] and raises ValueError(\"Unable to detect encoding for non-empty content\") when encoding is falsy. This matches the test's expected exception and message, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/161", "raw_responses": {"test_log_to_csv_content": "```plaintext\n[Explanation]\nThe function writes a CSV with columns named 'type', 'timestamp', 'message' (lowercase) while the test builds expected_df with columns 'Type', 'Timestamp', 'Message' (capitalized). pandas.assert_frame_equal will fail due to the column name mismatch (even though the row values match). Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_no_log_entries": "```plaintext\n[Explanation]\nThe test patches builtins.open to return an empty file. task_func reads lines, finds no valid entries, and raises ValueError(\"No valid log entries found\"), which matches the test's assertRaises expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_incorrect_format_log": "```plaintext\n[Explanation]\nThe test patches builtins.open to return a single line that does not match the regex. task_func reads the line, finds no valid entries and raises ValueError(\"No valid log entries found\"). The test expects a ValueError, so it will pass. (Note: pandas is imported at module load but isn't used before the exception; assuming pandas is available in the environment.)\n\n[Result]\nPASS\n```", "test_partial_correct_log": "```plaintext\n[Explanation]\nThe function reads the provided log file, uses the regex to match lines like \"TYPE: [YYYY-MM-DD HH:MM:SS] - MESSAGE\", validates the timestamp, and appends valid entries to a DataFrame which is saved to 'log_data.csv'. The test's file contains one valid log line and one invalid line; the code will parse the single valid entry, create the CSV, return 'log_data.csv', and pd.read_csv on that file will have one row. No errors are raised, so the test assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_malformed_timestamp": "```plaintext\n[Explanation]\nThe test patches builtins.open to return a log line with timestamp month \"00\". The regex matches and datetime.strptime('%Y-%m-%d %H:%M:%S') raises ValueError for the invalid month, which the function re-raises. The test expects a ValueError, so it will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/37", "raw_responses": {"test_case_1": "", "test_case_2": "```plaintext\n[Explanation]\nThe function trains a RandomForestClassifier, computes feature importances, sorts them descending, and plots a horizontal barplot with seaborn using the sorted DataFrame. The plot labels and title match the test's expectations. The test compares the descending-sorted feature_importances_ to the bar widths; because the DataFrame is sorted descending before plotting, the bars' widths appear in the same descending order as the sorted importances. The returned model and ax types and labels/title also match. Therefore the code should satisfy the provided test.\n\n[Result]\nPASS\n```", "test_case_3": "", "test_case_4": "```plaintext\n[Explanation]\nThe test dataframe has only one feature ('N'), so the trained RandomForestClassifier will assign importance 1.0 to that single feature. The function returns a RandomForestClassifier and a seaborn/matplotlib Axes; it sets the title, xlabel, and ylabel exactly as the test expects. The plotted bar's width will equal the single feature importance (1.0), so the list comparison of importances vs. bar widths will match. Therefore the code will pass this test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": ""}}
{"id": "BigCodeBench/308", "raw_responses": {"test_dataframe_structure": "```plaintext\n[Explanation]\nThe function called with no additional fields produces columns FIELDS in order plus 'Average Grade'. The data dict insertion order (Python 3.7+) and explicit addition of 'Average Grade' ensure the DataFrame columns equal ['Physics','Math','Chemistry','Biology','English','History','Average Grade']. The test only checks column names and pandas/statistics behavior used here is compatible, so the test will pass.\n\n[Result]\nPASS\n```", "test_additional_fields": "```plaintext\n[Explanation]\nThe function builds a DataFrame with the original FIELDS plus the provided additional fields and an \"Average Grade\" column. With random.seed(0) set in the test, the generated grades are deterministic and all subject values (student rows and the appended average row) stay within 0\u2013100. Column ordering matches FIELDS + additional_fields + ['Average Grade'], so the expected columns and the between(0,100) checks pass.\n\n[Result]\nPASS\n```", "test_grades_range": "```plaintext\n[Explanation]\nThe function uses random.randint(0, 100) to generate all student grades (inclusive bounds), and the test seeds the same random module before calling task_func(), so all generated student grades are within 0\u2013100. The added \"Average\" row for each subject is the mean of values in that range, hence also within 0\u2013100. The test excludes the 'Average Grade' column, so all checked columns pass the between(0, 100) assertion.\n\n[Result]\nPASS\n```", "test_average_grade": "```plaintext\n[Explanation]\nThe function computes each student's average using statistics.mean on the same per-student grades that end up as the DataFrame's subject columns. The test seeds random, calls task_func(), and compares the stored 'Average Grade' to the pandas mean of the student's subject values (row[:-1].mean()). Both calculations operate on identical numbers and will match within floating-point tolerance. No off-by-one or type issues affect the student rows, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_subject_average": "```plaintext\n[Explanation]\nThe function computes subject means over the student rows and then appends that mean as the 'Average' row. The test compares the 'Average' row values to df[column].mean(). Although df[column].mean() is computed after the average row is appended, algebra shows including the mean value doesn't change the mean: (sum + mean)/(N+1) == mean. The calculations use the same numeric data and floating differences (if any) are within assertAlmostEqual tolerance. The code runs with the given random seed. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_non_negative_grades": "```plaintext\n[Explanation]\nThe function generates grades with random.randint(0, 100) (no negatives) and computes averages (means) of non-negative numbers, so all DataFrame entries are >= 0. The added average row/column are means of non-negative values and remain non-negative. Therefore the assertion (df >= 0).all().all() will be True.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/443", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nWith P being the identity matrix the einsum returns T unchanged, so the 27 samples passed to KMeans are only 0s and 1s (T contains only 0/1). There are only 2 distinct data values but the code requests n_clusters=3. KMeans cannot produce 3 distinct clusters from only two unique points and scikit-learn will fail (raise an error or produce fewer than 3 unique labels), so the assertion expecting 3 unique cluster labels will not hold.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function flattens the (4,3,3) result into 36 one-dimensional samples and returns 36 labels. The test constructs 12 samples of dimensionality 3 (shape (12,)) via tensordot+swapaxes+reshape and expects 12 labels (P.shape[0]*n_clusters = 4*3 = 12). The shapes mismatch (36 vs 12), so the equality assertion will fail (and the shape assertion will also fail).\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function computes result shape (4,3,3) -> flattened length 36 and plots 36 points. The test expects 4*3 = 12 plotted points. Title and Axes type match, but the number of plotted points (36) does not equal expected 12, so the test fails.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function computes result = np.einsum('ij,jkl->ikl') giving shape (4,3,3) -> 36 flattened samples. KMeans is run on these 36 one-dimensional samples with n_clusters in [1,3,5]. Given the continuous random T and different P rows, the flattened values will be sufficiently distinct so KMeans will produce exactly n_clusters labels (and n_clusters <= n_samples). There are no obvious errors or exceptions in the code for this test (n_init and random_state provided), so the assertions checking the number of unique clusters will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test expects an exception for n_clusters values -1, 0, and 'invalid'. The function builds data and then constructs/uses sklearn.cluster.KMeans with the given n_clusters and calls fit_predict. Scikit-learn validates n_clusters (either in the constructor or during fit) and will raise an exception (ValueError/TypeError) for negative, zero, or non-integer string values. Therefore task_func will raise exceptions for all three invalid inputs, satisfying the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function uses np.einsum('ij,jkl->ikl', P, T) without shape checks. For the first call P with shape (2,2) and T (3,3,3) the contracted dimension j mismatches (2 vs 3) and einsum raises ValueError. For the second call P (4,3) and T (2,2) the second operand lacks the required 3 dimensions for 'jkl', causing einsum to raise ValueError as well. Both asserted exceptions match, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nPassing random_state='invalid' into KMeans will cause scikit-learn's check_random_state to reject the string and raise a ValueError during fit_predict. The function calls fit_predict, so the ValueError will be raised as the test expects.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/637", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function appends a 'Student_ID' column to the DataFrame after creating the five course columns, so the returned DataFrame has 6 columns not 5. The test expects shape (10, 5), so the first assertion will fail (the plot title assertion would pass).\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function adds a 'Student_ID' column to the DataFrame after creating five course columns, so for num_students=50 the DataFrame shape is (50, 6). The test expects (50, 5), causing the first assertion to fail. The plot title matches the expected string, but that does not prevent the overall test failure.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe DataFrame creation adds five course columns then appends a 'Student_ID' column, so df.shape is (100, 6). The test expects (100, 5). Although the plot title matches, the mismatched DataFrame shape causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func(1). The function creates a DataFrame with 5 course columns and then adds a 'Student_ID' column, resulting in shape (1, 6). The test expects shape (1, 5), so the first assertion will fail. (The plot title check would pass, but the shape mismatch causes the test to fail.)\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a DataFrame with 5 course columns and then adds a 'Student_ID' column, so for num_students=5 the DataFrame shape is (5, 6). The test expects shape (5, 5), so the first assertion will fail. The plot title matches, but that does not matter because the shape assertion fails.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/530", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nFor the provided df_valid input, the function correctly detects duplicate name \"Alice\", floors ages (keeps 25), builds Counter({25: 2}), creates a matplotlib Axes with title \"Age Distribution for Duplicate Names\" and x/y labels \"Age\"/\"Count\". These match the test assertions, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function explicitly checks for negative ages with \"if (df['age'] < 0).any(): raise ValueError(...)\". The provided test DataFrame contains an age of -1, so the function will raise ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe input DataFrame has no repeated names, so name_counts > 1 yields no duplicate_names. The function checks this and returns (Counter(), None). All preliminary checks (empty, columns, negative ages) pass for the provided df_no_duplicates. Therefore the returned values match the test's expected Counter() and None.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function correctly processes the DataFrame of three identical \"Alice\" rows with age 25: it does not raise errors, rounds ages (no change), identifies \"Alice\" as a duplicate, builds Counter({25: 3}), and returns a matplotlib Axes with the title and x/y labels set as expected. The histogram bins calculation works for identical ages. Thus the test_case_4 assertions will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function identifies duplicate names (Alice and Bob) in df_mixed and collects their ages [25, 26, 25, 27], producing Counter({25: 2, 26: 1, 27: 1}) which matches expected. It also returns a matplotlib Axes with title and labels set to 'Age' and 'Count', satisfying the plot checks. No errors are raised for this input.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function floors float ages to integers, finds duplicate names (Alice and Bob), and builds the age list [25, 26, 25, 27], producing Counter({25: 2, 26: 1, 27: 1}) which matches expected. It also returns a matplotlib Axes with title, xlabel 'Age' and ylabel 'Count'. No checks raise errors for this input.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function checks if df.empty and raises ValueError(\"DataFrame cannot be empty\") as its first operation. The test provides an empty DataFrame (columns 'name' and 'age' with no rows), so df.empty is True and the ValueError will be raised, satisfying the test.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/857", "raw_responses": {"test_successful_transfer": "```plaintext\n[Explanation]\nThe test patches glob.glob to return ['file1.txt','file2.csv'] for any pattern and patches shutil.move (unused). The implemented function uses shutil.copy2 and iterates over each extension; because glob.glob returns the same two files for both '.txt' and '.csv', the function copies both files twice and appends their names twice. The returned list becomes ['file1.txt','file2.csv','file1.txt','file2.csv'], which does not match the expected ['file1.txt','file2.csv']. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_source_directory": "```plaintext\n[Explanation]\nThe test patches glob.glob to return an empty list, and task_func calls glob.glob for each extension, so no files are found and the loop does nothing. Although the test also patches shutil.move, task_func uses shutil.copy2; that doesn't matter because no copy is attempted when glob returns []. The function therefore returns an empty list as expected.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_file_extensions": "```plaintext\n[Explanation]\nThe test patches glob.glob to return an empty list, so task_func iterates extensions but finds no files to copy. shutil.copy2 is not called, transferred_files remains [], and the returned value matches the expected []. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches glob.glob and shutil.move and expects a failure when move raises an exception and a warning containing \"Unable to move file\". The implementation uses glob.glob (so the mocked glob applies) but uses shutil.copy2 to copy files, not shutil.move. Because copy2 is not mocked, it will succeed copying the real file created in setUp, so transferred_files will contain the filename and no warning with the expected text will be issued. Therefore the test's assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_all_extensions": "```plaintext\n[Explanation]\nThe test patches glob.glob to always return the same list of four files for every extension. task_func iterates over each extension and calls glob.glob each time, so it will append the same four filenames once per extension (resulting in duplicates), whereas the test expects each filename exactly once. Also, the test patches shutil.move but the function uses shutil.copy2 (irrelevant to the failure). Therefore the returned list will not match the expected list.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/955", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test will fail. The function lowercases the text and uses re.findall(r'[a-zA-Z0-9_]+'), which strips punctuation \u2014 so the x-tick label is 'world' not 'world!'. Also, the code appends the modified_strings (['hello']) to the extracted words, causing 'hello' to be counted twice. As a result the first bar's height is 2, but the test expects 1.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test expects a single x-tick label 'hello_world!' and a bar height of 1. The function lowercases the text and uses a regex that strips punctuation, producing words ['hello', 'world'] and separately adds modified_strings ['hello_world']. The x-tick labels become ['hello', 'world', 'hello_world'], so get_xticklabels()[0].get_text() returns 'hello', not 'hello_world!'. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function lowercases the text and strips punctuation via the regex, so x-tick labels become 'hello' and 'world' (no exclamation, lowercase). The test expects 'Hello' and 'world!' (original case and punctuation), so the label assertions will fail. (The axis type and bar heights would pass, but the label checks cause failure.)\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a matplotlib Axes from plt.subplots and sets x-tick labels to include the modified string 'Lorem_ipsum'. For the input, modified_strings becomes ['Lorem_ipsum'] and it's appended to the words list, so 'Lorem_ipsum' appears among the xtick labels. The Axes instance is returned. Therefore the assertions in the test (instance type and presence of 'Lorem_ipsum') will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function produces x-axis labels ['hello', 'world', 'hello_world'] (no exclamation mark). The test expects one of the tick labels to be 'hello_world!' (with '!'), which is not present. The bar height check would pass, and the Axes instance check should pass, but the missing punctuation in the ticklabels causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function lowercases the text but does not lowercase or add punctuation to the modified mystrings. For the input, modified_strings becomes ['Hello_World'] (no '!'), and the x-tick labels become ['hello', 'world', 'Hello_World']. The test expects 'Hello_World!' among the x-tick labels, which is not present, so the first assertion fails (the second assertion about height would pass).\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function appends the modified string 'not_in_text' to the word list unconditionally (all_words = words + modified_strings), so it becomes part of unique_words and actual_words and is used as an x-tick label. For the input text 'Hello world!' that means 'not_in_text' will appear in the plot labels, causing the test's assertNotIn check to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function checks \"if not text: raise ValueError(...)\" and since text is an empty string (''), it raises a ValueError, which is a subclass of Exception. The test expects any Exception, so the raised ValueError satisfies the assertion.\n\n[Result]\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function lowercases the input text and finds words ['test','123','is','fun']. It appends modified_strings ['test_123', '#$%!'] to the word list, so the x-axis tick labels become ['test','123','is','fun','test_123','#$%!']. Both 'test_123' and '#$%!' therefore appear in ax.get_xticklabels(), so the test assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_10": "```plaintext\n[Explanation]\nThe function counts both occurrences from the text (3) and the two entries in mystrings (2), yielding a bar height of 5. The test expects height 3. The x-label check would pass, but the frequency assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/1022", "raw_responses": {"test_valid_input": "```plaintext\n[Explanation]\nThe function filters rows where the date column equals today's date (df[column].dt.date == today). The test CSV contains only future dates (today+1, +2, +3), so the returned DataFrame will be empty. The test asserts the DataFrame is not empty, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func('non_existing.csv', 'Date') and expects FileNotFoundError. The function immediately checks os.path.exists and raises FileNotFoundError when the file is missing. Therefore the behavior matches the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_column_not_found": "```plaintext\n[Explanation]\nThe test writes a CSV whose header is \"NotDate\" (with leading spaces) so there is no \"Date\" column. task_func checks for the column and raises ValueError. The test expects a ValueError via assertRaises, so the code will pass this test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_file": "```plaintext\n[Explanation]\nThe test creates an empty CSV file and calls task_func. The function checks existence, attempts pd.read_csv which raises pandas.errors.EmptyDataError for an empty file, and the except clause returns an empty DataFrame. The test asserts df.empty which will be True. No other errors occur.\n\n[Result]\nPASS\n```", "test_no_future_dates": "```plaintext\n[Explanation]\nThe function reads the CSV, converts the 'Date' column to datetimes, filters rows equal to today's date, and returns the filtered (and sorted) DataFrame. The test creates a CSV with 2020 dates (not equal to today), so the function will return an empty DataFrame and the assertion df.empty will be True. No errors are raised.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/162", "raw_responses": {"test_histogram_content": "```plaintext\n[Explanation]\nThe test calls task_func(text) to get an Axes, but then calls ax.hist(...) itself with bins=np.arange(max(word_lengths) + 2) - 0.5 and asserts the returned bins equal that expected array. ax.hist returns the bins array supplied, so the assertion succeeds. The regex used in task_func still produces the same word lengths for the provided text, and no errors occur; thus the test will pass.\n\n[Result]\nPASS\n```", "test_empty_text": "```plaintext\n[Explanation]\nThe function returns an Axes object even for empty input (it creates a subplot and sets limits), so calling ax.hist([], bins=[]) in the test will execute on that Axes. Matplotlib's hist with empty data and empty bins yields no patches, satisfying the assertion that len(patches) == 0. No exceptions are raised by task_func, and the test's setup/teardown don't interfere with the returned Axes.\n\n[Result]\n```plaintext\nPASS\n```", "test_single_word": "```plaintext\n[Explanation]\nThe function extracts the word \"Hello\" (length 5) and plots a histogram on a new Axes. The test then calls ax.hist([5], bins=[4.5, 5.5]) on that returned Axes; this call returns counts and patches for the provided data and bins independent of the previously drawn histogram. For the bin [4.5, 5.5], the count n[0] will be 1 and one patch will be created. No other errors occur for this input.\n\n[Result]\n```plaintext\nPASS\n```", "test_histogram_bin_counts": "```plaintext\n[Explanation]\nThe function extracts alphabetic words from the input and computes lengths, producing the exact list [3,3,5,4,4,3,5,5,4,3]. task_func returns the Axes object, and the test then calls ax.hist with bins [2.5,3.5,4.5,5.5], which yields three bins with counts [4, 3, 3] as asserted. No behavior in task_func interferes with that subsequent hist call, so the test conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_rwidth_parameter_effect": "```plaintext\n[Explanation]\nThe function calls ax.hist(...) exactly once when words are present. With the test input the regex finds words, so hist is called. The test patches plt.Axes.hist, captures the call, and checks the 'rwidth' kwarg; the function passes rwidth through unchanged (0.5). Therefore the assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/89", "raw_responses": {"test_original_data_unchanged": "```plaintext\n[Explanation]\nThe function makes a copy of the input array at the start (original_data = data.copy()) and never modifies the original 'data' in-place. All subsequent operations create new arrays (e.g. np.delete, stats.zscore) or use the copy for plotting. Therefore the returned original_data has the same contents as the input self.data and the test assertion np.testing.assert_array_equal(self.data, original_data) will pass.\n\n[Result]\nPASS\n```", "test_data_without_outliers": "```plaintext\n[Explanation]\nThe module imports \"from sklearn.preprocessing import StandardScaler\" (and also matplotlib/scipy). If sklearn is not installed in the test environment, the import will raise ImportError at import time and the test will not run. Because the unused sklearn import is present, the most likely outcome is a failure before the test's assertion executes. (If all imports were available, the function would remove no rows for the provided data and the length check would pass.)\n\n[Result]\nFAIL\n```", "test_return_type": "```plaintext\n[Explanation]\nThe function returns (original_data, filtered_data, tuple(outlier_indices)). For the provided input it executes the z-score calculation and plotting without raising exceptions, and the types match: original and filtered are numpy arrays and outlier indices are returned as a tuple. The test only asserts these types. There are extra imports and a plt.show() call, but in typical test environments required packages are available and plotting does not block, so no error is expected.\n\n[Result]\n```plaintext\nPASS\n```", "test_no_plotting": "```plaintext\n[Explanation]\nThe function computes Z-scores, removes the outlier row, and calls plt.show(). The test patches matplotlib.pyplot.show and only asserts that show() was called. The code will execute without errors for the provided data, so the mocked show() will be invoked and the assertion passes.\n\n[Result]\nPASS\n```", "test_no_change_in_data_dimension": "```plaintext\n[Explanation]\nThe function removes rows using np.delete(..., axis=0), so the number of columns is unchanged. With the provided data (4x2), the filtered_data will still have 2 columns. No runtime errors occur for this input (z-score computation and plotting are valid), so the test asserting equal column counts will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_plot_titles": "```plaintext\n[Explanation]\nThe function creates a matplotlib figure with two subplots and sets their titles to 'Data with Outliers' and 'Data without Outliers'. The test patches plt.show so the figure remains available, then retrieves plt.gcf() and checks the axes' titles. Given the function sets those exact titles, the assertion will succeed (no other behavior in the test depends on the outlier detection). Therefore the code will pass this test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/310", "raw_responses": {"test_file_creation": "```plaintext\n[Explanation]\nThe function writes a CSV file named by the test, returns its absolute path, and the file is created using open(..., 'w'), so os.path.exists(path) will be True. The required modules (random, os, csv, statistics.mean) are imported in the provided code, so random.seed(0) in the test will work when the test and code run in the same execution context. The tearDown removes the created file. Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_file_content_rows": "```plaintext\n[Explanation]\nThe function writes a CSV with a header (1 row), 100 people (PEOPLE_COUNT) rows, and a final averages row \u2014 total 1 + 100 + 1 = 102 rows. The test seeds random and calls task_func, then reads the CSV and asserts len(rows) == 102. The code creates the file with the expected number of rows and returns the path; tearDown removes the created file. Although the test code calls random.seed(0) without importing random, the solution module already imports random, so the name is available in the execution environment used by the tests. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_averages_calculation": "```plaintext\n[Explanation]\nThe test seeds the same random generator, calls task_func to write 100 rows plus an averages row, then reads back the per-person numeric columns and recomputes means to compare with the stored averages. The code uses the same random module, writes header/person rows/averages, and uses statistics.mean. Numeric values are small integers so sums/divisions are exact in floating-point, and CSV read/write preserves the numeric strings so float conversions match exactly. The teardown removes the generated file. There are no mismatches or errors, so the test should pass.\n\n[Result]\nPASS\n```", "test_header": "```plaintext\n[Explanation]\nThe test calls random.seed(0) but the test file does not import the random module, so a NameError will occur before task_func is even invoked. Although task_func writes the correct header, the missing import in the test causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_average_row_label": "```plaintext\n[Explanation]\nThe function writes a final CSV row starting with the string 'Average'. The test seeds the random generator (available because the module imported random), calls task_func to create 'test_average_row_label.csv', then reads the file and asserts rows[-1][0] == 'Average'. The code indeed writes ['Average', avg_age, ...] as the last row and the file is created in the current directory, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/445", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates jittered numpy points, builds a scipy.spatial.Voronoi, and plots using matplotlib, returning (vor, ax). The Voronoi object and point_region length match the input, and ax is a matplotlib Axes with children after plotting. The module-level import \"import matplotlib.pyplot as plt\" makes plt available to the test. No operations here should raise errors in the test environment.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function resets numpy's RNG with np.random.seed(seed) and applies a tiny uniform jitter to the input points before building the Voronoi diagram. Calling task_func with the same seed (0) twice produces identical jitter and thus identical vor.ridge_points; using a different seed (1) produces different jitter and different ridge_points. The returned Voronoi object and Axes are provided as expected, so the test assertions will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function sets a fixed random seed and applies small jitter (1e-10) to avoid degeneracy for nearly coincident points (the test uses points with separation 1e-12). That makes the points non-degenerate for Qhull, so Voronoi(jittered_points) should succeed. The function creates a matplotlib Axes and plots into it, so ax is an Axes instance and has children (plot elements). vor.point_region has length equal to the number of input points. Therefore the assertions in the test are satisfied.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func with only two 2D points and expects an exception. SciPy's Voronoi (via Qhull) requires at least three non-collinear points for 2D and raises an error for only two points. task_func does not catch this error (the call to Voronoi(...) will raise), so the exception propagates and satisfies the test's assertRaises.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe test passes a 1D array (shape (3,)) to task_func. The function applies jitter (preserving the 1D shape) and then calls scipy.spatial.Voronoi on that array. Voronoi requires a 2D array of point coordinates (shape (n_points, ndim)) and will raise an exception when given a 1D input. The test expects an exception, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe test expects an exception when calling task_func('Not valid points'). In the function, jittered_points = points + np.random.uniform(..., points.shape) will attempt to access points.shape on a Python string, raising an AttributeError (a subclass of Exception). This satisfies the with self.assertRaises(Exception) check, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/532", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns Counter(values) which for df = [1,2,3,4,5] yields Counter({1:1,2:1,3:1,4:1,5:1}). The test expects an empty Counter(). The plot checks would pass, but the counter assertion fails.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function returns a Counter of the Series values and an Axes. For the input with all values equal, Counter will be Counter({1: 5}). The plotting code runs without raising an exception: norm.fit yields sigma=0 which produces warnings/NaNs but not an exception, and scaling by bin_width=0 results in a plotted curve of NaNs (harmless). The returned ax has title and axis labels set as required. Thus the test assertions will be satisfied.\n\n[Result]\nPASS\n```", "test_case_3": "", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a Counter of all values, including those that occur only once (e.g., 4:1). The test expects a Counter containing only the duplicated values (5,1,2,3) and omitting the unique value 4. Therefore the returned Counter will not equal the expected one and the test will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a histogram with the specified number of bins and returns the Axes object. For the provided DataFrame of 100 random values, ax.hist will produce one Rectangle patch per bin, so len(ax.patches) equals the bins argument (2, 10, 20). The code handles the non-empty DataFrame path, scales/plots the normal curve as a Line2D (not affecting patches), and returns the Axes. The test's plt references come from the module-level import matplotlib.pyplot as plt in the code, so plt is available. Assuming scipy is available for norm (as imported at top), the test assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_6": ""}}
{"id": "BigCodeBench/654", "raw_responses": {"test_return_types": "", "test_target_value_found": "```plaintext\n[Explanation]\nThe test provides a numpy array of strings. task_func filters rows correctly but x_values and y_values remain string dtype (e.g. '332', '1', '33'). optimize.curve_fit/exp_decay perform numeric operations (np.exp, float * x), which will raise a TypeError when applied to string arrays. Additionally, the x values are identical ('332') making the fit ill-posed. As a result the function will raise an exception and won't return coefficients, so the test will fail.\n\n[Result]\nFAIL\n```", "test_target_value_not_found": "```plaintext\n[Explanation]\nThe module imports scipy.optimize and matplotlib at top-level. If those packages are unavailable in the test environment, importing the module will raise ModuleNotFoundError/ImportError and the test will not reach the ValueError expected by the unit test. Although if scipy and matplotlib were present, calling task_func with no matching target produces empty x/y arrays and scipy.optimize.curve_fit would raise a ValueError (which would satisfy the test). Because the test environment is unlikely to provide scipy/matplotlib, the code will most likely fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_not_enough_points": "```plaintext\n[Explanation]\nThe test calls task_func with small_array shape (3,1). After filtering, filtered_data has only one column, so accessing filtered_data[:, 1] raises an IndexError (\"index 1 is out of bounds for axis 1 with size 1\"), not a ValueError. The test expects a ValueError, so the raised exception type does not match and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_functionality": ""}}
{"id": "BigCodeBench/865", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a DataFrame preserving the 'Item' order, applies scipy.stats.zscore to 'Count' (which yields mean \u2248 0 for [100,200,150]) and MinMaxScaler to 'Weight' (mapping 0.5\u21920, 0.6\u21920.5, 0.7\u21921). All test assertions (items order, normalized weight within [0,1], and abs(mean of normalized counts) \u2264 0.001) are satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function uses scipy.stats.zscore for counts and sklearn MinMaxScaler for weights. For the input counts (-100, -200, -150) the z-score normalization yields values that sum to zero (mean ~ 0 within floating precision), meeting the mean tolerance. The MinMaxScaler maps weights (-0.5, -0.1, -0.2) into [0,1] (0,1,0.75), satisfying the min/max assertions. Therefore the test conditions are met.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nzscore on a constant Count column yields NaN for each entry (std=0), satisfying the isna() check. MinMaxScaler on a constant Weight column produces 0.0 for every entry, satisfying the weight equality check. No other errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function uses scipy.stats.zscore for counts and sklearn MinMaxScaler for weights. For the input counts [1e6, 2e6, 1.5e6], z-score normalization yields [-1.2247..., 1.2247..., 0.0], whose mean is effectively 0 (well within the 0.001 tolerance). MinMaxScaler maps weights [0.5, 0.6, 0.7] to [0.0, 0.5, 1.0], which lie within [0, 1]. No errors or edge cases affect this specific test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe input contains a single item. scipy.stats.zscore on a single-value series yields NaN, so 'Normalized Count' is NaN (satisfying isna()). sklearn's MinMaxScaler maps a constant single feature to 0.0, so 'Normalized Weight' equals 0.0. Both test assertions will be met.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_return_value": "```plaintext\n[Explanation]\nThe test provides a malformed input: the third tuple has four elements ('C', 999, 3, 0.7) while task_func expects tuples of length 3. Pandas will raise a ValueError when constructing the DataFrame with columns=['Item','Count','Weight'] due to the shape mismatch, so the function will not return the expected DataFrame. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_large_data_amount": "```plaintext\n[Explanation]\nThe function performs the same operations the test expects: z-score normalization of counts (using scipy.stats.zscore) and min-max scaling of weights (using sklearn's MinMaxScaler). It builds a DataFrame in the same order and returns the same column names. The test computes expected values using the same functions on the same input; the DataFrame indices and ordering align. Assigning the scaler's (n,1) output to a single DataFrame column is a common pattern and produces the scalar normalized values per row, so values will match. No dtype checks are enforced in the test. Therefore the code will produce the expected DataFrame for the provided test.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/963", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test calls task_func on an existing empty source directory. The function creates the target directory (already exists), makes an empty zip file (writes no entries because there are no matching files), and returns its path. The test counts matching-extension entries in the zip (none), expecting 0. This behavior matches the test assertion.\n\n[Result]\n```\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function walks the given source directory and only adds files whose extensions are in ['.txt', '.docx', '.xlsx', '.csv']. The test's source directory contains only 'a.pdf' and 'b.gif', so no files are added and the produced zip has zero matching entries. The function creates the target directory and returns the zip path without raising errors, so the test's assertion of 0 passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function zips files with extensions ['.txt', '.docx', '.xlsx', '.csv'] from the given source directory. The test creates 'some_matching_files' containing c.txt, d.docx, and e.png, then calls task_func on that directory. The code will include c.txt and d.docx (2 files) in the zip, so the zip_file_count returns 2, matching the test expectation.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function zips files with extensions ['.txt', '.docx', '.xlsx', '.csv'] from the given source directory. The test calls task_func with the 'all_matching_files' directory which contains exactly four files with those extensions. The code walks the directory, adds those files to the zip using relative names, and returns the zip path. The test counts entries with those extensions in the zip; this will be 4. No errors or edge cases apply.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function zips files with extensions ['.txt', '.docx', '.xlsx', '.csv'] recursively. In the test, the source directory passed is .../nested_dir which contains nested/j.txt and nested/k.docx (and l.png). The code walks the directory, matches .txt and .docx, adds them to the zip, and the test counts 2 matching entries. No errors or edge cases affect this case.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function walks the given source directory, includes files with extensions .txt, .docx, .xlsx, .csv into a zip, and returns the zip path. The test's source directory \"mixed_extensions\" contains o.txt, p.docx, q.unknown, r.csv \u2014 three matching files. The code correctly creates the zip and adds the three matching files; the test counts matching entries and expects 3. No errors or edge cases affect this case.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function walks the given source directory, adds files whose lowercased names end with one of the valid extensions ('.txt', '.docx', '.xlsx', '.csv') to a zip stored in the target directory. The test provides a source subdirectory containing exactly four files with those extensions (subdir1/s.txt, subdir2/t.xlsx, subdir3/u.docx, subdir2/v.csv). The code correctly creates the zip and stores relative paths, and the test's counting function matches those suffixes. No errors or edge cases affect this test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1039", "raw_responses": {"test_file_found": "```plaintext\n[Explanation]\nThe test expects that after computing the hash the code calls secure_socket.send(...) and secure_socket.close(). The provided task_func wraps the socket, reads the requested path, computes and returns the SHA256 hex digest, but never calls send or close on the secure socket. Therefore the assertions mock_secure_socket.send.assert_called() and mock_secure_socket.close.assert_called() will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe function returns 'File not found' when os.path.exists is False (so the result assertion passes) but it does not send that message back over the secure socket nor close the secure socket. The test additionally asserts mock_secure_socket.send(...) and mock_secure_socket.close() were called. Those calls are absent, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches ssl.SSLContext so ssl.create_default_context() returns the mocked context, and configures wrap_socket to return a mock secure socket whose recv raises 'Test exception'. The function catches the exception and returns \"Error: Test exception\", and wrap_socket is called with the mocked client socket \u2014 these assertions pass. However, the test also expects mock_secure_socket.close.assert_called(). The provided function does not call secure_socket.close() (no finally/close on exception), so close() is never called. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_task_func_empty_file": "", "test_task_func_large_file": ""}}
{"id": "BigCodeBench/177", "raw_responses": {"test_word_frequencies": "```plaintext\n[Explanation]\nThe function lowercases all content words before counting, so output keys are all lowercase (e.g., 'data', 'what', 'python'). The test's expected dictionary distinguishes cases (e.g., 'Data' and 'data', 'What' and 'what'), so the returned dict will not match the expected keys even though total counts align. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_no_matching_articles": "```plaintext\n[Explanation]\nThe test passes a non-empty DataFrame that has 'Title' and 'Content' columns but no titles containing \"like\" or \"what\". The function checks columns and emptiness (passes), filters titles with a case-insensitive regex and gets an empty filtered_df, iterates zero times, and returns an empty dict {}. This matches the test's expected result.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test passes an empty DataFrame with the required columns. The function checks `if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:` and raises a ValueError when df.empty is True. The test expects an empty dict {} instead of an exception, so the function will raise and the test will fail.\n\n[Result]\nFAIL\n```", "test_case_sensitive_handling": "```plaintext\n[Explanation]\nThe function lowercases all content words before counting, so the returned dictionary has lowercase keys (e.g., 'technological', 'these'). The test's expected dictionary uses capitalized keys ('Technological', 'These'), so the dictionaries will not match and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_columns": "```plaintext\n[Explanation]\nThe test passes a DataFrame without 'Title' and 'Content' columns. The function's initial check raises ValueError when those columns are missing, and the test expects a ValueError. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/313", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function extracts the target text from the filename (it removes bracketed parts from the filename), but the test expects the target to come from the file content (\"example[content]\"). The created file is named \"test_file1.txt\" (no brackets), so task_func will use \"test_file1.txt\" as the key in moved_files, not \"example\". Therefore the assertion expecting 'example' in moved_files fails.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function extracts text from filenames (it removes bracketed parts from the filename) rather than from file contents. The test expects grouping by the text before brackets inside each file's CONTENT (e.g., 'example' and 'sample'), but the filenames are 'test_file1.txt', etc., so moved_files will have keys like 'test_file1.txt' instead. Therefore the assertions expecting 'example' and 'sample' keys will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function extracts the target text from filenames (it strips bracketed text from the filename, not from file contents). The test creates a file named \"test_file1.txt\" whose file content is \"[example]content\". Since the filename contains no brackets, the target_text becomes \"test_file1.txt\" and moved_files will have that key. \"content\" (which only appears in the file content) will not be a key in moved_files, so the assertion self.assertNotIn('content', moved_files) holds.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe code extracts the first non-whitespace token from the filename. For 'test_file1.txt' this is 'test_file1.txt', so the function creates a subdirectory named 'test_file1.txt', moves the file into it, and returns moved_files = {'test_file1.txt': ['test_file1.txt']}. The test expects moved_files to be empty ({}), so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function determines the target text from the filename (it strips bracketed parts from the filename, not the file content). For the test file 'test_file1.txt' there are no brackets in the filename, so target_text becomes 'test_file1.txt'. The code creates a subdirectory named 'test_file1.txt', moves the file there, and records it in moved_files. moved_files will therefore be non-empty, satisfying the assertion. The test helper's use of os/shutil is fine because those modules are imported in the module containing task_func.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/454", "raw_responses": {"test_move_no_files": "```plaintext\n[Explanation]\nThe function checks that both TemporaryDirectory paths exist (they do), uses glob to find *.txt files (none exist), so files_to_move is empty and moved_files remains []. The test calls task_func and expects an empty list length \u2014 this will be satisfied. The unused helper references os but is not invoked, so no NameError occurs.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_extension": "```plaintext\n[Explanation]\nThe function builds the glob pattern as os.path.join(src_dir, f\"*.{ext}\"). With ext == '' this becomes \"src_dir/*.\" which does not match \"test.txt\", so no files are found or moved. The temporary directories exist, so no exceptions are raised. The function returns an empty list as expected.\n\n[Result]\nPASS\n```", "test_move_specified_extension_files": "```plaintext\n[Explanation]\nThe test patches glob.glob to return two source file paths and patches os.path.exists to simulate that the source and destination directories exist and that only file1 already exists in the destination. The implementation builds destination paths with os.path.join and checks existence before moving; it will therefore attempt to move only file2. shutil.move is patched so no actual file operations occur. The test allows either a call with dest_dir or dest_dir + '/file2.txt' (the second assertion is used), and moved_files contains the destination path appended by the function. All behaviors match, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_no_files_moved_with_different_extension": "```plaintext\n[Explanation]\nThe function checks that the source and destination directories exist, builds a glob pattern for \"*.txt\", and moves any matching files. The test creates a .md file only, so glob finds no \"*.txt\" files, moved_files is an empty list, and len(files_moved) == 0 as asserted. No errors arise from directory existence.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_exception_raised_when_dirs_do_not_exist": "```plaintext\n[Explanation]\nThe test calls task_func twice after cleaning up the temp directories. The function checks os.path.exists for the source first and raises FileNotFoundError if it's missing. In the first call the source is removed so FileNotFoundError is raised. In the second call both source and destination are removed, and the function still raises FileNotFoundError (due to the missing source). Both assertRaises checks expect a FileNotFoundError, so the test passes.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/553", "raw_responses": {"test_non_empty_lists": "```plaintext\n[Explanation]\nThe function creates a pandas DataFrame and calls df.plot(kind='bar'), which returns a matplotlib AxesSubplot (a subclass of matplotlib.axes.Axes). For the given inputs (lists of length 3), the code will produce a 3x3 DataFrame and return an Axes object. The isinstance check against matplotlib.axes.Axes will succeed.\n\n[Result]\nPASS\n```", "test_empty_a_list": "```plaintext\n[Explanation]\nCalling task_func([], ['A','B','C']) makes a DataFrame with 0 rows and 3 columns. pandas.DataFrame.plot on an empty DataFrame still creates and returns a matplotlib Axes (an AxesSubplot), which is an instance of matplotlib.axes.Axes. No exceptions are raised, so the isinstance check in the test passes.\n\n[Result]\nPASS\n```", "test_empty_b_list": "```plaintext\n[Explanation]\nWhen b is empty, num_cols becomes 0 so the function creates a DataFrame with shape (3, 0) (no columns). pandas treats a DataFrame with zero columns as empty, and DataFrame.plot on an empty DataFrame raises an error (e.g. \"No numeric types to plot\") instead of returning a matplotlib.axes.Axes. Therefore the test, which expects an Axes instance, will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_both_lists_empty": "```plaintext\n[Explanation]\nWhen both lists are empty, the function creates an empty DataFrame (0 rows, 0 columns) and calls df.plot(kind='bar'). Pandas raises an error for plotting when there are no numeric columns to plot (e.g. \"No numeric types to plot\"), so the function will not return a matplotlib.axes.Axes object and the test will fail.\n\n[Result]\nFAIL\n```", "test_a_list_longer_than_columns": "```plaintext\n[Explanation]\nThe function computes num_cols = min(len(b), len(COLUMNS)) = 2, creates a DataFrame of shape (6,2), and df.plot(kind='bar') returns a matplotlib Axes (AxesSubplot), so isinstance(ax, matplotlib.axes.Axes) will be True. No other errors in this test case.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/655", "raw_responses": {"test_extract_topics": "```plaintext\n[Explanation]\nThe test only checks that task_func(self.texts, 2) returns a list with length 2 and that each topic is a list. The implementation constructs exactly num_topics topics and appends a list of top words for each topic, so both conditions are satisfied. Potential external dependency (nltk stopwords) is handled by nltk.download('stopwords') and typically available in the test environment; no content checks are performed.\n\n[Result]\nPASS\n```", "test_invalid_num_topics": "```plaintext\n[Explanation]\nCalling task_func with num_topics=0 leads to constructing an NMF model with n_components=0. sklearn.decomposition.NMF raises a ValueError for non-positive n_components (on fit), so the test's with self.assertRaises(ValueError) will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_texts": "```plaintext\n[Explanation]\nThe test calls task_func([], 1) and expects two values (topics, ax). The provided task_func returns only a single value (topics), so the test will fail with a ValueError due to attempting to unpack two values from one. Additionally, if reached, the function would call TfidfVectorizer.fit_transform on an empty list which would raise an error (empty vocabulary), so it cannot satisfy the test's expectations of returning an empty topics list and ax=None.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_single_text": "```plaintext\n[Explanation]\nThe function preprocesses and vectorizes the input and runs NMF with n_components equal to the requested num_topics. NMF.components_ has shape (n_components, n_features), so the code always returns a list with length num_topics. For a single document and num_topics=1 this yields one topic, so the assertion len(topics) == 1 will hold (assuming required libraries/data are available).\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_all_stopwords": "```plaintext\n[Explanation]\nThe test calls task_func expecting two return values (topics, ax), but the provided function returns only one value (topics), causing an unpacking error. Additionally, for an input consisting solely of stopwords the function produces empty processed documents; TfidfVectorizer with stop_words='english' will result in an empty vocabulary and raise a ValueError before reaching any return. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/870", "raw_responses": {"test_default_data": "```plaintext\n[Explanation]\nThe function transposes the default list of tuples, filters numeric entries (int/float), computes means, and builds a DataFrame indexed as \"Position 0/1/2\". For the provided default data:\n- Position 0 has only strings \u2192 NaN (np.isnan True).\n- Position 1 mean of [1,2,3,4,5] \u2192 3.0.\n- Position 2 mean of [2.1,3.2,4.3,5.4,6.5] \u2192 4.3.\nAll assertions in the test match these results, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_custom_data": "```plaintext\n[Explanation]\nThe function transposes the tuples, filters numeric values, computes means, and builds a DataFrame with indices 'Position i'. For the input [('x',10,20.5),('y',20,40.6),('z',30,60.7)] it computes Position 1 mean = (10+20+30)/3 = 20.0 and Position 2 mean = (20.5+40.6+60.7)/3 = 40.6, so both assertions will be True.\n\n[Result]\nPASS\n```", "test_incomplete_data": "```plaintext\n[Explanation]\nThe function transposes with zip_longest(fillvalue=np.nan), filters numeric ints/floats while excluding np.nan, and computes means. For the given incomplete_data, Position 1 collects [1,2,4,5] -> mean 3.0 and Position 2 collects [3.2,5.4,6.5] -> mean ~5.0333333. Index names and column match the test. No type errors occur, so both assertions will be true.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_data": "```plaintext\n[Explanation]\nThe function explicitly checks \"if not data_list\" and returns pd.DataFrame(columns=['Mean Value']). Passing an empty list triggers this branch, producing an empty DataFrame (df.empty is True), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_non_numeric_data": "```plaintext\n[Explanation]\nAll tuple entries are strings, so the code filters out non-numeric values and produces NaN for each position. The resulting DataFrame contains only NaNs, so df.isna().values.all() is True.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/964", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function handles .txt files by reading with utf-8 and writing a single-column CSV. The test writes a simple ASCII .txt file, so reading and conversion succeed. The output file name and count match the test expectations, so the code will pass the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function will find the created sample.docx, read its paragraphs using python-docx, join them into a string, place that string into a DataFrame, and write sample.csv to the target directory. No errors are expected (modules used are imported in the test), and converted_count will be 1, matching the test assertions.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test creates a sample.xlsx using pandas (so openpyxl must be available to write it), then calls task_func which recognizes '.xlsx', reads it with pandas.read_excel, converts to text and writes sample.csv in the target directory. No steps in the code prevent this; exceptions are caught but none are expected because the test already wrote the .xlsx successfully. The function will return 1 and the output file will exist, satisfying both assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test creates a single sample.csv in the source directory. task_func supports '.csv', reads it with pandas (header=None), wraps the content into a single-row DataFrame under 'Text', and writes a CSV to the target directory. No exceptions are triggered, so converted_count becomes 1 and the output file exists. Therefore the assertions in the test will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test creates empty temporary source and target directories but does not create any files. task_func checks the source exists, iterates os.listdir (empty), never processes any files, and returns converted_count which remains 0. This matches the test's expected value 0.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function immediately checks os.path.exists(source_directory) and raises FileNotFoundError when the path does not exist. The test calls task_func with '/path/does/not/exist' and expects a FileNotFoundError, so the behavior matches the test expectation.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function only converts files with extensions .txt, .docx, .xlsx, or .csv. The test creates only an unsupported.pdf file in the source directory, so the loop will skip it and converted_count remains 0, matching the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function processes .txt, .docx, .xlsx, and .csv files in the source temp directory and writes them as CSVs to the target directory. The test creates one file of each supported type named \"sample.ext\". Although all conversions write to the same output name (overwriting), the code increments converted_count for each successful conversion. No exceptions are expected (pandas and python-docx usage is correct), so task_func will return 4 as asserted.\n\n[Result]\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe test places the only supported file (sample.txt) inside a nested subdirectory. The code iterates only over os.listdir(source_directory) and skips directories (does not recurse), so it never sees the nested file and returns 0. The test expects 1 converted file, so the code will fail this test.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/1040", "raw_responses": {"test_queue_empty_condition": "", "test_server_response": "```plaintext\n[Explanation]\nThe server binds on localhost:12345, accepts a connection, reads client data, enqueues a response that appends a timestamp, and sends it back. The test starts the server in a thread, connects a client, sends \"Hello, Server!\" and receives a reply. The server's logic correctly decodes the incoming bytes, builds a response containing the original message, and sends it back; timing (select timeout 1s) allows the response to be produced before the test's recv. No code paths in this test cause exceptions (no exceptional socket conditions), so the assertion that the response contains the original message will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_multiple_connections": "", "test_no_data_received": "```plaintext\n[Explanation]\nThe server accepts the connection but only sends data in response to received data. The test connects a client and immediately calls recv with a 2s timeout without sending anything; since the server doesn't proactively send or close the connection, client.recv will block and then raise socket.timeout as expected. No other errors (port in use, exceptions) are likely in this single-test context.\n\n[Result]\nPASS\n```", "test_server_closes_after_duration": "```plaintext\n[Explanation]\nThe server is started with run_duration=10s and will be listening when the test runs (setUp waits 1s). The client connects and calls recv without sending data; the server does not send anything by default, so client.recv will block until the socket timeout (2s) and raise socket.timeout, which matches the asserted expected exceptions. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_large_data_transfer": "```plaintext\n[Explanation]\nThe server accepts the connection, receives the 1000-byte payload, appends a timestamp, queues the ~1033-byte response, and sends it back. Although the code doesn't handle partial send re-queueing, on localhost/loopback a single send of ~1KB is very likely to transmit the full message, so the client will receive the full sequence of 1000 'A's and the test's condition will be satisfied. The test joins the server thread (which runs 10s) but that only delays teardown and doesn't affect correctness.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/184", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation/numbers, removes the listed stopwords, and uses CountVectorizer to produce counts. For the provided inputs the remaining tokens are exactly ['test'], ['python','cool'], and ['nltk','sklearn','useful','text','analysis']; CountVectorizer produces columns in alphabetical order (analysis, cool, nltk, python, sklearn, test, text, useful) with the counts matching the expected DataFrame. Data types and indices align, so the assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function lowercases, strips non-letters (removing punctuation and digits), and removes stopwords (including \"is\" and \"with\"). The processed texts become:\n- \"hello world\"\n- \"gpt amazing\"\n- \"chat chatgpt\"\nCountVectorizer then extracts the tokens and produces columns in alphabetical order: ['amazing','chat','chatgpt','gpt','hello','world'], with counts matching the expected DataFrame.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation/numbers, removes the listed stopwords, and uses CountVectorizer to produce term counts. For the given inputs it yields the tokens: cool, deep, develops, future, latest, learning, models, openai, stay, updated \u2014 matching the expected DataFrame (including ordering and counts). Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function lowercases, removes non-letters, filters STOPWORDS (which includes \"the\" and \"over\"), and uses CountVectorizer producing the vocabulary in alphabetical order. After preprocessing the texts become:\n1) \"quick brown fox\"\n2) \"jumps lazy dog\"\n3) \"lorem ipsum dolor sit\"\nThe resulting matrix columns and counts match the expected DataFrame exactly (same columns and values), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lowercases, removes punctuation, strips stopwords (including \"you\", \"are\", \"a\"), and vectorizes the remaining tokens. For the inputs:\n- \"Hello there!\" -> \"hello there\" -> tokens hello, there\n- \"General Kenobi.\" -> \"general kenobi\" -> tokens general, kenobi\n- \"You are a bold one.\" -> \"bold one\" (stopwords removed) -> tokens bold, one\n\nCountVectorizer yields the features in alphabetical order: bold, general, hello, kenobi, one, there, and the counts per row match the expected DataFrame. Dtypes and indices align, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/324", "raw_responses": {"test_empty_file_list": "```plaintext\n[Explanation]\nWith an empty file_list the function creates no threads, returns the empty exit_codes list, and never calls subprocess.Popen. The test patches subprocess.Popen and asserts it was not called; that will hold true. The returned result [] matches the expected [].\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_valid_files": "```plaintext\n[Explanation]\nThe function calls process.wait() and then reads process.returncode. The test only configures mock_popen.return_value.poll.return_value = 0, not the mocked returncode or wait behavior. Because subprocess.Popen is a MagicMock, process.returncode will itself be a MagicMock (not integer 0), so the function will append MagicMock objects instead of integers [0, 0]. Therefore the assertion comparing to [0, 0] will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_valid_directory_and_files": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and sets mock_popen.return_value.poll.return_value = 0, but the code calls process.wait() and then reads process.returncode. The mock's returncode is not set, so process.returncode will be a Mock object (not integer 0). Thus the function returns a list of Mock objects instead of [0, 0]. The Popen call count would be 2, but the return values mismatch causes the assertion to fail.\n\n[Result]\nFAIL\n```", "test_process_still_running": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen but only sets mock_popen.return_value.poll.return_value = None. The function calls process.wait() and then uses process.returncode. The mocked Popen object's returncode is a Mock by default (not None) and wait() does not set it here, so the function will return a list containing a Mock object rather than [None]. Thus the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_multiple_processes_with_different_exit_codes": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen but only sets poll.side_effect; the function calls process.wait() and then reads process.returncode. Because mock_popen.return_value was not configured for wait() or returncode (and the same mock instance is returned for all calls), the code will collect MagicMock returncode values instead of [0, 1, None]. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/458", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function builds the DataFrame as pd.DataFrame([normalized_dict]) which yields a single-row DataFrame whose 'a' cell is the list [2,4,6] and 'b'/'c' are scalars. The test expects a 3-row DataFrame with 'a' expanded into separate rows and 'b'/'c' repeated per row. The shapes and contents therefore differ, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe input JSON is '{}' (an empty dict). The function loads it and creates normalized_dict = {}. It then constructs the DataFrame with pd.DataFrame([normalized_dict]), which produces a DataFrame with 1 row and 0 columns (not a completely empty DataFrame). The test expects pd.DataFrame() (0 rows, 0 columns), so the DataFrames differ and the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function produces a single-row DataFrame where the 'a' column contains the list [2, 'apple', 6] as a single cell (pd.DataFrame([normalized_dict])). The test expects a DataFrame with three rows (the list elements expanded into rows) and repeated values for 'b', 'c', and 'd'. The shapes and values therefore do not match, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function parses the JSON and converts numeric strings using regex to floats doubled: \"1\" -> 2.0, \"2.5\" -> 5.0, and leaves \"string\" unchanged. The resulting DataFrame has columns a, b, c with values [2.0], [5.0], [\"string\"]. The expected DataFrame has [2], [5.0], [\"string\"], and the test uses check_dtype=False, so the float vs int dtype difference is ignored. Column order and values match, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe input JSON contains \"a\" as a list [1,2,{\"b\":3}] and \"c\" as 4.9. The function doubles numbers but keeps lists as single cells, producing a DataFrame with one row where 'a' is the entire list [2, 4, {'b': 3}] and 'c' is 9.8. The test expects three rows (one per list element) with 'a' values 2, 4, {'b': 3} and 'c' repeated 9.8. The shapes and contents therefore differ, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/560", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nThe function correctly parses the input, sets the labels and title, and creates three bars. However, Axes.patches includes the axes background patch in addition to the bar rectangles, so len(ax.patches) will be 4 (3 bars + 1 axes patch), causing the assertion expecting 3 to fail.\n\n[Result]\nFAIL\n```", "test_full_year_data": "```plaintext\n[Explanation]\nThe function parses the 12 comma-separated entries correctly, creates 12 bars via ax.bar (one Rectangle per entry), and returns the Axes. Therefore len(ax.patches) will be 12 and the unittest assertion should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_partial_year_data": "```plaintext\n[Explanation]\nThe function splits the input into three entries, parses months and values, and calls ax.bar with three x positions and heights, producing three Rectangle patches. No errors occur for the given input format, so len(ax.patches) == 3.\n\n[Result]\nPASS\n```", "test_incorrect_data_format": "```plaintext\n[Explanation]\nThe function only reads the year from the first entry and never checks that all entries share the same year. The test supplies entries from 2022 and 2023, so no ValueError will be raised and the assertRaises will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_data": "```plaintext\n[Explanation]\nWhen data == '' the code sets entries = [''] and then in the loop entry.split('-') yields ['']. Accessing parts[1] causes an IndexError, not a ValueError. The test expects a ValueError to be raised, so the behavior does not match and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/969", "raw_responses": {"test_incremental_values": "```plaintext\n[Explanation]\nThe function computes per-column cumulative sums and applies MinMaxScaler to each column. For A: cumsum [1,3,6] -> [0.0, 0.4, 1.0]. For B: cumsum [3,5,6] -> [0.0, 0.66666667, 1.0]. The input has no NaNs, is non-empty, and numeric, and column names are preserved. These match the expected DataFrame (within atol), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_negative_numbers": "```plaintext\n[Explanation]\nThe function computes cumulative sums per column and applies MinMax scaling. For column A cumulative sums are [-1, -3, -6] -> scaled to [1.0, 0.6, 0.0]. For B cumulative sums are [-3, -5, -6] -> scaled to [1.0, 0.33333333, 0.0]. No NaNs, non-numeric types, or empty DataFrame issues occur. The output matches the expected values within the given tolerance.\n\n[Result]\nPASS\n```", "test_all_zeros": "```plaintext\n[Explanation]\nThe input columns are all zeros, so cumsum is [0,0,0] per column. MinMaxScaler maps constant features to zeros, producing the expected float zeros. The DataFrame is non-empty, has no NaNs, and columns are numeric, so no exceptions are raised.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_same_numbers": "```plaintext\n[Explanation]\nThe function computes per-column cumulative sums and applies sklearn's MinMaxScaler to each column independently. For the test input A=[5,5,5] and B=[2,2,2], the cumulative sums are A=[5,10,15] and B=[2,4,6], which scale to [0.0,0.5,1.0] for both columns. The function checks pass (non-empty, no NaNs, numeric dtypes) and returns matching values and columns. The test uses assert_frame_equal with atol tolerance, so the output matches the expected DataFrame.\n\n[Result]\nPASS\n```", "test_non_numeric_data_raises": "```plaintext\n[Explanation]\nThe function checks column dtypes using pd.api.types.is_numeric_dtype and will raise a TypeError when any column is non-numeric. Given the input DataFrame with column 'A' of strings, task_func will raise TypeError as the test expects.\n\n[Result]\nPASS\n```", "test_nan_values_raise": "```plaintext\n[Explanation]\nThe test calls task_func on a DataFrame containing a NaN and expects a ValueError. The function checks df.isnull().any().any() and raises ValueError(\"DataFrame contains NaN values\") when any NaN is present, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test calls task_func with an empty DataFrame and expects a ValueError. The function checks df.empty at the start and raises ValueError(\"DataFrame is empty\"), so the behavior matches the test expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/1042", "raw_responses": {"test_successful_email_send": "```plaintext\n[Explanation]\nThe test patches getpass.getpass but does not patch builtins.input. task_func calls input() twice to get sender and recipient emails; since input() is unmocked the test will block or raise EOFError, causing the test to fail. The mocked getpass side_effect is irrelevant because input() is called first and is not patched.\n\n[Result]\n```plaintext\nFAIL\n```", "test_email_with_empty_message": "```plaintext\n[Explanation]\nThe test provides a MagicMock client_socket whose recv() returns a MagicMock (truthy) whose decode() returns ''. task_func checks \"if not message_data\" \u2014 but message_data is truthy, so it proceeds. The code then calls input() twice (for sender and recipient) and getpass.getpass once. The test only patches getpass.getpass, not builtins.input, so the unpatched input() calls will block or raise EOFError during the unit test, causing the test to error/fail. (Additionally, even if inputs were patched, send_message would be invoked on the context-manager's __enter__ return value, not on mock_smtp.return_value, so the test's assertion would not reliably detect the call.)\n\n[Result]\n```plaintext\nFAIL\n```", "test_smtp_server_connection_error": "```plaintext\n[Explanation]\nThe test expects task_func to raise smtplib.SMTPConnectError when smtplib.SMTP is invoked. However, the function uses input() twice to get sender and recipient emails but the test only patches getpass.getpass (not input). As a result, the first two input() calls will block or raise EOFError during the test, and the function will not reach the point where smtplib.SMTP is constructed. Therefore the test will not observe the expected SMTPConnectError.\n\n[Result]\nFAIL\n```", "test_socket_closes_after_operation": "```plaintext\n[Explanation]\nThe test provides a MagicMock client_socket returning data so task_func proceeds. The function never calls client_socket.close(), so client_socket.close.assert_called_once() will fail. Additionally, task_func uses input() for emails (not mocked) and getpass.getpass only once, while the test sets multiple side effects; this would cause blocking or input-related errors during the test. The core reason the test fails is the missing client_socket.close() call.\n\n[Result]\n```plaintext\nFAIL\n```", "test_successful_email_dispatch": "```plaintext\n[Explanation]\nThe test expects task_func to read sender and recipient from patched inputs and to send a success message back to the client socket and close it. The provided code:\n- Calls input() twice (for sender and recipient) but the test only patches getpass.getpass; input() is not patched, so the test will block or raise EOFError.\n- Only calls getpass.getpass once (for the password), while the test's side_effect suggests different expectations.\n- Does not call client_socket.send(...) nor client_socket.close() (it only prints to stdout), so the assertions expecting client_socket.send and client_socket.close will fail.\n\nTherefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/461", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test runs task_func on a temporary bash script that sleeps 5 seconds and only asserts that the returned dict contains the keys 'CPU Usage' and 'Memory Usage'. The function checks the file exists, launches the script with subprocess, monitors it with psutil, accumulates CPU and memory samples, and returns a dict with those two keys. There are no value assertions and the timeout (10s) exceeds the script duration, so the function will complete and return the required keys. (This assumes the environment has the psutil module available, which the code imports.)\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe script runs a sleeping bash process for 5s (timeout is 10s), so the monitoring loop executes without hitting the timeout. psutil.Process.cpu_percent() returns a float and memory_info().rss is an int; accumulating them preserves their types. No obvious exceptions occur (file exists and is executable, subprocess launched with 'bash'). Thus the returned 'CPU Usage' is a float and 'Memory Usage' is an int, satisfying the test assertions.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function begins by checking os.path.exists(script_path) and raises FileNotFoundError if the path does not exist. The test calls task_func('non_existent_script.sh') expecting a FileNotFoundError. Since that file is not created in the test, the function will raise FileNotFoundError, satisfying the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function launches the bash script and polls psutil.cpu_percent() in a loop with a 0.1s sleep. Although the first cpu_percent() call may return 0.0, subsequent calls will report nonzero usage while the CPU-bound bash loop (10000 echo iterations) is still running. Given the script's workload and the 0.1s sampling interval, at least one positive cpu_percent reading is expected before the script exits, so resources['CPU Usage'] will be > 0.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe script file exists and the function launches it with bash. psutil (if available) will observe the process and the function always returns a 'Memory Usage' integer (initialized to 0 and incremented if readings occur). The test only asserts that Memory Usage >= 0, which will be true (0 or positive). Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function checks the script exists, launches it with subprocess.Popen(['bash', script_path]) and monitors it with psutil, accumulating CPU and RSS memory; it always returns a dict (unless FileNotFoundError is raised). The test creates the script, calls task_func with a long timeout, and only asserts the return is a dict. There are no syntax errors or obvious runtime errors in typical Linux CI where bash and psutil are available, so the function will return a dict and the test's assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe module imports psutil at top-level. If psutil is not installed in the test environment the import will raise ModuleNotFoundError before the test runs. While the function logic would return a dict for timeout=0 (it terminates the subprocess immediately and returns the metrics), the missing third-party dependency causes the test to fail in most environments.\n\n[Result]\nFAIL\n```", "test_case_8": "```plaintext\n[Explanation]\nThe script path exists and task_func launches the bash script which blocks on read. The monitoring loop enforces the timeout (5s), then terminates the subprocess and returns a dictionary with 'CPU Usage' and 'Memory Usage'. No exceptions are raised in normal operation, so the function returns a dict as asserted by the test.\n\n[Result]\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe test passes a path containing an embedded null byte (via os.path.join with an absolute component). Calling os.path.exists(script_path) with such a path raises a ValueError (\"embedded null character\") rather than returning False. The code only raises FileNotFoundError when os.path.exists(...) is False and does not catch ValueError, so the ValueError will propagate instead of the expected FileNotFoundError. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_case_10": "```plaintext\n[Explanation]\nThe module imports the third-party psutil at top-level. If psutil is not available in the test environment, importing the module will raise ImportError and the test will error out before calling task_func. The test only requires that task_func returns a dict for a script that immediately exits; the function logic would produce a dict in that case, but the dependency on psutil makes the code likely to fail in a typical environment without psutil.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/187", "raw_responses": {"test_default_parameters": "", "test_custom_cities": "```plaintext\n[Explanation]\nThe module imports geopandas and shapely at top-level. In typical test environments geopandas is not available, causing a ModuleNotFoundError on import so the test never runs. (If geopandas were available, the function would return a GeoDataFrame with the two city names and the test assertions would pass.)\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_dic": "```plaintext\n[Explanation]\nThe function logic would raise a ValueError for dic={'Lon': 'invalid', 'Lat': (-90, 90)} because the code checks that 'Lon' and 'Lat' are tuples and raises ValueError when they're not. However, the module does top-level imports of geopandas and shapely (and numpy), which are not part of the Python standard library and may not be available in the test environment. If geopandas or shapely is missing, importing the module will raise ModuleNotFoundError before the test can call task_func, causing the test to fail.\n\n[Result]\nFAIL\n```", "test_coordinate_ranges": "```plaintext\n[Explanation]\nThe function validates the dictionary, generates longitudes and latitudes with numpy.random.uniform within the provided ranges, creates shapely Point objects, and places them in the 'Coordinates' column of a GeoDataFrame. The test calls task_func with Lon and Lat ranges (0,10) and checks that each Point's x and y are between 0 and 10 inclusive. numpy.random.uniform produces values in [0,10) (so never >10) and >=0, and the Points' .x and .y attributes are used correctly. Input types match the validation. Therefore the assertion should hold.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_return_type": "```plaintext\n[Explanation]\nThe function constructs and returns a geopandas.GeoDataFrame (gdf = gpd.GeoDataFrame(...)), so the object created is an instance of gpd.GeoDataFrame. The test asserts that the returned value is an instance of gpd.GeoDataFrame. Given geopandas is imported as gpd in the submitted module (and will be available to the test via import), the isinstance check will succeed.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/326", "raw_responses": {"test_no_bat_files": "```plaintext\n[Explanation]\nglob.glob is patched to return an empty list, so bat_files is empty, the loop is not entered, and task_func returns the initialized empty list. The test patches subprocess.Popen (unused by the function) which has no effect. The returned [] matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```", "test_single_bat_file_success": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and expects task_func to use Popen.wait() returning 0. The implementation uses subprocess.run (not Popen), so the Popen mock is never used. subprocess.run will be called for a real command ('file1.bat') and will either raise/return a nonzero code, so the result won't match [('file1.bat', 0)]. Also glob.glob is patched correctly, but that doesn't salvage the mismatch. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_single_bat_file_failure": "```plaintext\n[Explanation]\nThe test patches glob.glob and subprocess.Popen. subprocess.run in the code will invoke the patched Popen, but subprocess.run uses Popen.communicate() and then reads p.returncode \u2014 it does not call p.wait(). The test only sets mock_process.wait.return_value = 1 and does not set mock_process.returncode or communicate(). As a result process.returncode in the code will be a MagicMock, not the integer 1, so the returned list will not equal [('file1.bat', 1)].\n\n[Result]\nFAIL\n```", "test_multiple_bat_files_mixed_results": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and expects task_func to use Popen.wait results. The provided implementation uses subprocess.run (not Popen), so the Popen mock is never used. As a result the test's mocked behaviors won't be applied and the returned list won't match the expected mocked outputs, causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches glob.glob to return ['file1.bat'] and patches subprocess.Popen to raise an Exception. Although the code calls subprocess.run, run internally constructs a subprocess.Popen instance; the patched Popen therefore causes subprocess.run to raise, which the code catches and appends (filename, None). The returned list matches the expected [('file1.bat', None)].\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/657", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lowercases and tokenizes the texts, removes the provided stopwords, and trains a gensim Word2Vec model on the resulting token lists. With the given stopwords_mock, \"python\" remains in the processed tokens, so the trained model will be a Word2Vec instance and include 'python' in model.wv.key_to_index. The code's parameters (vector_size, min_count=1) ensure the word is included.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function cleans the texts to ['hello'], ['machine','learning'], ['python'] using the provided stopwords list, then trains and returns a gensim Word2Vec model with min_count=1 so 'python' will be in the vocabulary. The provided stopwords argument avoids needing nltk corpora at runtime. The model object is an instance of Word2Vec and model.wv.key_to_index will contain 'python'. Therefore the test assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test passes an empty texts list. The function builds processed_texts as an empty list and then calls Word2Vec(processed_texts, ...). Gensim's Word2Vec constructor requires a non-empty corpus (it attempts to build vocabulary from sentences) and will raise an error when given an empty iterable, so the function will not return a Word2Vec instance and the test will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "", "test_case_5": "```plaintext\n[Explanation]\nThe function lowercases and tokenizes the provided texts, removes the provided stopwords list (so nltk stopword download is irrelevant for this test), and trains a gensim Word2Vec with min_count=1. For texts ['Bonjour','Hola','Ciao'] the vocabulary will contain 'bonjour', so the model is a Word2Vec instance and 'bonjour' will be in model.wv.key_to_index. No other errors are triggered for this test case.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/879", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function performs reasonable validation, builds the contingency table, checks expected frequencies, and returns the chi-square p-value. For the provided test, the DataFrame columns are object dtype (so the type checks pass), both categories appear, and expected counts are well above 5. With np.random.seed(12) and independent random choices, the chi-square p-value will be a non-significant value near 0.5; thus it should fall within 0.4\u20130.6 as asserted.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe input DataFrame produces a 2x2 contingency table [[25,25],[25,25]]; chi2_contingency yields chi2=0 and p_value=1.0. All checks in the function pass (non-empty, columns present, object dtype allowed, multiple categories, expected frequencies >=5). The test asserts p\u22481 within 0.1, so the function will return 1.0 and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function will accept the DataFrame (columns exist and are object dtype), build a 4x4 contingency table from 200 samples, and the expected cell counts will be well above 5 (200/16 \u2248 12.5 on average). Therefore chi2_contingency runs and returns a valid p-value in [0,1], satisfying the test assertion.\n\n[Result]\n```plaintext\nPASS\n```", "test_edge_case_empty_dataframe": "```plaintext\n[Explanation]\nThe function immediately checks `if data.empty` and raises ValueError(\"Data cannot be empty\"). The test supplies an empty DataFrame (no rows), so data.empty is True and the function raises ValueError as the test expects.\n\n[Result]\nPASS\n```", "test_edge_case_non_categorical": "```plaintext\n[Explanation]\nThe test provides two float columns. The function checks that each column is either categorical or object dtype and raises TypeError otherwise. For numeric float Series both checks are false, so the function raises TypeError as the test expects.\n\n[Result]\nPASS\n```", "test_edge_case_single_category": "```plaintext\n[Explanation]\nThe input DataFrame has only one category per column. The function checks len(contingency_table.index) < 2 or len(contingency_table.columns) < 2 and will raise ValueError(\"Each column must have multiple categories\") before any further checks. This matches the unittest's expectation of a ValueError.\n\n[Result]\nPASS\n```", "test_edge_case_large_categories_small_counts": "```plaintext\n[Explanation]\nThe test builds two categorical columns drawn from 10 categories with only 20 rows, so many expected cell frequencies in the contingency table will be below 5. The function computes expected frequencies with chi2_contingency and raises ValueError if any expected < 5. The pandas dtype checks will accept string categories (object dtype), so execution reaches the expected-frequency check and raises ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```", "test_col_not_in_df": "```plaintext\n[Explanation]\nThe test calls task_func with col1='a' which is not a column in the DataFrame. The function immediately checks for column existence and raises ValueError(\"Columns must be in the data\"), which matches the test's expectation of a ValueError.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/971", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists all regular files in the provided directory and appends one tuple per file to the result. The test creates five files in a temporary directory and only asserts that the returned list length equals the number of created files. There is no early return or filtering that would exclude these files, and directory existence checks pass. No platform-specific behavior affects the count. Therefore the code will produce a list of length 5 and satisfy the test.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test calls task_func with a non-existing path and asserts a ValueError. The function checks os.path.exists and raises ValueError when the path does not exist, so this test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns an empty list when the directory contains no items. The test creates a temporary empty directory and asserts len(result) == 0. The code performs os.listdir on the directory and will produce an empty result list for an empty directory, so the assertion will hold.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function lists files in the given directory, uses os.stat to get st_size and returns tuples (name, size, creation_iso, mod_iso). The test only checks that each filename is present with the correct size. The code correctly writes the files, checks isfile, reads st_size, and the sizes will match the test's expected sizes. No platform-specific behavior affects file sizes, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lists files in the given directory and returns an entry for each file. The test creates only the five files in a temporary directory, calls task_func on that directory, and asserts the sorted file names match the created filenames. task_func uses os.listdir and os.path.isfile to include all files; no filtering or errors affect file names. The test does not check timestamps or formats. Therefore the code will produce the expected file name list and pass the test.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function returns creation time using st_ctime and modification time using st_mtime as UTC ISO strings. The test parses those ISO strings and asserts creation_time <= modification_time. On typical systems for newly created files st_ctime (inode change time) is equal to or greater than mtime, and in practice for files just written both timestamps are usually equal, satisfying the <= check. The ISO format produced is parseable by datetime.fromisoformat and both datetimes are timezone-aware, so the comparison works. No other errors (path checks, file listing) affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function lists directory entries and appends only entries for which os.path.isfile(item_path) is True. The test creates five files in the temp directory and one subdirectory with a file inside it; the subdirectory will be skipped by the isfile check. The function will therefore return five tuples, so len(result) equals len(self.files). No other assertions are made, so the test should pass.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function lists all files in the given directory (non-recursively), appends a tuple whose first element is the file name for every file, and returns that list. The test creates several files in a temporary directory and only asserts that each created file name appears in the returned list of names. There are no path, permission, or platform-dependent failures here that would omit those files, so the code will include all created filenames.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe test passes a path to a regular temporary file and asserts task_func raises ValueError. task_func checks existence then raises ValueError when os.path.isdir(directory_path) is False. For a NamedTemporaryFile path this condition is true, so ValueError will be raised and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_10": ""}}
{"id": "BigCodeBench/1053", "raw_responses": {"test_display_plot": "```plaintext\n[Explanation]\nThe test patches pandas.read_csv to return a DataFrame with text, so task_func will process the combined text, create a matplotlib Axes object and return it (since save_path is not provided). No exceptions are raised in this flow, so the returned value is not None and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_save_plot": "```plaintext\n[Explanation]\nThe test code references pd.DataFrame but never imports pandas as pd, causing a NameError in the test before the function is exercised. Therefore the provided code cannot pass this test as written.\n\n[Result]\nFAIL\n```", "test_empty_file": "```plaintext\n[Explanation]\nThe test patches pandas.read_csv to return an empty DataFrame with a 'Text' column. The function combines the (empty) text into an empty string and passes it to CountVectorizer, which raises a ValueError (\"empty vocabulary...\"). The exception is caught by the generic except block, which prints the error and returns None. The test asserts that the returned value is None, so the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_file_path": "```plaintext\n[Explanation]\nThe test patches pandas.read_csv to raise FileNotFoundError. task_func calls pd.read_csv inside a try/except and specifically catches FileNotFoundError, then re-raises a FileNotFoundError. The test expects a FileNotFoundError, so the behavior matches and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_large_data_set": "```plaintext\n[Explanation]\nThe function reads the patched DataFrame (1000 distinct 'word#' tokens), combines them into one document, and CountVectorizer successfully extracts ~1000 features (each count = 1). The plotting code creates and returns an Axes object when no save_path is given. No exceptions are triggered by the provided input, and the test only asserts the result is not None. Therefore the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/13", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test patches subprocess.call but the code uses subprocess.run, so the subprocess.run calls are not mocked. When the code tries to run ['wget', url], subprocess.run will raise FileNotFoundError (wget not present) which is not caught by the code (it only catches CalledProcessError). This causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP to raise Exception('Failed to connect to FTP server') when called. task_func calls ftplib.FTP(ftp_server) and catches the exception, re-raising with the message f\"Failed to connect to FTP server {ftp_server}: {str(e)}\". For ftp_server='invalid_server' this matches the expected message \"Failed to connect to FTP server invalid_server: Failed to connect to FTP server\". The patch target matches the ftplib reference used in the function, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so ftp.login raises Exception('Failed to login'). The code catches that and re-raises Exception with message \"Failed to log into FTP server ftp.dlptest.com with user invalid_user: Failed to login\", which exactly matches the test's expected string. No other code runs before the exception that would interfere.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so ftp.login raises Exception('Failed to login'). task_func catches that and raises Exception with message \"Failed to log into FTP server ftp.dlptest.com with user dlpuser: Failed to login\", which exactly matches the test's expected string. No other operations occur before the login error, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so ftp.cwd raises Exception('Failed to change directory'). task_func attempts ftp.cwd, catches that exception, and raises a new Exception with the message:\n\"Failed to change to directory /invalid_directory on server ftp.dlptest.com: Failed to change directory\"\nwhich exactly matches the test's expected string. No other errors occur before cwd, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/92", "raw_responses": {"test_cluster_centers": "```plaintext\n[Explanation]\nThe function validates inputs, fits KMeans with n_clusters=3, and plots data points plus centroids using ax.scatter(..., label='Centroids'). scatter returns a PathCollection whose get_offsets() contains one offset per centroid (3). The test's use of pd is satisfied by the module's top-level import. Therefore the assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_cluster_error": "```plaintext\n[Explanation]\nThe function validates n_clusters and raises ValueError when n_clusters <= 1. The test calls task_func(self.data, 1), so the validation triggers and the ValueError is raised as expected. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_valid_input": "```plaintext\n[Explanation]\nThe function validates inputs, runs KMeans on the provided 100x2 DataFrame, and returns the labels array. For n_clusters=3 and 100 samples, kmeans.fit_predict will produce an array of length 100. The test only asserts len(labels) == 100, which will be true. Necessary imports (pandas, sklearn, matplotlib, numpy) are present in the code, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_data_type": "```plaintext\n[Explanation]\nThe test calls task_func with a plain list ([[1,2],[3,4]]) and expects a ValueError. The function immediately checks isinstance(data, pd.DataFrame) and raises ValueError(\"Data must be a pandas DataFrame\") for non-DataFrame inputs, so the expected exception will be raised before any clustering code runs. The top-level import \"import pandas as pd\" in the provided code ensures pd is available for that check.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_cluster_number": "```plaintext\n[Explanation]\nThe test calls task_func with n_clusters = -1. The function explicitly checks \"if not isinstance(n_clusters, int) or n_clusters <= 1: raise ValueError(...)\", so passing -1 (an int <= 1) will raise a ValueError as the test expects. Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_return_type": "```plaintext\n[Explanation]\nThe function creates and returns a matplotlib Axes object (ax) from plt.subplots and the test checks isinstance(ax, plt.Axes). The module imports pandas and matplotlib as plt at top-level, so pd and plt are available for the test's setup and assertion. KMeans clustering on the provided DataFrame will run successfully and the returned ax is an instance of matplotlib.axes.Axes. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_return_labels": "```plaintext\n[Explanation]\nThe function accepts a pandas DataFrame and n_clusters=3, validates inputs, runs sklearn KMeans(random_state=42) and returns labels. On 100 random 2D points KMeans will produce exactly 3 unique labels. The test's use of pd works because pandas is imported at top-level in the provided code. No exceptions are expected from the clustering or plotting steps, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/199", "raw_responses": {"test_valid_input": "", "test_invalid_datetime_type": "```plaintext\n[Explanation]\nThe function begins by checking isinstance(utc_datetime, datetime) and raises ValueError if not a datetime. Passing a string will trigger this check, matching the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_missing_timezone_for_custom_city": "```plaintext\n[Explanation]\nThe function explicitly checks \"if city not in timezones: raise ValueError(...)\" while iterating cities. The test provides cities including 'Paris' but the timezones dict lacks 'Paris', so a ValueError will be raised as expected. Other input validations succeed (utc datetime has pytz.UTC, seed is int), so the test's assertRaises(ValueError) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_custom_cities_and_timezones": "```plaintext\n[Explanation]\nThe function validates inputs, converts the provided UTC datetime (which in the test is datetime(..., tzinfo=pytz.UTC)) to each city's local time, and appends an entry for every city in the provided cities list. The test provides matching timezone entries for both 'New York' and 'London', so no ValueError is raised and the returned DataFrame contains those two cities. The assertion comparing sets of cities will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_reproducibility_with_seed": "```plaintext\n[Explanation]\nBoth calls to task_func set the same seed (set_seed(seed)) and use random.randint (imported from the same random module), so the generated weather conditions sequence is reproducible. The utc_datetime provided has tzinfo=pytz.UTC (passes the tz check), timezone conversions and formatted strings are deterministic, and the DataFrame construction preserves order and types. No other nondeterministic behavior exists, so the two DataFrames will be identical.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/341", "raw_responses": {"test_numeric_data": "```plaintext\n[Explanation]\nThe function validates input and for numeric data uses seaborn.histplot(kde=True) and sns.boxplot. For the provided numeric DataFrame, it returns a matplotlib.figure.Figure with two axes. The histogram axis will contain patches (bars) and the boxplot axis will contain lines, satisfying the three assertions. The top-level import of matplotlib.pyplot as plt in the submitted code makes plt available when the test runs in the same execution context. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_categorical_data": "```plaintext\n[Explanation]\nThe test calls task_func on a categorical Series. The function's first subplot uses sns.countplot(data=data, ...), which may work, but the second subplot for categorical data draws a horizontal bar chart via ax2.barh(...). Bar charts add Rectangle patches (accessible via ax.patches), not Artist collections. The unit test asserts that fig.axes[1].collections has length > 0, which will be false for the barh approach. Therefore the test's expectation is not met and the code will fail the test.\n\n[Result]\nFAIL\n```", "test_mixed_data": "```plaintext\n[Explanation]\nThe function creates a Figure with two subplots and, for a numeric column, uses seaborn.histplot(kde=True) on the first axis (producing rectangle patches) and seaborn.boxplot on the second axis (producing Line2D objects). The test checks the figure type, number of axes, that the first axis has patches, and that the second axis has lines \u2014 all of which are satisfied by this implementation. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_column": "```plaintext\n[Explanation]\nThe test calls task_func with a DataFrame and a non-existent column name and expects an exception. The implementation checks \"if col not in df.columns: raise ValueError(...)\", so it will raise a ValueError which satisfies self.assertRaises(Exception). No other errors occur in this test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe function checks df.empty and raises ValueError(\"Input df cannot be empty\"). The test creates an empty DataFrame and uses assertRaises(ValueError) around task_func(empty_df, 'empty'), so the expected exception is raised. plt.close() is available because the submitted code imports matplotlib.pyplot as plt at module scope, so the subsequent plt.close() call will succeed. Therefore the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/678", "raw_responses": {"test_basic_operation": "```plaintext\n[Explanation]\nThe function correctly creates the directory and a \"processed\" subdirectory, reads all .json files sorted alphabetically, converts each JSON list-of-dicts to a DataFrame adding a 'source' column, moves processed files into the processed folder, concatenates the DataFrames, and returns the result. The test creates two JSON files each with two records, so the returned DataFrame will have 4 rows. The test only asserts len(df) == 4, which the implementation satisfies.\n\n[Result]\nPASS\n```", "test_empty_directory": "```plaintext\n[Explanation]\nThe test creates an empty directory and calls task_func. The function creates a \"processed\" subdirectory, finds no .json files (json_files empty), and returns an empty pandas DataFrame. The test asserts df.empty which will be True. There are no missing imports (os/json/shutil were imported in the module), and cleanup removes the directory. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_non_json_files": "```plaintext\n[Explanation]\nThe function only processes files ending with '.json'. The test creates 'a.json' with one record and 'b.txt' which is ignored. task_func will read a.json, produce a DataFrame with one row, and return it. Therefore len(df) == 1 and the assertion passes.\n\n[Result]\nPASS\n```", "test_single_file": "```plaintext\n[Explanation]\nThe function reads JSON files in the given directory, converts them to DataFrames, adds a source column, moves files to a processed subdirectory, and returns the concatenated DataFrame. For the test, a single 'a.json' with one record is created. The code will find and read that file, produce a DataFrame with one row, and return it. The top-level imports (json, os, shutil, pandas) are present so the test's helper can use os/json/shutil as well. Therefore len(df) will be 1 and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_with_empty_json_file": "```plaintext\n[Explanation]\nThe function reads 'a.json' containing an empty list, creates an empty DataFrame, adds a 'source' column (still zero rows), moves the file, concatenates the single empty DataFrame and returns it. The returned DataFrame is empty, so df.empty is True. No exceptions are raised.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/897", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function seeds Python's random module when provided, generates exactly 100 choices from 1..6 so the returned frequency array has length 6 and sums to 100. It creates and returns a matplotlib Axes with title and x/y labels set. All assertions in the test (length, sum, isinstance, and label/title checks) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nFor rolls=0 the list comprehension produces an empty results list, so frequencies = [results.count(i) for i in NUMBERS] yields six zeros and numpy array matches [0,0,0,0,0,0]. The function creates an Axes, sets the title and axis labels as required. The seed handling does not affect the zero-roll case. All asserted values match.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function seeds Python's random module, uses random.choice on the fixed list [1..6] for 100000 trials, counts exact occurrences and sets the specified axis title/labels. These operations are deterministic with seed=3, so the returned frequency array and axis labels match the test's expected values.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function seeds Python's random module with seed=4, performs one choice from NUMBERS=[1,2,3,4,5,6], and counts frequencies. With seed=4 the first choice yields the value 2, so the frequency array becomes [0,1,0,0,0,0]. The code also sets the histogram title and axis labels exactly as the test expects.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function seeds Python's random module when seed is provided, generates exactly 'rolls' choices from the 1-6 list, and computes frequencies whose sum equals the number of rolls. It sets the title, x-label, and y-label to the exact strings checked. Therefore the three assertions in the test should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/985", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly parses the JSON string, validates entries, builds a DataFrame with Country and Population columns, writes the CSV with index=False, and returns the file path and DataFrame. The test reads the same CSV and compares DataFrames; types, column names, order, and values match for the provided input. No errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe input json_data is '{}' which json.loads parses as an empty dict. The function checks \"if not data:\" and raises ValueError(\"JSON data is empty\"). The test expects a ValueError for this input, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nAll three calls in the test raise ValueError as expected:\n- First: JSON is valid but missing the \"Countries\" key -> raises ValueError.\n- Second: Top-level keys are country names, still missing \"Countries\" -> raises ValueError.\n- Third: Malformed JSON -> json.loads raises JSONDecodeError, caught and re-raised as ValueError.\n\nTherefore the code meets the test expectations.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function calls os.makedirs(output_dir, exist_ok=True) before writing the CSV. Given the test provides a valid JSON ('{\"Countries\": {\"Country A\": 1000}}'), the JSON parsing and validation succeed, the directory will be created, and no exceptions are raised. The test only checks that the new directory exists after calling task_func, so the code will satisfy this.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe JSON includes Unicode country names and integer populations. task_func uses json.loads (which supports Unicode), validates types, builds a DataFrame with \"Country\" and \"Population\" columns, saves the CSV to the provided temporary directory, and returns the file path and DataFrame. The test only checks the DataFrame format, presence of the two country names in df.Country, and that the CSV file exists \u2014 all of which the code satisfies.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe input JSON has \"Countries\" as an empty dict. The function builds validated_data by iterating countries_data.items() \u2014 with no items, validated_data stays empty. pd.DataFrame(validated_data) creates an empty DataFrame with no columns, so the test's check for 'Country' and 'Population' in df.columns fails. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function validates that each population value is an int or float and raises ValueError otherwise. In all three test calls at least one population is a string or null (None) or negative; the function will detect a non-numeric value (or negative) and raise ValueError before writing a file. Therefore each with self.assertRaises(ValueError) block will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_8": "```plaintext\n[Explanation]\nThe function correctly parses the JSON, validates that the country name is a string and population is a non-negative number (0 is allowed), builds a DataFrame, writes the CSV to the provided temporary directory, and returns the file path and DataFrame. For the test input '{\"Countries\": {\"Uninhabited Island\": 0}}' all checks pass and the DataFrame contains the expected row and population value.\n\n[Result]\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function correctly parses the JSON, uses math.floor on float populations, creates a DataFrame with \"Country\" and \"Population\" columns, writes the CSV to the provided directory, and returns (file_path, df). For the given input it will floor 1234567.89 -> 1234567 and 98765.432 -> 98765, and the CSV file will exist. All assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/208", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function seeds numpy with the provided seed and generates a deterministic sequence of \u00b11 steps, computes the cumulative random walk, and returns the statistics. The test also sets the same seed and calls task_func(100, seed=0), so the produced statistics match the expected values (numeric types and precision are compatible with assertAlmostEqual).\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\ntask_func(50, seed=42) generates 50 steps, computes a cumulative sum (length 50), and calls ax.plot(random_walk). The plotted line's y-data is the random_walk array, so ax.lines[0].get_ydata() has length 50. The test's assertion len(y_data) == 50 will succeed.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function raises ValueError for non-positive integers, so task_func(0) triggers the expected exception. For task_func(1) it generates a single-step random walk and plots it; ax.lines[0].get_ydata() will have length 1. Therefore the test assertions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a dict 'stats' and a matplotlib Axes 'ax'. Calling task_func(10) with default seed runs without errors, 'stats' contains keys 'mean' and 'std', and the returned ax is an instance of matplotlib.axes.Axes. Therefore the test assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function generates a random walk of length `elements`, plots it on `ax` via `ax.plot(random_walk)`, and sets the title to 'Random Walk'. For elements=100 the plotted y-data length will be 100 and the title matches exactly. There are no errors in creating the Axes or plotting that would affect these assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/93", "raw_responses": {"test_transformed_data_shape": "```plaintext\n[Explanation]\nThe function performs PCA with n_components=2 on a 10x2 DataFrame, returns a DataFrame created from the PCA-transformed array. For the test input the PCA fit_transform produces a (10,2) array and transformed_df will therefore have shape (10,2). There are no errors in argument validation, plotting, or return values that affect this test.\n\n[Result]\nPASS\n```", "test_invalid_n_components": "```plaintext\n[Explanation]\nThe function checks \"if not isinstance(n_components, int) or n_components <= 0\" and raises ValueError. The test calls task_func(..., 0), so the condition is true and ValueError is raised as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_n_components_type": "```plaintext\n[Explanation]\nThe test passes a string 'two' for n_components. The function immediately checks isinstance(n_components, int) and n_components > 0, and will raise ValueError for a non-int input. That matches the test's expected exception, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_plot_axes": "```plaintext\n[Explanation]\nThe function calls ax.scatter exactly once when n_components >= 2, which adds a single PathCollection to ax.collections. Given the test uses n_components=2 and numeric DataFrame input, PCA and the scatter call will succeed, so len(ax.collections) will be 1.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_values": "```plaintext\n[Explanation]\nThe test's setUp() creates self.data using np.random.rand(10) before the test method seeds the RNG. The np.random.seed(42) call inside test_values() happens too late, so the input data passed to task_func differs from the dataset used to produce the hard-coded expected PCA output. Although the test allows an overall sign flip, it still requires the PCA results to match the expected values for that specific seeded dataset. Therefore the produced transformed_data will not match the expected values and the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/346", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test creates script1.py that exits with code 0. task_func verifies the file exists, builds a command [sys.executable, script_path], runs it with subprocess.run(check=True...), and returns result.returncode. The script returns 0, so task_func returns 0 and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test runs script2 which exits with code 1. task_func calls subprocess.run(..., check=True) which raises subprocess.CalledProcessError on non-zero exit; the function catches and re-raises it instead of returning the process return code. Therefore the function will raise an exception and not return 1, so the assertion expecting return_code == 1 will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe test runs script3 which prints its argv and exits with code 0. task_func constructs a command [sys.executable, script_path, 'arg1', 'arg2'] and calls subprocess.run with check=True, capturing output and returning result.returncode. The script exists (created in setUp), returns 0, so task_func returns 0 and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func(..., False) for a script that sleeps 2 seconds. The implementation checks the file exists, starts the process with subprocess.Popen(cmd) when wait is False, and returns None. That matches the test's assertion self.assertIsNone(return_code). Starting a subprocess and returning immediately will not raise an exception here, so the test should pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test invokes task_func on a script that raises an unhandled Exception. task_func runs the script via subprocess.run(..., check=True), so a non-zero exit due to the unhandled exception will cause subprocess.CalledProcessError to be raised. The function re-raises that error, matching the test's assertRaises expectation. The required modules (os, subprocess) are imported at top-level of the provided code, so the test's use of those names succeeds. Therefore the test should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/477", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates 100 x and y values in [0,1) and a category array using np.random.choice with seed=42. DataFrame shape and column order are correct, x/y ranges hold, and ax is a matplotlib Axes. The only potential issue is that sampling with replacement does not guarantee all categories appear, but with N=100 and the fixed seed (42) the random draw will include all five categories (probability of missing any category is effectively zero). Therefore the test assertions should all pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function returns a DataFrame with columns ['x','y','category'] and generates 50 random x and y in [0,1). It samples categories with replacement when N >= len(CATEGORIES); although that does not mathematically guarantee every category appears, with seed=42 and 50 draws from two categories both 'X' and 'Y' will appear (probability of only one category is essentially zero). The plot Axes is created by plt.subplots so isinstance(ax, plt.Axes) is True. All asserted conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function creates x, y, and category arrays of length N and constructs a DataFrame from them. For the tested N values (5, 10, 50, 200) the code uses np.random.choice with size=N (replace=True), so the 'category' array length matches N. The DataFrame will have three columns and N rows for each tested N. Plotting calls do not affect the DataFrame and are safe in a headless test environment. Therefore the shape assertions will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function uses np.random.choice(..., replace=True) when N >= len(CATEGORIES), which in general does not guarantee every category appears. However, for the provided test cases N defaults to 100 and the RNG seed is fixed to 42. With 100 draws and seed 42, each of the given category lists (lengths 2, 3, and 1) will have all their categories present in the sampled array, so set(df['category']) equals set(C) for each case. No other errors affect the test.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function sets numpy's RNG with np.random.seed(seed) at the start, then generates x, y and categories using numpy random calls. Calling task_func with the same seed resets the RNG and produces identical arrays and category assignments, so df1 equals df2. A different seed produces different random values, so df3 differs. The plotting code does not modify the DataFrame. Therefore the assertions in the test will hold.\n\n[Result]\nPASS\n```", "test_case_6": "", "test_case_7": "```plaintext\n[Explanation]\nFor N=3 and 4 categories, the code takes the else branch (N < len(CATEGORIES)) and uses np.random.choice with replace=False to select 3 distinct categories. The DataFrame has 3 rows and the sampled categories are unique, so both assertions (len(df)==3 and 3 unique categories) hold. No runtime errors are triggered by plotting in this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nWith N=50 and CATEGORIES=['X'], the code takes the branch N >= len(CATEGORIES) and calls np.random.choice(..., replace=True), which will produce an array of 50 'X' values. The DataFrame's 'category' column will therefore be all 'X', satisfying the test assertion.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_9": "```plaintext\n[Explanation]\nThe function samples 50 categories with replacement from [1,2,3] using a fixed seed (42). Although sampling with replacement does not mathematically guarantee every category appears, with 50 draws the probability that any category is missing is effectively zero, and the deterministic seed 42 produces draws that include all three values. The DataFrame's 'category' column therefore contains {1,2,3}, so the assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/567", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly parses the string into integers, creates the histogram with bins np.arange(1, 7) - 0.5 (i.e. 0.5..5.5), and sets the title and axis labels as required. Matplotlib will place reasonable integer tick marks at 1..5 within the 0.5\u20135.5 x-limits, so ax.get_xticks() will yield [1.0,2.0,3.0,4.0,5.0], which compares equal to [1,2,3,4,5] in the test. Thus all assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function sets bins to [4.5, 5.5] for input '5-5-5-5-5', so matplotlib will typically place x-ticks at the bin edges and/or midpoint (e.g. [4.5, 5.0, 5.5]) rather than a single tick [5]. While the labels and title match, the xticks assertion expecting [5] will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function sets bins for the single value '7' to [6.5, 7.5] and does not set x-ticks explicitly. While labels and title match, Matplotlib will choose automatic ticks for the x-axis (likely [6.5, 7.0, 7.5] or [6.5, 7.5]) rather than a single tick [7]. The test expects ax.get_xticks() to equal [7], so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function sets title and axis labels correctly, so those assertions pass. However, the test asserts that the x-tick locations equal the specific data values [1, 2, 4, 8, 10]. The code never sets xticks to the raw data; matplotlib's automatic tick locator will produce a different set (e.g. integer range or evenly spaced ticks), not exactly the data values. Therefore the xticks assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a histogram and sets title/labels correctly, but it does not set x-ticks. Matplotlib will choose automatic tick locations for the x-axis (likely [0.0, 50.0, 100.0, 150.0] or other rounded values) rather than exactly [1, 50, 100, 150]. The test asserts the exact xticks, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/720", "raw_responses": {"test_return_value": "```plaintext\n[Explanation]\nThe test's setUp creates task_func_data/Output.txt with a header row (non-empty). task_func() creates the same directory, opens the existing file in append mode, checks file size (non-zero) so it does not re-write the header, appends 10 data rows, and returns the file path string 'task_func_data/Output.txt'. This matches the test's expected return value.\n\n[Result]\nPASS\n```", "test_file_existence": "```plaintext\n[Explanation]\nThe test's setUp creates 'task_func_data/Output.txt' with a header. task_func() appends 10 rows to that file and does not remove it. The code checks file size before writing header, so it won't overwrite the existing file. No exceptions or incorrect operations occur, so the file will exist after calling task_func().\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_content": "```plaintext\n[Explanation]\nThe test expects the first data row's humidity to be between 50 and 60. The code generates humidity with random.uniform(40.0, 80.0), so many generated values will fall outside 50-60. Because the value is random and not constrained to 50-60, the assertion will often fail. Other aspects (header, temperature range) match the test setup, but the humidity range mismatch causes the test to fail.\n\n[Result]\nFAIL\n```", "test_data_appending": "```plaintext\n[Explanation]\nThe test's setUp creates a file with a single header line. task_func() appends 10 data rows each time (writer.writerows(data) with data of length 10) and only writes a header if the file is empty. After the first call the file will have 1 + 10 = 11 lines; after the second call 11 + 10 = 21 lines. The test expects the final count to be initial_count + 1, but it's actually initial_count + 10, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_headers_only_once": "```plaintext\n[Explanation]\nThe test pre-creates task_func_data/Output.txt with the header row. task_func() opens the file in append mode and only writes the header when os.stat(file_path).st_size == 0. Since the file already contains the header (size > 0), subsequent calls append data but do not add another header. Calling task_func() twice therefore leaves exactly one header row, so the assertion len(headers) == 1 will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/906", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function checks filename.endswith('_processed'), but the test file is named 'file2_processed.txt' (ends with '.txt'), so it will not be added to the archive. The archive will not contain 'file2_processed.txt', causing the assertion to fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function zips only files whose names end with '_processed'. The test creates 'file1.txt' and 'file3.txt' (none end with '_processed'), so the created archive will contain zero entries. The code creates the archive (possibly empty) and returns its path; reading it yields an empty namelist as asserted. Therefore the test passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test creates empty source and target directories and calls task_func. The function creates an archive in the target directory and only adds files whose names end with '_processed'. With no files present, the created ZIP will contain zero entries, so archive.namelist() is empty. The assertion expecting length 0 will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function returns os.path.join(target_dir, archive_name), so the returned path will contain 'custom_archive.zip' as asserted by the test. (Even though the code's filename check uses endswith('_processed') and won't add 'file2_processed.txt' to the archive, an archive file is still created with the requested name.)\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function creates the target directory, builds archive path as os.path.join(target_dir, 'archive.zip'), opens a zip file (even if source is empty) and returns that path. The test sets up empty source and target dirs and expects that exact path. There are no missing imports for the test because os/shutil were imported at top of the code. The returned path will match expected_path.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/988", "raw_responses": {"test_file_is_file": "```plaintext\n[Explanation]\nThe test creates several files in a temporary directory and calls task_func with the single predicate 'is_file'. The implementation lists directory items and uses os.path.isfile on each item path, so each created file will be reported True for 'is_file'. Predicate filtering and result structure match the test's expectations, so the assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_file_is_not_dir": "```plaintext\n[Explanation]\nThe test creates four files (not directories) in a temporary directory and calls task_func with the single predicate 'is_dir'. The function checks each item with os.path.isdir, which returns False for those files, and returns entries for all created filenames. The result entries contain exactly the requested predicate. No errors or edge cases apply here.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_dir_is_dir": "```plaintext\n[Explanation]\nThe function checks existence of the temp directory, lists its items, and evaluates 'is_dir' using os.path.isdir on each item path. The test creates the directories 'somedir' and 'aDirectory123' in a fresh TemporaryDirectory and calls task_func with ['is_dir']. The results will contain those names with the 'is_dir' predicate set True and the inner dicts will have the expected single key. No errors or edge-cases apply here.\n\n[Result]\nPASS\n```", "test_dir_is_not_file": "```plaintext\n[Explanation]\nThe function lists items in the temporary directory and evaluates only the 'is_file' predicate. The test creates two directories inside the temp dir, so os.path.isfile(...) will be False for each, matching the test's assertions. No errors are raised by the directory checks or predicate handling.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_has_special_char": "```plaintext\n[Explanation]\nThe function checks filenames (without extensions) for non-alphanumeric/underscore characters using a regex. The test creates three files: 'fi!e', 'fi@', and 'f.ile.txt'. For each, os.listdir returns the exact names, and os.path.splitext('f.ile.txt')[0] yields 'f.ile', which contains '.' so the regex matches. The function produces a dict entry per file with only the 'has_special_chars' key set to True. All assertions in the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_has_no_special_char": "```plaintext\n[Explanation]\nThe test creates four files: 'file_', '_file', 'file.txt', 'some_file.txt'. task_func checks 'has_special_chars' by stripping the extension and searching for characters not in [a-zA-Z0-9_]. All four names contain only letters and underscores (no forbidden characters), so the function will set has_special_chars to False for each file. The result dict entries exist and contain exactly that single predicate, so all assertions in the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_has_numbers": "```plaintext\n[Explanation]\nThe function checks 'has_numbers' using re.search(r'\\d', item) on the filename (including extension), and it will return True for '123', '123.txt', 'text123', and 't1e2x3t4'. The temporary directory is created empty, the test files are created, and result contains those filenames as keys with a single predicate entry 'has_numbers'. No errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_multiple_predicates": "```plaintext\n[Explanation]\nThe function correctly lists the temp directory, evaluates all four valid predicates for the created file 'test1!.txt' (is_file True, is_dir False, has_special_chars True because of '!', has_numbers True because of '1'), and returns a dict keyed by the filename. No errors or mismatches occur for this test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_deduplicate_predicates": "```plaintext\n[Explanation]\nThe test creates a single file \"test_file\" and calls task_func with duplicated predicate ['is_file', 'is_file']. The function deduplicates predicates, validates 'is_file', lists the directory, and returns {'test_file': {'is_file': True}}. The test assertions check that exactly one item is returned and that the 'is_file' predicate is present and accounted for; both conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_predicates": "```plaintext\n[Explanation]\nThe test calls task_func with an existing temporary directory and an empty predicate list. The function deduplicates and filters predicates, resulting in an empty valid_predicates_list, and then raises ValueError(\"No valid predicates provided.\"). This matches the test's expectation. No other checks (like missing directory) interfere.\n\n[Result]\nPASS\n```", "test_invalid_predicates": "```plaintext\n[Explanation]\nThe test calls task_func with an existing temporary directory and predicates ['foo','bar'].\nThe function filters predicates against the allowed set and raises ValueError when none are valid.\nNo earlier checks prevent reaching that raise, so the expected ValueError is produced.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_nonexistent_directory_error": "```plaintext\n[Explanation]\nThe test calls task_func('nonexistent_dir', ['is_file']) expecting a FileNotFoundError. The function immediately checks os.path.exists(dir_path) and raises FileNotFoundError when the path does not exist, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1057", "raw_responses": {"test_default_input": "```plaintext\n[Explanation]\nThe function uses default lists of 10 animals and 7 foods when no arguments are provided, builds a 10x7 DataFrame, and shuffles rows/columns which does not change its shape. The test only checks the shape (10,7), so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_custom_input": "```plaintext\n[Explanation]\nThe function builds a DataFrame with one row per provided animal and one column per provided food, then shuffles rows and columns using numpy's RNG. For the test inputs (3 animals, 4 foods) the DataFrame shape will remain (3, 4). The test seeds Python's random, but the code uses numpy.random; however the test only asserts shape, which is unaffected by randomness. No errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_input": "```plaintext\n[Explanation]\nThe test passes empty lists for animals and foods expecting an empty DataFrame shape (0,0). The function treats empty lists (len == 0) as missing and replaces them with default lists, producing a DataFrame with shape (10,7) instead of (0,0). Therefore the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_single_input": "```plaintext\n[Explanation]\nFor the inputs animals=['Dog'] and foods=['Meat'], the function builds a 1x1 DataFrame with the single entry \"Dog:Meat\". The row/column shuffling uses numpy but with size 1 has no effect. The test checks the shape is (1,1) and that 'Dog:Meat' appears in result.values; both conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_partial_default": "```plaintext\n[Explanation]\nThe function receives animals=['Dog','Cat','Elephant'] and uses the default foods list of length 7. It constructs a DataFrame with one row per animal and one column per food, so initial shape is (3,7). The subsequent row/column shuffles do not change the DataFrame shape. No errors are raised, so the asserted shape (3,7) will match.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/211", "raw_responses": {"test_download_and_extract": "", "test_2": "", "test_3": "```plaintext\n[Explanation]\nThe function calls os.makedirs(destination_directory, exist_ok=True) with the absolute path '/path/to/destination' from the test. The test does not patch os.makedirs, so the code will attempt to create directories at root, which will likely raise a PermissionError in the test environment before reaching the mocked requests and open calls. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_4": "```plaintext\n[Explanation]\nThe test patches os.path.join (a MagicMock) but does not set its return to a real path string. The code uses that mocked join result as zip_filename and later calls os.remove(zip_filename). Passing a MagicMock to os.remove triggers a TypeError because os.remove expects a str/bytes/os.PathLike. This causes the test to error/fail. Other assertions (requests.get and open call) would pass because the same mocked join return_value is used, but the unmocked os.remove breaks the run.\n\n[Result]\nFAIL\n```", "test_5": "```plaintext\n[Explanation]\nThe function calls os.makedirs(destination_directory, exist_ok=True) with destination '/path/to/destination' which the test did not patch; attempting to create '/path' will likely raise a PermissionError in the test environment. Even if that succeeded, the code uses the filename \"temp_download.zip\" when calling os.path.join, while the test expects an open call with a path ending in 'data_download.zip' (the test patches basename to 'data_download.zip'). These mismatches cause the test to fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/360", "raw_responses": {"test_normal_functionality": "", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test calls task_func('non_existent.xlsx', 'Sheet1') and expects a FileNotFoundError. The implementation checks os.path.exists(file_location) at the start and raises FileNotFoundError if the file does not exist, so the function will raise the expected exception. No other parts of the code are executed for this test, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_sheet_name": "```plaintext\n[Explanation]\nThe test calls task_func with a non-existent sheet. The function checks file existence (file created by setUp) and calls pd.read_excel inside a try/except that catches ValueError and re-raises a ValueError with a custom message. Therefore a ValueError will be raised as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_correct_mean_and_std_values": "```plaintext\n[Explanation]\nThe test expects population standard deviation (10.0) for columns A and B, but the code uses pandas Series.std() which defaults to sample standard deviation (ddof=1) and returns ~14.1421356237 for the two-value columns. Therefore the resulting dictionary will not match the expected values.\n\n[Result]\nFAIL\n```", "test_bar_chart_labels": "```plaintext\n[Explanation]\nThe function sets ax.set_xlabel('Columns') and ax.set_ylabel('Values') and returns the figure. The test creates 'test.xlsx' with sheet 'TestSheet', calls task_func, and asserts those axis labels \u2014 all conditions are met. No other behavior in the code interferes with the assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_value": "```plaintext\n[Explanation]\nThe code uses pandas.Series.std() which defaults to sample standard deviation (ddof=1). For columns [10, 30] and [20, 40] that yields ~14.1421, but the test expects population standard deviation 10.0. Therefore the returned result dict will not match the expected values and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/486", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a matplotlib Axes via plt.subplots(), returns that Axes, and sets the x/y labels to 'Time' and 'Value'. The unit test checks isinstance(ax, plt.Axes) and the two labels \u2014 all conditions are satisfied by the code.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function sets numpy's RNG seed at the start (np.random.seed(seed)) and then draws the same number of normal samples in the same order, so repeated calls with identical parameters and seed produce identical 'Value' arrays. The test compares the plotted y-data arrays with np.array_equal, which will be True. No other nondeterministic behavior affects the y-data.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function seeds NumPy's RNG at the start of each call and generates random normal values added to a deterministic trend. Calling with seed=42 and seed=52 therefore yields different random_values (and thus different y-data). The test checks that the two y-data arrays are not exactly equal, so the assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function returns the Axes object created by plt.subplots() (a matplotlib.axes._axes.Axes), and the test checks isinstance(ax, plt.Axes) which is True for that object. The input parameters (start=0, end=10000, step=100, trend=-0.001) produce valid time points and values; numpy and pandas usage is correct and plotting will succeed. Therefore the test's assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a matplotlib figure and returns the Axes object (fig, ax = plt.subplots(...); return ax). For the provided inputs (start=0, end=10000, step=100, trend=0.0) it generates valid timestamps and values without errors. The returned object is an Axes (or subclass), so isinstance(ax, plt.Axes) will be True.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function does not raise an exception when start_time > end_time. range(start, end+1, step) produces an empty list, leading to empty arrays and an empty DataFrame; matplotlib accepts empty plotting data and the function returns an Axes object. The test expects an Exception, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_7": "```plaintext\n[Explanation]\nThe function calls range(start_time, end_time + 1, step) with step=0, which raises a ValueError (\"range() arg 3 must not be zero\"). The test expects an exception, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function returns an Axes with datetime x-values, but matplotlib does not populate tick label text until the figure is rendered (a draw is performed). The test inspects ax.get_xticklabels() immediately and checks their text for '1970'. Without calling fig.canvas.draw() or otherwise forcing a draw, the tick Text objects typically have empty text, so the test's condition will not be satisfied.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/574", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns a matplotlib Axes, plots the noisy data with scatter (which does not add to ax.lines) and two plotted curves (fitted and original), sets the x/y labels, and calls ax.legend(). curve_fit should succeed with the given default signal, so all assertions (instance type, len(ax.lines) == 2, labels, and legend presence) will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a Matplotlib Axes object (ax), so isinstance(ax, plt.Axes) passes. ax.lines[0] is the first plotted Line2D (the fitted curve), which uses the x array of length array_length (150) so the length check is satisfied. The maximum consecutive difference of the fitted sine samples is approximately amplitude*frequency*dx; with dx = 4\u03c0/(150-1) \u2248 0.084, typical fitted amplitude and frequency near 1 give max diff \u226a 1.1, so the inequality holds. The code uses scipy.optimize.curve_fit but under normal test environment SciPy is available; no other errors are expected.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function returns a Matplotlib Axes and plots two Line2D objects (fitted curve then original sine). ax.lines[0] therefore yields the fitted y-values. The fitted curve is a smooth sine model sampled densely (100 points across 4\u03c0), so adjacent differences are small (roughly derivative*dx \u226a 3). curve_fit with the given initial guess will generally recover reasonable sine parameters even with noise_level=2.0, so the test condition np.max(np.abs(np.diff(y_data))) <= 3.0 is satisfied. The return type check (isinstance(ax, plt.Axes)) also passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_varying_noise_levels": "```plaintext\n[Explanation]\nThe module-level import \"from scipy.optimize import curve_fit\" will raise ImportError if SciPy is not available in the test environment. That prevents task_func from being defined/executed and the unittest will fail before reaching the assertion. Therefore the test is expected to fail.\n\n[Result]\nFAIL\n```", "test_plot_outputs": "```plaintext\n[Explanation]\nThe function depends on SciPy (from scipy.optimize import curve_fit) which is not guaranteed to be available in the test environment. If scipy is missing the import will raise ImportError and the test will error out. Even if scipy is present, the function returns a matplotlib Axes object with a 'figure' attribute, so the test assertion would pass. Because the presence of SciPy is uncertain and typically unavailable in minimal environments, this code is likely to fail the provided test.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/723", "raw_responses": {"test_correct_scraping": "```plaintext\n[Explanation]\nThe test patches csv.writer and expects writerows to be called on that mock. The code uses csv.DictWriter (not csv.writer) and calls writeheader() and writer.writerow(data) once \u2014 it never calls writerows, nor does it invoke the patched csv.writer. Therefore the test's mock_writer.writerows.assert_called() will fail.\n\n[Result]\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise Exception('Invalid URL'). task_func calls urllib.request.urlopen and does not catch exceptions, so the injected exception will propagate. The test uses assertRaises(Exception) around task_func('invalid_url'), so the behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_table": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to return a mock whose read() returns the provided HTML bytes. The function calls urllib.request.urlopen (which is patched), parses the HTML with BeautifulSoup, handles missing <title> by using \"No Title\", finds no <p> tags (producing an empty paragraphs string), writes 'scraped_data.csv', and returns the CSV file path. No operations in the code will raise an exception for the given input, so the returned value matches the expected 'scraped_data.csv'.\n\n[Result]\nPASS\n```", "test_no_table": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to return HTML with a paragraph and then asserts that task_func raises an Exception. The provided task_func parses the HTML, writes a CSV, and returns 'scraped_data.csv' without raising any exception. Therefore the test's expected exception will not occur and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_overwrite_existing_csv": "```plaintext\n[Explanation]\nThe test expects the code to open the CSV with open(CSV_FILE_PATH, 'w'), to use csv.writer and call writer.writerows with [['New Data']], and to extract table cell content. The implementation:\n- opens the file with additional kwargs (newline='', encoding='utf-8'), so mock_open.assert_called_once_with(CSV_FILE_PATH, 'w') will fail.\n- uses csv.DictWriter (which calls the patched csv.writer internally but calls writer.writerow, not writer.writerows), so mock_writer.writerows is never called.\n- extracts <p> paragraph texts (there are no <p> tags in the test HTML), so it will not produce [['New Data']].\n\nTherefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/914", "raw_responses": {"test_case_1": "", "test_case_2": "```plaintext\n[Explanation]\nThe function converts dates to POSIX seconds for training, fits a LinearRegression on perfectly linear daily prices, then predicts for the next 7 days (last_date + 1..7 days). The timestamp conversion used for training (astype('int64') // 1e9) and for prediction (timestamp()) are consistent, and the data has an exact +1 per day pattern, so the model will extrapolate to [207.0, 208.0, ..., 213.0]. The returned list will match the expected floats.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function fits a linear model on timestamps (in seconds) vs closing_price for 7 consecutive days with prices increasing by 1 each day. It then predicts the next 7 days by adding whole-day Timedeltas, so the model will produce successive +1 increments. The data is perfectly linear, so LinearRegression will predict [307.0, ..., 313.0]. The conversions to/from timestamps are consistent for pandas Timestamps in a typical test environment, so the returned floats match the expected list exactly.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "", "test_case_5": ""}}
{"id": "BigCodeBench/990", "raw_responses": {"test_hex_string_sample": "```plaintext\n[Explanation]\nThe input hex \"4a4b4c\" decodes to bytes b'JKL' -> \"JKL\". base64 of b'JKL' is \"SktM\". UTF-8/UTF-16/UTF-32 and ASCII round-trips yield \"JKL\". urllib.parse.quote with safe='' does not percent-encode alphanumerics, so URL is \"JKL\". ROT13 of \"JKL\" is \"WXY\". All assertions match the function's outputs.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_hex_string_1": "```plaintext\n[Explanation]\nThe input hex '68656c6c6f' decodes to b'hello'. The function produces base64 'aGVsbG8=', utf-8 'hello', utf-16 and utf-32 roundtrip to 'hello', ASCII decoding succeeds, URL quoting leaves 'hello', and ROT13 yields 'uryyb'. All expected values in the test match the function's outputs.\n\n[Result]\nPASS\n```", "test_hex_string_2": "```plaintext\n[Explanation]\nThe hex string '776f726c64' decodes to b'world'. The function decodes hex to UTF-8 correctly, base64-encodes the bytes to 'd29ybGQ=', and the UTF-16/UTF-32 encode-then-decode roundtrips return 'world'. ASCII decoding succeeds, URL quoting leaves alphanumerics unchanged, and codecs.encode(..., 'rot13') produces 'jbeyq'. All asserted values match.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_hex_string_3": "```plaintext\n[Explanation]\nThe input '616263' unhexlifies to b'abc'. base64.b64encode(b'abc') -> 'YWJj'. Decoding/round-tripping with utf-16 and utf-32 (which include BOM) returns 'abc'. ASCII decoding, URL quoting of 'abc', and codecs ROT13 on 'abc' produce 'abc', 'abc', and 'nop' respectively. All asserted values match, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_hex_string_4": "```plaintext\n[Explanation]\nThe function decodes '313233' to bytes b'123' and then to UTF-8 string '123'. Base64 of b'123' is 'MTIz'. UTF-16/UTF-32 branches encode then decode back to the original string, yielding '123'. ASCII decoding, URL quoting, and ROT13 on digits all leave '123' unchanged. All expected values in the test match the function outputs.\n\n[Result]\n```plaintext\nPASS\n```", "test_hex_string_non_ascii": "```plaintext\n[Explanation]\nFor input 'c3a9' the code unhexlifies to bytes b'\\xc3\\xa9' which decodes to UTF-8 '\u00e9'. base64.b64encode yields 'w6k='. The utf-16 and utf-32 encode/decode roundtrips return '\u00e9'. ASCII decoding raises UnicodeDecodeError so it returns 'Not representable in ASCII'. urllib.parse.quote on '\u00e9' yields '%C3%A9'. codecs.encode with rot13 leaves non-ASCII '\u00e9' unchanged. All asserted values match the expected outputs.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1077", "raw_responses": {"test_example_case": "```plaintext\n[Explanation]\nThe code parses the timestamps (milliseconds accepted by %f), localizes all to UTC then converts to the same timezone \u2014 which preserves intervals. The consecutive differences are 61s and 61s, so the mean is 61.0. numpy.float64 return is acceptable for assertAlmostEqual. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_different_timezones": "```plaintext\n[Explanation]\nThe function parses the given timestamps (year '21' -> 2021), localizes them as UTC, converts to the requested timezone, and computes consecutive differences. Timezone conversion does not change intervals, so differences are 61 seconds each and the mean is 61.0. numpy and pytz usage is correct and types match assertAlmostEqual.\n\n[Result]\nPASS\n```", "test_varying_differences": "```plaintext\n[Explanation]\nThe code treats the input datetimes as UTC, converts all to Asia/Tokyo (same offset applied to all), so differences are unchanged. Differences are 61s and 122s, mean = (61+122)/2 = 91.5. The parsing, pytz timezone, and numpy mean usage are correct, so the test will pass.\n\n[Result]\nPASS\n```", "test_single_time_string": "```plaintext\n[Explanation]\nThe function checks if len(time_strings) < 2 at the start and returns 0.0 in that case. The test provides a single timestamp, so the function returns 0.0 which matches the expected value.\n\n[Result]\nPASS\n```", "test_span_across_days": "```plaintext\n[Explanation]\nThe function parses the two timestamps as UTC datetimes, converts them to Asia/Tokyo, and computes differences between consecutive times. Converting both timestamps by the same fixed offset (Asia/Tokyo is UTC+9, no DST) preserves the interval, so the difference is 2 minutes = 120.0 seconds. numpy.mean returns 120.0, matching the expected value.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_out_of_order_strings": "```plaintext\n[Explanation]\nThe function computes signed consecutive time differences (converted from UTC to the target timezone), not absolute differences. For the input order ['12:02:02', '12:00:00', '12:01:01'] the consecutive diffs are -122 and 61 seconds, whose mean is -30.5. The test expects the mean of absolute differences (|\u2212122| and |61|) = 91.5. Therefore the code will not match the expected output.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/214", "raw_responses": {"test_image_size_and_type": "```plaintext\n[Explanation]\nThe function seeds numpy with 42, generates a (20,20,3) array via np.random.randint(0, 256, ... , dtype=np.uint8), and returns it. The test calls task_func with the default seed and checks shape, dtype, and exact contents. The code uses range_high+1 correctly (256) so randint produces values 0-255. There are no other modifications to the array, and matplotlib figure creation does not alter the returned data. Therefore the generated image will match the expected array for seed 42.\n\n[Result]\nPASS\n```", "test_random_seed_reproducibility": "```plaintext\n[Explanation]\nThe function sets both random.seed and np.random.seed to the provided seed and then immediately generates the image using np.random.randint. No other operations consume numpy's RNG between calls, so calling task_func(seed=42) twice produces identical arrays. Matplotlib figure creation does not affect the RNG. The test checks array equality and will pass.\n\n[Result]\nPASS\n```", "test_range_values": "```plaintext\n[Explanation]\nThe function correctly generates a uint8 image with values drawn from np.random.randint(range_low, range_high + 1), so pixel values will lie between 100 and 200 inclusive. However, the module-level import \"import cv2\" may raise ImportError if OpenCV is not installed in the test environment, causing the test to fail before the function runs.\n\n[Result]\nFAIL\n```", "test_error_on_invalid_range": "```plaintext\n[Explanation]\nThe function would raise ValueError for range_low >= range_high as the test expects. However, the module does a top-level \"import cv2\" which will cause an ImportError in environments without OpenCV installed, preventing the test from running. Because that import may fail before the test can call task_func, the code will not reliably pass the test.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_return_types": "```plaintext\n[Explanation]\nThe function itself returns a matplotlib Axes and a numpy array as required. However, the top-level import includes \"import cv2\" which is unnecessary and will raise ModuleNotFoundError in environments where OpenCV is not installed. That import will prevent the test from running, causing failure.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/492", "raw_responses": {"test_case_1": "", "test_case_2": "```plaintext\n[Explanation]\nThe code sets each row's 'Date' to start_date.replace(hour=0, minute=0, second=0, microsecond=0) (midnight of the start day). The test expects the first 'Date' to equal the original datetime.fromtimestamp(three_days_ago/1000.0), which includes the original time-of-day. This mismatch causes the equality assertion to fail. Columns and product uniqueness would pass, but the Date check fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe test passes a float (timestamp()*1000) to task_func, but the function requires epoch_milliseconds to be an int and raises ValueError. Therefore the test will not get a DataFrame. Additionally, even if the input were an int, the function truncates the start date to midnight for the 'Date' column while the test expects the original timestamp-with-time, so the Date comparison would also fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function validates inputs, generates rows with keys in the order 'Product', 'Date', 'Sales', and uses datetime.fromtimestamp for the provided epoch. The DataFrame's first Date equals start_date only if start_date has hour==0. The test compares to datetime.fromtimestamp(1577836800000/1000.0) and the epoch corresponds to 2020-01-01 00:00:00 UTC; in a typical test environment (UTC) start_date.hour == 0 so the saved Date matches. The columns order and product uniqueness checks also pass.\n\nTherefore, under the common UTC environment the test case will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test passes a timestamp one day in the future. The function converts that to a datetime and compares it to datetime.now(); since the start_date will be after end_date, the function raises ValueError as expected. All input validations succeed prior to that check, so the raised exception matches the test's assertRaises.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function validates 'products' and raises ValueError if its length is not exactly 5. The test passes a list of length 4, so task_func will raise ValueError as expected.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function checks that the products list has 5 unique elements; with products=['a','a','b','c','d'] len(set(products)) != 5 so it raises ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/752", "raw_responses": {"test_case_test_size": "```plaintext\n[Explanation]\nThe test calls task_func with test_size values 5, -1, 0, and 1 and expects an exception each time. In the code:\n- test_size 5 and -1 fail the explicit validation (0 <= test_size <= 1) and raise ValueError.\n- test_size 0 bypasses validation but train_test_split(test_size=0) raises an error, so an exception occurs.\n- test_size 1 passes the validation (it's within 0 <= test_size <= 1) and is passed as an integer to train_test_split, which treats 1 as a valid number of test samples (not an invalid fraction). The function runs successfully and returns a score instead of raising an exception.\n\nBecause the call with test_size=1 does not raise, one of the assertRaises checks fails. Therefore the test case will not pass.\n\n[Result]\nFAIL\n```", "test_case_random_state": "```plaintext\n[Explanation]\nThe test calls task_func with non-integer random_state values ('a', [1,2], {'a':2}) as the fourth positional argument. The function validates random_state with isinstance(random_state, int) and raises ValueError for non-integers. These are Exceptions, so each assertRaises(Exception, ...) will succeed. No earlier checks fail for the provided numeric DataFrame and valid test_size.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_df": "```plaintext\n[Explanation]\nThe test passes an integer (3) as data. The function immediately checks \"if not isinstance(data, pd.DataFrame):\" and raises a ValueError when data is not a DataFrame. The test expects an exception via assertRaises(Exception,...), so the raised ValueError satisfies that.\n\n[Result]\nPASS\n```", "test_case_target_column": "```plaintext\n[Explanation]\nThe function checks \"if target_column not in data.columns\" and raises a ValueError. The test calls task_func with a target column that is not present and uses assertRaises(Exception,...), so an exception will be raised as expected.\n\n[Result]\nPASS\n```", "test_case_empty_df": "```plaintext\n[Explanation]\nThe test provides an empty DataFrame (no rows). The function checks data.empty and raises ValueError(\"data cannot be empty\"). assertRaises(Exception, ...) will catch this ValueError, so the test's expectation of an exception is satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_non_numeric_values": "```plaintext\n[Explanation]\nThe DataFrame column 'A' contains a string, so pandas will give it an object dtype. The function checks all columns with pd.api.types.is_numeric_dtype and will raise ValueError(\"data must contain only numeric values\") before training. The test expects an exception, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_1": "```plaintext\n[Explanation]\nThe function performs the expected operations (validation, numeric check, train_test_split with random_state=0 and test_size=0.2, fit LinearRegression, return R^2). The test constructs a deterministic DataFrame using np.random.default_rng(seed=0) and calls task_func with default parameters, so the split and model training are reproducible. The returned float R^2 matches the expected value in the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function's input validations accept the provided DataFrame (non-empty, contains target column, numeric dtypes, integer random_state default, valid test_size). Splitting and training proceed normally on the noiseless linear relation y = 2*x1 + 1; LinearRegression will recover the exact relation and model.score (R^2) on the test set is 1.0 (a float). Therefore the assertions in the test (is instance float and almost equal to 1.0) will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe implementation correctly fits a LinearRegression on the noiseless linear data and returns R^2 = 1.0. However, sklearn's model.score returns a numpy scalar (np.float64), not a built-in Python float. The test first calls self.assertIsInstance(result, float), which will fail because isinstance(np.float64(1.0), float) is False. Thus the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function validates inputs, splits the DataFrame (using the provided test_size=0.4 and random_state=42), fits a LinearRegression on features x1 and x2, and returns the R^2 score on the test set. The test constructs the data deterministically (NumPy RNG seed 0) and passes the same split parameters; sklearn's train_test_split and LinearRegression are deterministic with the given random_state. There are no type or logic errors that would change the numeric result, so the computed float R^2 will match the expected value within the assertion tolerance.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/998", "raw_responses": {"test_valid_file": "```plaintext\n[Explanation]\nThe test creates a tar.gz file in setUp and patches urlretrieve and hashlib.md5 so the checksum check passes. task_func extracts the tar file and returns True but does not delete the TARGET_TAR_FILE after successful extraction. The test asserts that the file does not exist after the call, so this assertion will fail.\n\n[Result]\nFAIL\n```", "test_invalid_checksum_valid_format": "```plaintext\n[Explanation]\nThe test creates a tar.gz file at downloaded_files.tar.gz, patches urllib.request.urlretrieve (so download doesn't overwrite/remove the file) and patches hashlib.md5 to return a mock whose hexdigest() yields 'invalidchecksum'. task_func will open the existing file, call the mocked md5, compare the checksum (mismatch), remove the TARGET_TAR_FILE, and return False. The test then asserts False and that the file was removed \u2014 both true. The code's imports (including tarfile) are in the same global namespace, so the test runs without import errors.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_download_failure": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve to raise an Exception. task_func will catch this exception, remove the existing TARGET_TAR_FILE (which setUp created), and return False. The test asserts False, so the behavior matches. All names used (tarfile, TARGET_TAR_FILE) are present, and the patched call intercepts correctly.\n\n[Result]\nPASS\n```", "test_file_removal_after_failure": "", "test_extraction_success": "```plaintext\n[Explanation]\nThe test's setUp creates a valid tar.gz file named downloaded_files.tar.gz. In the test, urllib.request.urlretrieve is patched (so it doesn't overwrite the existing file) and hashlib.md5 is patched to return a mock whose hexdigest() returns the expected checksum. task_func will open the existing tar file, call the mocked md5 (which yields the expected checksum), pass the checksum check, extract the tar and return True. The code's imports (including tarfile) are present, so the test will run and assertTrue(result) will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
