{"id": "BigCodeBench/211", "raw_responses": {"test_download_and_extract": "```plaintext\n[Explanation]\nThe code calls os.makedirs(destination_directory, exist_ok=True) with destination_directory set to '/path/to/destination' (the test's constant). The test did not patch os.makedirs, so the function will attempt to create directories at the root path, which will typically raise a PermissionError in the test environment. Therefore the test will not complete successfully and will fail before reaching the mocked file/network logic.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_2": "```plaintext\n[Explanation]\nThe function calls os.makedirs(destination_directory, exist_ok=True) with destination '/path/to/destination', but the test does not patch os.makedirs. In a typical test environment creating a directory at '/path/to/destination' will raise a PermissionError (or otherwise fail), so the test will not proceed to the mocked requests.get/open/ZipFile calls. Therefore the code will not pass the provided test.\n\n[Result]\n```plaintext\nFAIL\n```", "test_3": "```plaintext\n[Explanation]\nThe test patches requests.get, zipfile.ZipFile, os.path.join, open, and others, but it does NOT patch os.makedirs. The code calls os.makedirs(destination_directory, exist_ok=True) with destination_directory = '/path/to/destination', which will attempt to create directories at the filesystem root and will raise a PermissionError in the test environment. Therefore the test will not complete successfully.\n\n[Result]\nFAIL\n```", "test_4": "```plaintext\n[Explanation]\nThe test patches many path and file functions but does not patch os.makedirs. The code calls os.makedirs('/path/to/destination', exist_ok=True) which will attempt to create top-level directories and likely raise a PermissionError in the test environment. Even if that did not error, the test expects the downloaded filename to come from basename ('data_download.zip'), while the code always uses \"temp_download.zip\", so the mock_open assertion would also fail. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_5": "```plaintext\n[Explanation]\nThe test patches os.path.join to return a MagicMock path and patches builtins.open and zipfile.ZipFile, but the code still calls os.makedirs(destination_directory) and os.remove(zip_filename) without patching them. os.makedirs will attempt to create the absolute '/path/to/destination' (likely causing a PermissionError in the test environment), and even if that succeeds, os.remove is called with zip_filename equal to the MagicMock returned by the patched os.path.join, which will raise a TypeError. Therefore the function will not complete as the test expects.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/360", "raw_responses": {"test_normal_functionality": "```plaintext\n[Explanation]\nThe test's create_dummy_excel calls df.to_excel(file_path, index=False, sheet_name='TestSheet') passing index=False as a second positional argument. In pandas to_excel the second positional parameter is sheet_name, so this results in \"to_excel() got multiple values for argument 'sheet_name'\" (TypeError) during setUp. The test therefore errors before exercising task_func. The task_func implementation itself would have satisfied the assertions, but the test helper is faulty and causes the test to fail.\n\n[Result]\nFAIL\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test calls task_func with a filename that does not exist ('non_existent.xlsx') and expects a FileNotFoundError. The implementation explicitly checks os.path.exists(file_location) and raises FileNotFoundError if the file is missing, so the function will raise the expected exception and the test will pass.\n\n[Result]\nPASS\n```", "test_invalid_sheet_name": "```plaintext\n[Explanation]\nThe test calls task_func('test.xlsx', 'NonExistentSheet') after creating test.xlsx with sheet 'TestSheet'. The function checks the file exists (it does) and then calls pd.read_excel with the nonexistent sheet name. pandas raises a ValueError for a missing sheet, which the function catches and re-raises as a ValueError. Therefore the test expecting a ValueError will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_correct_mean_and_std_values": "```plaintext\n[Explanation]\nThe function uses pandas.Series.std() which defaults to sample standard deviation (ddof=1). For columns A and B with values [10, 30] and [20, 40], pandas will compute std \u2248 14.1421, but the test expects population standard deviation 10.0. Therefore the returned result dict will not match the expected values and the test will fail.\n\n[Result]\nFAIL\n```", "test_bar_chart_labels": "```plaintext\n[Explanation]\nThe test creates 'test.xlsx' with a sheet 'TestSheet' and calls task_func which reads the file, computes stats, and builds a matplotlib figure. The function explicitly sets ax.set_xlabel('Columns') and ax.set_ylabel('Values'), which matches the test assertions. There are no other interfering behaviors for this test, so the labels will match.\n\n[Result]\nPASS\n```", "test_value": "```plaintext\n[Explanation]\nThe function uses pandas.Series.std() which defaults to sample standard deviation (ddof=1), giving std \u2248 14.1421 for columns A and B. The test expects population std = 10.0, so the returned result dict will not match the expected values; thus the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/486", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns the Axes object from plt.subplots, sets the x and y labels to 'Time' and 'Value', and uses no operations that would raise errors for the provided inputs. The isinstance check against plt.Axes will succeed for the returned Axes, so the test assertions should all pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function calls np.random.seed(seed) at the start and then generates the random_values with np.random.normal. Each invocation resets the global numpy RNG to the same seed, so the generated values (and thus the plotted y-data) are identical across both calls. No other nondeterministic operations affect the y-data before it's plotted. Therefore the np.array_equal check will be True.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function seeds numpy RNG with the given seed and draws random normal values, then adds the same linear trend. Calling it with seed=42 and seed=52 will produce different random arrays (and thus different y-data). The plotted line y-data are taken directly from those arrays, so np.array_equal will be False. No other code behavior makes the outputs identical.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function creates a matplotlib Axes via plt.subplots() and returns it. For the given inputs (start=0, end=10000, step=100, trend=-0.001) the code runs without errors and returns an Axes instance; isinstance(ax, plt.Axes) will be True. (Required libraries are imported and used correctly.)\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a matplotlib figure and returns the Axes object from plt.subplots. The returned ax is an instance of matplotlib.axes.Axes, so isinstance(ax, plt.Axes) will be True. No runtime errors occur for the provided inputs.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe test calls task_func(10000, 0, 100, 0.001) expecting an exception. The implementation uses range(start_time, end_time + 1, step). For start > end with a positive step this yields an empty list, and the function proceeds to create empty arrays/DataFrame and plots them without raising any exception. Therefore no exception is raised and the test's with self.assertRaises(Exception) will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe test calls task_func with step=0. Inside the function, range(start_time, end_time + 1, step) is executed; Python raises a ValueError when range's step is zero. The test expects an exception via assertRaises(Exception), so the function behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_8": "```plaintext\n[Explanation]\nThe function plots datetime x-values for timestamps 0..1000 ms. Matplotlib's AutoDateFormatter typically formats such short time ranges showing only time-of-day (e.g. \"00:00:00\") and not the year, so none of ax.get_xticklabels() are likely to contain the substring \"1970\". Therefore the test's assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/574", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns an Axes object and plots two Line2D objects (the fitted curve and the original sine) while the noisy data is plotted with scatter (a PathCollection, not a Line2D), so len(ax.lines) == 2 holds. Labels and legend are set. curve_fit may emit warnings but should not raise for the default parameters. Therefore the test assertions match the behavior.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns an Axes with two Line2D objects (fitted curve first, original sine second). For array_length=150, the fitted line x-data length is 150. The fitted sine has amplitude and frequency close to 1, so the maximum consecutive y difference is roughly derivative * dx \u2248 1 * (4\u03c0/149) \u2248 0.084, well below 1.1. Therefore the assertions should hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function returns a matplotlib Axes and the first Line2D on ax.lines is the fitted curve (ax.plot of y_fitted), so the test's extraction of y_data is correct. For a fitted sinusoid with typical returned amplitude near the true value (~1) and frequency near 1, the maximum adjacent difference in y_fitted is small (much less than 3.0). Although the noise is large, curve_fit typically recovers a reasonable sine fit here, so the asserted bound (<= 3.0) will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_varying_noise_levels": "```plaintext\n[Explanation]\nThe function returns a matplotlib Axes object (ax) and the test asserts that the returned value is an instance of plt.Axes. The module imports matplotlib.pyplot as plt at top level, so when the test imports the submission (commonly using from <module> import *), plt will be available in the test namespace. curve_fit is used but should succeed for the provided x/y data (including noise levels 0, 0.1, 0.5) and will not raise an exception in normal environments with scipy installed. Therefore the function should return an Axes instance for each noise level and satisfy the assertion.\n\n[Result]\n```plaintext\nPASS\n```", "test_plot_outputs": "```plaintext\n[Explanation]\nThe module does a top-level \"from scipy.optimize import curve_fit\". In many test environments SciPy is not available, causing an ImportError before the test can call task_func. If SciPy were present, the function creates a matplotlib Axes and returns it (Axes have a .figure attribute), so the test would pass; however the missing SciPy import makes the test fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/723", "raw_responses": {"test_correct_scraping": "```plaintext\n[Explanation]\nThe test patches csv.writer to return a mock_writer and expects mock_writer.writerows to have been called. The code uses csv.DictWriter; DictWriter calls the (patched) csv.writer and then uses its own writeheader() and writerow() methods which internally call mock_writer.writerow, not mock_writer.writerows. Therefore mock_writer.writerows is never called and the assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise Exception('Invalid URL'). task_func calls urllib.request.urlopen without catching exceptions, so the exception will propagate. The unittest expects an exception with assertRaises, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_table": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to return a mock response whose read() yields the provided HTML. task_func calls urlopen (mocked), reads bytes, parses with BeautifulSoup (no title or paragraphs in the HTML), builds data, writes a CSV file, and returns the constant 'scraped_data.csv'. The test only asserts the returned value equals 'scraped_data.csv', so the function's behavior meets the test. No runtime errors are expected in this test scenario.\n\n[Result]\nPASS\n```", "test_no_table": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to return HTML with no table and expects task_func to raise an Exception. The provided task_func parses the HTML, writes a CSV (using title and paragraph text), and returns the CSV filename; it does not check for a table or raise an exception. Therefore the function will return normally and the test's assertRaises will fail.\n\n[Result]\nFAIL\n```", "test_overwrite_existing_csv": "```plaintext\n[Explanation]\nThe test patches csv.writer and expects csv.writer to be used and writerows to be called with [['New Data']]. The implementation uses csv.DictWriter (not csv.writer) and calls writer.writeheader() and writer.writerow(data). Also the code opens the file with extra keyword arguments (newline='' and encoding='utf-8'), while the test asserts open was called exactly with (CSV_FILE_PATH, 'w') \u2014 this argument mismatch will fail the assertion. Therefore the test will not pass.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/914", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe model fits a perfect linear trend (closing_price increases by 1 per day). Training uses integer-second timestamps and predictions use future timestamps; in a UTC environment these are consistent, so the LinearRegression yields exact increments and predicted_prices.tolist() becomes [107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0], matching the expected output. (A potential timezone inconsistency with timestamp() could break this in non-UTC environments, but assuming the test runs in UTC, it passes.)\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function fits a linear regression on timestamps (seconds) vs. closing prices and predicts the next 7 days. For the provided DataFrame (daily increasing prices by 1), the model will learn a slope of 1 price-unit per day and predict [207.0 .. 213.0]. Potential pitfalls (astype('int64') on datetimes or timestamp() using local timezone) are unlikely to affect the given test in a typical environment (pandas version where astype works and CI using UTC). Therefore the code will produce the expected list of predictions.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function fits a linear regression on timestamps vs prices for perfectly linear daily data (300..306). Converting dates to seconds and predicting for the next 7 days yields successive +1 increments. Numeric operations produce the expected exact integer float values (e.g. 307.0 .. 313.0), so the returned list matches the test's expected list.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe code trains a linear regression on date timestamps (in seconds) vs. closing prices, generates next 7 daily timestamps, predicts and returns the list of floats. The input dates are daily at midnight and closing prices increase by 1 per day, producing a perfect linear relationship; sklearn will fit the expected slope and intercept and predict [407.0 .. 413.0]. There are no type or runtime issues for this dataframe shape and operations, so the returned list will match exactly.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function trains a linear regression on the provided daily timestamps and closing prices. It converts training dates to integer seconds, predicts using future dates converted to timestamps, and returns the predicted prices as a list. For the given monotonic +1-per-day data (500..506), the fitted model will predict +1 per day for the next 7 days (507..513). Data shapes and conversions are consistent and the returned list will match the expected floats.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/990", "raw_responses": {"test_hex_string_sample": "```plaintext\n[Explanation]\nFor input '4a4b4c' the function decodes to bytes b'JKL' and utf-8 string 'JKL'. base64.b64encode yields 'SktM'. Encoding to utf-16/utf-32 then decoding returns 'JKL'. ASCII decode succeeds as 'JKL'. urllib.parse.quote with safe='' still leaves alphanumerics unencoded, giving 'JKL'. codecs.encode with 'rot13' maps 'JKL' -> 'WXY'. All asserted values match the test.\n\n[Result]\nPASS\n```", "test_hex_string_1": "```plaintext\n[Explanation]\nAll conversions for the test input '68656c6c6f' ('hello') are correct (hex, base64, utf-8, utf-16, utf-32, ASCII, URL). However, codecs.encode is called with the encoding name 'rot13', which is not a standard codec name in Python (the registered codec is 'rot_13'). This will raise a LookupError and cause the function to fail the test.\n\n[Result]\nFAIL\n```", "test_hex_string_2": "```plaintext\n[Explanation]\nThe function correctly unhexlifies the input, decodes to UTF-8, base64-encodes the bytes, and round-trips UTF-16/UTF-32 by encode/decode which returns the original string. ASCII decoding and URL quoting for purely alphabetic ASCII succeeds. The ROT13 encoding via codecs.encode(utf8_string, 'rot13') is available and produces the expected 'jbeyq'. For the input '776f726c64' (which is \"world\") all asserted outputs match.\n\n[Result]\n```plaintext\nPASS\n```", "test_hex_string_3": "```plaintext\n[Explanation]\nThe input hex '616263' unhexlifies to b'abc'. The code decodes to UTF-8 'abc' and base64-encodes b'abc' to 'YWJj'. UTF-16/UTF-32 paths encode then decode with BOM handling, returning 'abc'. ASCII decoding succeeds, URL quoting leaves alphanumerics unchanged ('abc'), and ROT13 of 'abc' yields 'nop'. All asserted values match, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_hex_string_4": "```plaintext\n[Explanation]\nThe input hex \"313233\" decodes to bytes b'123' and to the UTF-8 string \"123\". base64.b64encode(b'123') -> \"MTIz\". The utf-16 and utf-32 roundtrip (encode then decode) return \"123\". ASCII decoding succeeds. urllib.parse.quote leaves digits unquoted, so \"123\". codecs.encode with ROT13 leaves digits unchanged, so \"123\". All asserted values match the function's outputs.\n\n[Result]\nPASS\n```", "test_hex_string_non_ascii": "```plaintext\n[Explanation]\nThe function correctly unhexlifies 'c3a9' to bytes b'\\xc3\\xa9', decodes to the UTF-8 character '\u00e9', and base64-encodes the original bytes to 'w6k='. The utf-16/utf-32 encode-decode roundtrips return '\u00e9'. ASCII decoding fails and yields 'Not representable in ASCII'. URL quoting of '\u00e9' produces '%C3%A9'. ROT13 leaves '\u00e9' unchanged. All asserted values in the test match these results.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1077", "raw_responses": {"test_example_case": "```plaintext\n[Explanation]\nThe code parses the timestamps, localizes them to UTC, converts to the target timezone, and computes consecutive differences. Converting all timestamps to the same timezone preserves the intervals, so the two 61-second gaps remain, and numpy.mean returns 61.0. The input format matches the parser, and required modules are imported. Therefore the test's expected value 61.0 will be produced.\n\n[Result]\n```plaintext\nPASS\n```", "test_different_timezones": "```plaintext\n[Explanation]\nThe function parses the timestamps, localizes them to UTC, then converts all to the requested timezone. Converting all timestamps by the same timezone offset preserves the intervals between consecutive times, so the differences remain 61 seconds each and the mean is 61.0. The input format matches the strptime format, and pytz/numpy are imported. The assertions with assertAlmostEqual will succeed.\n\n[Result]\nPASS\n```", "test_varying_differences": "```plaintext\n[Explanation]\nThe code parses the three timestamps (12:00:00, 12:01:01, 12:03:03), converts them to the same timezone (Asia/Tokyo) which does not change relative differences, computes differences 61s and 122s, and returns the mean (61 + 122) / 2 = 91.5. The function returns a numeric float (numpy mean) that matches the test's expected value.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_time_string": "```plaintext\n[Explanation]\nThe function immediately returns 0.0 when the input list has fewer than two timestamps. The test supplies a single timestamp, so task_func returns 0.0 as expected. No runtime errors are triggered.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_span_across_days": "```plaintext\n[Explanation]\nThe function parses the two timestamps, localizes them to UTC, and converts both to Asia/Tokyo. Converting both by the same timezone offset does not change their difference. The original gap between '31/03/21 23:59:00.000' and '01/04/21 00:01:00.000' is 2 minutes = 120 seconds, and the mean of that single difference is 120.0. The code returns this value (as a float).\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_out_of_order_strings": "```plaintext\n[Explanation]\nThe function computes consecutive differences without taking absolute values. For the input order, differences are -122 and 61 seconds, giving a mean of -30.5. The test expects the mean of absolute differences (122 and 61) = 91.5. Therefore the code will not match the expected output.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/214", "raw_responses": {"test_image_size_and_type": "```plaintext\n[Explanation]\nThe function seeds numpy's RNG with the default seed 42 and generates a (20,20,3) uint8 array using np.random.randint(0, 256). It returns the raw image array unchanged. Given the same seed and range, the produced array is deterministic and matches the expected nested list. The shape and dtype checks also pass.\n\n[Result]\nPASS\n```", "test_random_seed_reproducibility": "```plaintext\n[Explanation]\nThe function sets the NumPy RNG seed (np.random.seed(seed)) before generating the image with np.random.randint, so repeated calls with the same seed produce identical arrays. No other non-deterministic operations affect the generated image. The test compares the two returned numpy arrays with the same seed, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_range_values": "```plaintext\n[Explanation]\nThe function uses np.random.randint(range_low, range_high + 1, ...) which yields integers in [range_low, range_high] inclusive, so for range_low=100 and range_high=200 all pixel values will be between 100 and 200. The dtype uint8 and plotting calls do not alter the array values. The ValueError check passes for the given inputs. Therefore the test assertion will hold.\n\n[Result]\n```plaintext\nPASS\n```", "test_error_on_invalid_range": "```plaintext\n[Explanation]\nThe function begins by checking if range_low >= range_high and raises ValueError in that case. Calling task_func(range_low=255, range_high=0) meets that condition, so a ValueError is raised as the test expects. The exception occurs before any plotting or external side effects.\n\n[Result]\nPASS\n```", "test_return_types": "```plaintext\n[Explanation]\nThe module imports cv2 at top-level. If OpenCV (cv2) is not installed in the test environment, importing the module will raise ImportError and prevent the tests from running. The rest of the function would return a matplotlib Axes and a numpy ndarray (which would satisfy the assertions) if cv2 were available. Because the cv2 import is likely missing in typical environments, the test will fail due to import error.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/99", "raw_responses": {"test_figure_type": "```plaintext\n[Explanation]\nThe function returns g.fig where g = sns.pairplot(...). seaborn.PairGrid has a .fig attribute that is a matplotlib.figure.Figure, so isinstance(self.fig, plt.Figure) will be True. The code should run on the small iris dataset without raising errors (numeric 'species' hue is allowed and missing Arial simply falls back), so the test will pass.\n\n[Result]\nPASS\n```", "test_axes_existence": "```plaintext\n[Explanation]\nThe function loads the iris dataset into a DataFrame, calls seaborn.pairplot which constructs a PairGrid with many subplot axes, and returns g.fig. The returned matplotlib Figure contains multiple axes, so len(self.fig.axes) > 0 holds. No other error-prone operations are present for the provided test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_figure_children": "```plaintext\n[Explanation]\nThe function builds a DataFrame from sklearn's iris data, calls seaborn.pairplot (which returns a PairGrid with a Figure at g.fig), and returns that Figure. pairplot will create axes/artists so fig.get_children() will be non-empty. Setting font to 'Arial' merely updates rcParams and is not fatal if the font isn't installed. There are no obvious errors that would occur for the given test.\n\n[Result]\nPASS\n```", "test_plot_labels": "```plaintext\n[Explanation]\nThe function builds a DataFrame using iris.feature_names (which include \"sepal length (cm)\"), calls seaborn.pairplot, and returns the figure. In a pairplot the leftmost column axes (including the top-left axes likely at fig.axes[0]) have y-axis labels set to the feature names, so ax.get_ylabel() will contain \"sepal length (cm)\" and thus the substring \"sepal length\". No exceptions are expected, so the test assertion should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_plot_title": "```plaintext\n[Explanation]\nThe function builds a DataFrame, calls seaborn.pairplot (which returns a PairGrid with a .fig attribute), then sets the figure suptitle via g.fig.suptitle(...). That creates fig._suptitle with the given text. There are no operations that would prevent the title from being set or raise an exception in the provided test context, so the test assertions checking for a non-None _suptitle and exact text should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/367", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function validates datetime objects, counts activities by weekday (Monday=0..Sunday=6), and plots bars for each day in order. The test supplies 2023-10-23 (Monday) and 2023-10-24 (Tuesday), so bars[0] and bars[1] heights will be 1 (1.0) and the rest 0. The axis title and labels match the test assertions. Therefore the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function counts activities by weekday correctly (weekday() -> 0..6), validates datetime instances, and constructs a bar chart with correct labels and title. For the given dates (Mon, Tue, Tue, Thu) the bar heights will be [1,2,0,1,0,0,0], matching the test assertions. get_height() returns floats equal to the asserted integers, so equality holds. Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function validates inputs, counts activities by weekday (weekday() returns 6 for Sunday), and plots bars in order ['Monday', ..., 'Sunday']. For the test input (two datetimes on 2023-10-29, a Sunday) counts[0..5] will be 0 and counts[6] will be 2. ax.bar creates patches in that same order, so bars[0..5].get_height() == 0 and bars[6].get_height() == 2. No errors expected.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function produces a bar chart with seven bars of height 0 for an empty activities list and will raise TypeError for non-datetime inputs, so the semantic behavior is correct. However, the test accesses ax.patches, which includes the Axes background Rectangle in addition to the bar Rectangles. That extra patch has non-zero height (usually 1.0), so the test's loop asserting bars[0..6] heights == 0 will fail on the first index. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function validates inputs as datetime objects, counts one activity per weekday for the provided consecutive dates (Mon-Sun), and returns an Axes with 7 bars whose heights are 1 (get_height() yields 1.0 which equals 1). The test uses the same module-level datetime import, so no NameError. Therefore the assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/492", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function sets current_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0), so the 'Date' values are truncated to midnight of that day. The test expects the first 'Date' to equal datetime.fromtimestamp(1631289600000 / 1000.0), which retains the original time (not midnight). Therefore the equality assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function sets each row's 'Date' to start_date.replace(hour=0, minute=0, second=0, microsecond=0) (midnight of the start day). The test expects the first 'Date' to equal datetime.fromtimestamp(three_days_ago / 1000.0) (the original timestamp with its time-of-day). These will not match, so the equality assertion will fail. Other assertions (columns and product list) would pass.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test passes a float (timestamp()*1000) but the function checks isinstance(epoch_milliseconds, int) and will raise ValueError. Additionally, even if the type check were satisfied, the function uses start_date.replace(..., hour=0, minute=0, ...) for the 'Date' values so the first 'Date' would not equal datetime.fromtimestamp(one_month_ago/1000.0) (time component lost). Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function accepts the given integer epoch and product list, validates inputs, and generates rows starting from the epoch date (2020-01-01 00:00:00 local time). The DataFrame columns are created in the order 'Product', 'Date', 'Sales' from the dicts. The first Date entry is start_date with hour/minute/second zeroed, which equals datetime.fromtimestamp(1577836800000/1000.0) used in the test. The product uniqueness and order are preserved; sorting the unique products matches the provided list. pandas returns Timestamp objects that compare equal to the datetime used in the assertion. Therefore the code satisfies all assertions in the test case.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function converts the provided epoch milliseconds to a datetime and compares it to datetime.now(); it raises ValueError when start_date >= end_date. The test passes a timestamp one day in the future, so start_date will be after end_date and the function will raise ValueError as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function checks that products is a list of exactly 5 elements and raises ValueError if not. The test passes a list of length 4, so task_func will raise ValueError immediately, satisfying the test's assertRaises.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_7": "```plaintext\n[Explanation]\nThe function validates inputs and raises ValueError if the products list does not contain 5 unique strings. The test passes products=['a','a','b','c','d'] which has duplicates, so the function will raise ValueError as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/579", "raw_responses": {"test_valid_csv_file": "```plaintext\n[Explanation]\nThe mocked CSV contains \"word1,word2\\nword3,word4\". csv.reader will yield the four cells, which after normalization and removing non-alpha characters become four occurrences of \"word\". Counter.most_common returns a list with one tuple, and the function creates a matplotlib Axes and returns (ax, most_common_words). The types asserted in the test (matplotlib.axes.Axes and list) match. The use of mock_open with encoding and csv.reader is compatible, and creating a figure should succeed in a typical test environment.\n\n[Result]\nPASS\n```", "test_file_not_found_error": "```plaintext\n[Explanation]\nThe function attempts to open the given CSV file. If the file does not exist, open raises FileNotFoundError which the function catches and re-raises as FileNotFoundError. The test calls task_func('non_existent.csv') inside assertRaises(FileNotFoundError), so the expected exception will be raised and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_io_error": "```plaintext\n[Explanation]\nThe test patches builtins.open to have side_effect = IOError, so calling open in task_func raises an IOError. task_func catches IOError and re-raises an IOError, which satisfies the test's assertRaises(IOError). No other errors occur in this test path.\n\n[Result]\nPASS\n```", "test_plot_output": "```plaintext\n[Explanation]\nThe mocked CSV provides 'word1,word1\\nword2,word2'. The function normalizes and strips non-letters, turning 'word1' and 'word2' into 'word', producing a single most-common word with count 4. A single bar is drawn; ax.patches includes the axes background rectangle plus the bar, so len(ax.patches) == 2. ax is a matplotlib.axes.Axes instance. Therefore the assertions will pass.\n\n[Result]\nPASS\n```", "test_normalized_text": "```plaintext\n[Explanation]\nThe function lowercases all words before counting (words = full_text.lower().split()), so the most_common_words entries will be lowercase (e.g., ('cafe', 2)). The test expects ('Cafe', 2) with an uppercase C, so the tuple will not be found and the assertion will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/998", "raw_responses": {"test_valid_file": "```plaintext\n[Explanation]\nThe test's setUp creates TARGET_TAR_FILE before calling task_func. The test patches urllib.request.urlretrieve and hashlib.md5 so the function reads the existing tar file and the mocked hexdigest matches EXPECTED_MD5_CHECKSUM. The code then extracts the tar.gz and returns True, but it does not delete TARGET_TAR_FILE after successful checksum/extraction. The test expects the file to not exist (assertFalse(os.path.exists(TARGET_TAR_FILE))), so that assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_invalid_checksum_valid_format": "```plaintext\n[Explanation]\nThe test's setUp creates TARGET_TAR_FILE. The test patches urllib.request.urlretrieve (so no overwrite) and patches hashlib.md5 to return a mock whose hexdigest() returns 'invalidchecksum'. task_func opens the existing tar file, calls the mocked md5, gets 'invalidchecksum' which does not match EXPECTED_MD5_CHECKSUM, so it removes TARGET_TAR_FILE and returns False. This matches the test's assertions (result is False and the file no longer exists).\n\n[Result]\n```plaintext\nPASS\n```", "test_download_failure": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve to raise an exception. The function catches any Exception, removes TARGET_TAR_FILE if it exists (setUp created it), and returns False. That matches the test's assertion.\n\n[Result]\nPASS\n```", "test_file_removal_after_failure": "```plaintext\n[Explanation]\nThe test's setUp creates downloaded_files.tar.gz, and the test patches urllib.request.urlretrieve (so download does nothing) and hashlib.md5 to return a hexdigest of 'invalidchecksum'. task_func reads the existing tar file, gets the mocked checksum which does not match EXPECTED_MD5_CHECKSUM, then calls os.remove(TARGET_TAR_FILE) and returns False. The test then asserts the file was removed. No exceptions occur, so the behavior matches the assertion.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_extraction_success": "```plaintext\n[Explanation]\nThe test creates a tar.gz file and patches urllib.request.urlretrieve and hashlib.md5 so the download is skipped and the computed hexdigest returns the expected checksum. The file exists from setUp, the patched md5 causes the checksum check to pass, and tarfile.extractall() runs without error. The function returns True, so the test's assertTrue(result) will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/752", "raw_responses": {"test_case_test_size": "```plaintext\n[Explanation]\nThe test expects an exception for test_size values 5, -1, 0, and 1. The function explicitly rejects 5 and -1. For 0, scikit-learn's train_test_split will raise an error (invalid test_size). For 1, however, the function accepts it (check allows 0 <= test_size <= 1) and an integer test_size=1 is valid for train_test_split (one sample in the test set), so no exception is raised. Therefore one assertRaises fails.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_random_state": "```plaintext\n[Explanation]\nThe test calls task_func(data, 'y', 0.2, 'a') (and similar with non-int random_state values). The function validates random_state with \"if not isinstance(random_state, int): raise ValueError(...)\" before proceeding, so passing a string, list, or dict causes a ValueError to be raised. The test expects an exception via assertRaises, so the code will satisfy the test.\n\n[Result]\nPASS\n```", "test_case_df": "```plaintext\n[Explanation]\nCalling task_func with data=3 (an int) triggers the input validation:\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"data must be a DataFrame\")\nThis raises a ValueError (a subclass of Exception), satisfying self.assertRaises(Exception, ...). Therefore the test passes.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_target_column": "```plaintext\n[Explanation]\nThe function checks for target_column in data.columns and raises ValueError(\"target_column must be a column of data\") when it's missing. The test provides a DataFrame without the 'not' column, so task_func will raise ValueError (a subclass of Exception). assertRaises(Exception, ...) will therefore succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_empty_df": "```plaintext\n[Explanation]\nThe test passes an empty DataFrame (has columns but no rows). The function checks data.empty and raises ValueError(\"data cannot be empty\"), which satisfies assertRaises(Exception, ...).\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_non_numeric_values": "```plaintext\n[Explanation]\nThe DataFrame contains a non-numeric value ('test') in column 'A'. The function checks each column with pd.api.types.is_numeric_dtype and raises ValueError(\"data must contain only numeric values\") for 'A'. The test expects an exception, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_1": "```plaintext\n[Explanation]\nThe function returns model.score(...) which is a numpy.float64, not a built-in Python float. The test first calls self.assertIsInstance(result, float), which will fail for a numpy.float64 even if the numeric value matches. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe implementation is correct functionally and would produce an R^2 of 1.0 for the perfect linear relation. However, model.score(...) returns a numpy.float64, not a built-in Python float. The test first does self.assertIsInstance(result, float), which will fail because numpy.float64 is not an instance of built-in float. Thus the test fails despite correct numeric value.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe DataFrame columns are all numeric and the target is an exact linear combination of the features (y = 2*x1 - 0.14*x2 + 25). train_test_split and LinearRegression will fit this relationship exactly, so model.score on the test set will be 1.0 (a float). Input validation checks in the function do not block this case.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function correctly validates inputs, uses sklearn's train_test_split and LinearRegression, and returns a float R^2. For the provided seeded random DataFrame and test_size=0.4 with random_state=42, the linear regression coefficients and resulting R^2 match the expected value (the quadratic term contributes only a small residual), so the computed score will equal the asserted 0.9985567445794377.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1085", "raw_responses": {"test_empty_text": "```plaintext\n[Explanation]\nThe function lowercases, removes punctuation, splits, counts words, and returns Counter.most_common(10). For an empty input this yields an empty list; the function also creates an empty matplotlib Axes and returns it. The test asserts that common_words == [], which matches the function's behavior. The module imports matplotlib.pyplot as plt, so plt.close() in tearDown will be available. No exceptions occur for the empty input.\n\n[Result]\nPASS\n```", "test_single_word": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation, splits into words, and uses Counter.most_common(10). For input 'test test test' this yields [('test', 3)], and the test unpacks and compares that value. The matplotlib figure return and plt usage in teardown do not affect the asserted result. Thus the code meets the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_punctuation": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation via regex, splits into words, and counts them. For input 'hello! hello, world.' it produces [('hello', 2), ('world', 1)], matching the test assertion. The test's plt.close() refers to plt which is imported at module level, so no NameError. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_sensitivity": "```plaintext\n[Explanation]\nThe function lowercases the input, removes punctuation, splits into words, counts them with Counter, and returns most_common(10). For 'Hello hello HeLLo' this yields [('hello', 3)]. matplotlib.pyplot is imported at module level (as plt), so the test's plt.close() in tearDown will find plt. Therefore the test expectation matches the function's output.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_common_scenario": "```plaintext\n[Explanation]\nThe function lowercases and strips punctuation, producing the words ['this','is','a','test','this','is','only','a','test'] with counts this/is/a/test = 2 and only = 1. Counter.most_common preserves the first-seen order for ties, giving [('this',2),('is',2),('a',2),('test',2),('only',1)] which matches the expected list. matplotlib.pyplot was imported at top-level in the same module, so plt.close() in tearDown is available. No other errors present.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/227", "raw_responses": {"test_nonexistent_audio_file": "```plaintext\n[Explanation]\nThe function checks os.path.exists(audio_file) and raises FileNotFoundError if it does not exist. The test patches os.path.isfile (not os.path.exists), so os.path.exists will still return False for a non-existent file, causing the function to raise FileNotFoundError as the test expects.\n\n[Result]\nPASS\n```", "test_empty_list_input": "```plaintext\n[Explanation]\nThe function checks file existence using os.path.exists, but the test patches os.path.isfile (not exists), so the existence check will fail and task_func will raise FileNotFoundError before attempting to reshape the empty list. The test expects a ValueError from reshaping, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_successful_matrix_creation": "```plaintext\n[Explanation]\nThe test patches os.path.isfile but the function checks os.path.exists, so the file-existence check will fail and raise FileNotFoundError. Additionally, the test expects librosa.display.specshow to be called, but the code uses ax.imshow instead. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_docstring_examples": "```plaintext\n[Explanation]\nThe test patches os.path.isfile to return True, but the function uses os.path.exists to check the audio file. Since os.path.exists is not patched and the file does not actually exist in the test environment, the function raises FileNotFoundError before reaching the patched soundfile.read. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_spl_calculation": "```plaintext\n[Explanation]\nThe test patches os.path.isfile but the code checks os.path.exists, so the FileNotFoundError will be raised (the patch does not affect os.path.exists). Even if the existence check passed, the function never applies the SPL to scale the matrix (it only normalizes by the matrix max), so the returned values would be non-negative (0..1) and not the negative dB values the test asserts. Therefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/100", "raw_responses": {"test_plot_attributes": "```plaintext\n[Explanation]\nThe code computes start_date = end_date.replace(day=end_date.day - 29). With the provided current date (2025-12-17, day=17) this attempts replace(day=-12) which raises a ValueError. The function will error before creating the plot or returning the Axes, so the test assertions are never reached.\n\n[Result]\nFAIL\n```", "test_reproducibility": "```plaintext\n[Explanation]\nThe function attempts to compute start_date with end_date.replace(day=end_date.day - 29). If end_date.day <= 29 this sets day to <= 0 which raises a ValueError, so task_func will error before returning axes. Given typical run dates (e.g. 2025-12-17 where day=17), the code will raise an exception and the unittest will fail to execute the reproducibility assertion. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_random_seed_effect": "```plaintext\n[Explanation]\nThe code constructs start_date using end_date.replace(day=end_date.day - 29). For many current days (e.g., day=17), end_date.day - 29 is non-positive, so datetime.replace raises ValueError. Additionally, building the date list with repeated .replace(day=...) will fail when days overflow month bounds. Thus the function will raise an exception before returning Axes, so the unittest will not pass.\n\n[Result]\nFAIL\n```", "test_data_range": "```plaintext\n[Explanation]\nThe function uses datetime.replace(day=end_date.day - 29). If today's day <= 29 (current date is 2025-12-17, day=17), this attempts to set a non-positive day and raises ValueError (\"day is out of range for month\"). The function will error before returning an Axes, so the unittest cannot run and the test fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_value_range": "```plaintext\n[Explanation]\nThe generated y-values are randint(1,100), so they would satisfy 0 <= v <= 100. However, the function computes start_date with end_date.replace(day=end_date.day - 29). For most calendar days (when end_date.day < 30) this passes a non-positive or invalid day to replace and raises a ValueError before any plot/values exist. Because that runtime error will occur in many execution dates, the test will not reliably pass.\n\n[Result]\nFAIL\n```", "test_value": "```plaintext\n[Explanation]\nThe code uses end_date = datetime.now() and then start_date = end_date.replace(day=end_date.day - 29). With the current date (2025-12-17, day=17) this attempts to set day = -12, causing datetime.replace to raise ValueError (\"day is out of range for month\") before any plotting or random number generation occurs. Therefore the test will not run to completion and will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/368", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists files in the source directory, selects one with random.choice after seeding, moves it to the existing destination directory, and returns the filename. The test creates five source files and an existing dest dir, calls task_func with seed=1, and only asserts the returned name is one of the source filenames, that the file now exists in the destination, and ends with '.txt'. Those conditions are satisfied by the implementation, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function lists files in the source directory, seeds the RNG (deterministically), selects one file, moves it with shutil.move, and returns its filename. In the test, src_test_dir_2 contains five files and the destination exists, so a file will be chosen (seeded), moved, and the returned name will be in the original list and exist in the destination. The imports (os, shutil) are present at module top, so the test's filesystem operations work. Therefore the test passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function lists files in the source directory and raises FileNotFoundError if the list is empty. The test creates src_test_dir_3 as an existing directory with no files, so calling task_func on it will raise FileNotFoundError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function lists files, seeds the random module with seed=2, picks and moves one file from src_test_dir_4 to dest_test_dir_4, and returns the filename. The test only verifies the returned name is one of the originally created filenames, that the file exists in the destination, and that it ends with '.txt'. These conditions will be met, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lists files in the given source directory, seeds the RNG with the default 100, selects a random filename from that list, moves the file to the destination directory, and returns the filename. The test sets up src_test_dir_5 with files file16.txt..file20.txt and an existing destination directory. The returned filename will be one of those entries and the file will exist in the destination after shutil.move. The imports (os, shutil) are present in the provided code, so the test's filesystem operations work. Therefore the test assertions should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/501", "raw_responses": {"test_valid_json": "```plaintext\n[Explanation]\nThe function parses the JSON list of dictionaries, writes headers and rows to an Excel file using xlwt, saves it to the given filename, and returns the absolute path. For the provided test input (a list of two dicts) the code will create 'test_valid.xls', os.path.exists(file_path) should be True, and the file is removed. No type or logic errors are present in this path.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_json": "```plaintext\n[Explanation]\nThe input JSON string has a trailing comma and json.loads(json_str) will raise json.JSONDecodeError. The code catches json.JSONDecodeError and re-raises a ValueError, so the test's with self.assertRaises(ValueError) will succeed.\n\n[Result]\nPASS\n```", "test_empty_json": "```plaintext\n[Explanation]\nFor input '[]' the function creates an xlwt Workbook with an empty sheet (no cells written) and saves it as 'test_empty.xls'. The returned absolute path exists. pandas.read_excel can read an .xls with an empty sheet and returns an empty DataFrame, so df.empty is True. Therefore the test assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_string_json": "```plaintext\n[Explanation]\nThe function begins by checking isinstance(json_str, (str, bytes, bytearray)) and immediately raises TypeError if that check fails. Calling task_func(12345, ...) provides an int, so the function will raise TypeError as the test expects. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_custom_sheet_name": "```plaintext\n[Explanation]\nThe function correctly parses the JSON list of a single dict, creates an xlwt workbook with the requested sheet name, writes headers and the data row, saves the file, and returns its absolute path. The test checks that the returned path exists and then removes the file. Assuming required packages (xlwt, json, os) are available in the environment, the code will create the file and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_file_content": "```plaintext\n[Explanation]\nThe function writes an .xls file with headers (\"Name\",\"Age\") and the two rows, returns the absolute path, and pandas can read that sheet back. The written numeric Age values are plain integers so pandas' read_excel will produce the same frame as pd.read_json for this input (matching columns, values and dtype), so the assertion should succeed and the file is removed afterward.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/582", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a histogram with bins=30 (so ax.patches will contain ~30 bars, >=5) and then calls ax.plot once to add the PDF (one Line2D), and returns the figure. Therefore the two assertions (at least 5 patches and exactly 1 line) are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_standard_functionality": "```plaintext\n[Explanation]\nThe function creates and returns a matplotlib Figure object (fig). The test checks that the returned object is an instance of plt.Figure; since the module imported matplotlib.pyplot as plt and returns the created fig, the isinstance check will succeed. There are no logic errors in task_func that would prevent creating or returning the figure.\n\n[Result]\n```plaintext\nPASS\n```", "test_varying_sizes": "```plaintext\n[Explanation]\nThe function creates a matplotlib Figure via plt.subplots and returns it for each requested size. The test only asserts that the returned object is an instance of plt.Figure for several sizes. There are no size-dependent errors, and the imports in the provided code include matplotlib.pyplot as plt, so the returned fig will satisfy isinstance(fig, plt.Figure). Thus the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_histogram_pdf_overlay": "```plaintext\n[Explanation]\nThe test sets the RNG seed and computes expected_bins using numpy's bins='auto' on 1000 samples. It then reseeds and calls task_func(size=1000). task_func always uses bins=30 for the histogram, so len(ax.patches) will be 30 while expected_bins (bins='auto' for these 1000 normal samples) is typically different (around mid-20s). The PDF line check would pass, but the bin-count assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_return_type": "```plaintext\n[Explanation]\nThe function calls plt.subplots() and returns the Figure object (fig). The test checks that the returned value is an instance of plt.Figure, which will be true. There are no other runtime issues in the provided code that affect this single test.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/760", "raw_responses": {"test_dataframe_structure": "```plaintext\n[Explanation]\nThe function builds a pandas DataFrame with 100 IDs, selected names, generated datetime.date objects and corresponding emails. For the call task_func(rng_seed=1) it sets the numpy seed, produces arrays/lists of length 100, and constructs the DataFrame with columns in the given order using a literal dict (insertion order preserved). The returned object is a DataFrame, has columns ['ID', 'Name', 'Date of Birth', 'Email'], and len(df) == 100, so the test assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_randomness_and_encoding": "```plaintext\n[Explanation]\nThe function uses np.random.choice from the provided latin_names and other_names, so every entry in df['Name'] must be one of those four names. Emails are constructed with the default email_domain 'example.com' (f\"...@{email_domain}\"), so every email contains '@example.com'. With rng_seed=1 the behavior is deterministic. Therefore both assertions in the test will be True.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_custom_parameters": "```plaintext\n[Explanation]\nThe function deterministically generates 100 names and dates when rng_seed is set. Emails are built using the passed email_domain, so all emails will end with '@test.com'. Dates are datetime.date objects with years drawn from np.random.randint(start_year, end_year + 1), so all dob.year values fall within 1990\u20131995. Error-prone normalization is wrapped in try/except and won't raise, so the test conditions are satisfied.\n\n[Result]\nPASS\n```", "test_invalid_year_range": "```plaintext\n[Explanation]\nThe function does not explicitly validate the year range, but it calls np.random.randint(start_year, end_year + 1). When start_year > end_year this becomes np.random.randint(low, high) with low >= high, causing numpy to raise a ValueError. The test expects a ValueError, so the code will raise one and the test will pass.\n\n[Result]\nPASS\n```", "test_empty_name_lists": "```plaintext\n[Explanation]\nThe function combines latin_names and other_names into all_names and calls np.random.choice(all_names, size=100). When both input lists are empty, np.random.choice on an empty array raises a ValueError. The test expects a ValueError, so the code will satisfy the test.\n\n[Result]\nPASS\n```", "test_rng": "```plaintext\n[Explanation]\nThe function seeds NumPy's RNG when rng_seed is provided and then uses only NumPy random functions to select names and generate dates. All other operations are deterministic (string processing and DataFrame construction). Resetting the seed at the start of each call makes the sequence identical, so calling task_func(rng_seed=1) twice yields equal DataFrames.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/915", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function computes Z-scores for the 'closing_price' column and flags values with absolute Z > 2. For the input [100,101,102,103,104,150], the mean and std produce a Z-score for 150 \u2248 2.23, so only 150 is identified as an outlier. The function returns that DataFrame and an Axes with title 'Outliers in Closing Prices' and labels 'Index' and 'Closing Price', matching the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function computes z_scores but never adds them to the returned outliers DataFrame. The test expects an added 'Z_score' column with value 2.004094170098539 for the outlier 100. While the outlier selection (only 100 with z \u2248 2.004094170098539 at threshold 1.5) is correct, the missing 'Z_score' column causes the asserted access outliers2['Z_score'] to fail. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function computes Z-scores but does not attach them to the returned DataFrame. The test expects a 'Z_score' column in outliers3, so accessing outliers3['Z_score'] will fail (KeyError) and the assertions cannot pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a DataFrame of outliers but does not add a 'Z_score' column. The test calls task_func with z_threshold=-1 which makes the mask True for all rows (so closing_price matches), but then asserts outliers3['Z_score'] equals a list of values. Since 'Z_score' is not present, the test will fail (KeyError).\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function computes z-scores but does not add them to the returned DataFrame. The test calls task_func on an empty DataFrame and then asserts both 'closing_price' and 'Z_score' columns exist (and are empty). While the empty input is handled and no outliers are found, the returned outliers DataFrame lacks the 'Z_score' column, so accessing outliers3['Z_score'] will raise a KeyError and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/999", "raw_responses": {"test_count_categories_data1": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve, builtins.open (with the provided CSV content), and os.remove, so no real network or filesystem operations occur. csv.DictReader will read the header \"category\" and the rows as provided; the function counts occurrences and returns a dict. The returned counts match the expected {'cat1': 2, 'cat2': 2, 'cat3': 1}. No exceptions are raised, so the code will pass this test.\n\n[Result]\nPASS\n```", "test_count_names_data2": "```plaintext\n[Explanation]\nThe function uses urllib.request.urlretrieve and os.remove which are both patched in the test. builtins.open is patched with mock_open that supplies the CSV contents. csv.DictReader can iterate over the mock file's lines, and the code correctly counts values in the 'name' column and returns the expected dictionary. No exceptions are raised, so the test assertion matches the returned result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_count_categories_data3": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve, os.remove, and builtins.open (mock_open) so no real network or file operations occur. The CSV data provided by read_data matches the expected rows. csv.DictReader will find the 'category' column and the code counts occurrences correctly, returning {'cat1': 2, 'cat2': 1, 'cat3': 2}. No exceptions are raised, so the function returns the expected result and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_count_names_data3": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve, builtins.open (with appropriate CSV read_data), and os.remove, so no real network or file operations occur. csv.DictReader will see the header ['name','other'] and the rows ('Alice' x3, 'Bob', 'Charlie'). The function counts values in the 'name' column and returns {'Alice': 3, 'Bob': 1, 'Charlie': 1}, matching the expected result. No exceptions are raised because the column exists and removals are mocked.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_non_existent_column": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlretrieve, os.remove, and builtins.open (mock_open) so no real network or file operations occur. The CSV mock contains headers ['name','other'], so requesting a non-existent column triggers the code path that calls os.remove (mocked) and raises ValueError. Although the ValueError is raised inside the try, the function's except block re-raises it after attempting cleanup; the test expects a ValueError, so it will pass. mock_open accepts the additional kwargs (newline, encoding) used in open, so no errors arise from that call.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1124", "raw_responses": {"test_valid_url_with_title": "```plaintext\n[Explanation]\nThe input string contains \"https://www.google.com\", which the regex in task_func will match. The test patches requests.get to return a MockResponse that (as implied by the test) provides .content with an HTML title and a no-op raise_for_status. task_func uses BeautifulSoup to extract the <title> text and returns \"Google\", matching the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_non_existent_website": "```plaintext\n[Explanation]\nThe regex extracts \"https://nonexistentwebsite12345.com\" from the input. requests.get is patched to raise requests.RequestException, which the function catches and returns \"Unable to fetch the content of the URL: {url}\" with the exact URL, matching the expected output.\n\n[Result]\nPASS\n```", "test_string_without_urls": "```plaintext\n[Explanation]\nThe input string contains no URLs, so re.findall returns an empty list and task_func returns \"No valid URL found in the provided string.\" which exactly matches the expected output. No network requests are made for this test.\n\n[Result]\n```plaintext\nPASS\n```", "test_multiple_urls_in_string": "```plaintext\n[Explanation]\nThe function finds the first URL in the string (the regex will match \"https://www.google.com\"), calls requests.get (which the test patches to return a MockResponse), parses response.content with BeautifulSoup and returns the <title> text. Given the test's patch and expected mocked content containing a title \"Google\", the function will return \"Google\" and match the assertion.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_url_with_no_title_tag": "```plaintext\n[Explanation]\nThe test patches requests.get to return a Mock and sets its .text to HTML without a <title>. The code calls response.raise_for_status() (a Mock, so no exception) and passes response.content to BeautifulSoup; response.content is a Mock (stringified) so no <title> tag is found and the function returns \"No title tag found in the webpage.\" which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_malformed_url": "```plaintext\n[Explanation]\nThe regex in task_func only matches URLs starting with \"http://\" or \"https://\". The input string contains \"ht://incorrect-url\", so no URLs are found and the function returns \"No valid URL found in the provided string.\" \u2014 which exactly matches the test assertion. The patched requests.get is not used because no fetch occurs.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/13", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP (used by the code) but patches subprocess.call while the code uses subprocess.run. Because subprocess.run is not mocked, the test will attempt to execute the real 'wget' command. In typical test environments 'wget' is not available (or the external call would fail), causing an error (FileNotFoundError or a non-zero exit) and the test to fail. Therefore the code will not pass the test.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP to raise Exception('Failed to connect to FTP server') when called. task_func attempts ftplib.FTP(ftp_server) inside a try/except and re-raises Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\"), producing \"Failed to connect to FTP server invalid_server: Failed to connect to FTP server\", which exactly matches the test's expected message. No other code paths are executed.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so ftp = ftplib.FTP(...) returns a mock whose login method raises Exception('Failed to login'). task_func catches that and raises Exception with the message \"Failed to log into FTP server ftp.dlptest.com with user invalid_user: Failed to login\", which exactly matches the test assertion. No other code runs before the mocked failure, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so task_func's ftp = ftplib.FTP(...) returns the mock. The mock's login is set to raise Exception('Failed to login'), so task_func catches that and re-raises an Exception with message \"Failed to log into FTP server ftp.dlptest.com with user dlpuser: Failed to login\", which matches the test's expected string. No other operations run before the exception, so the behavior matches the test.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test patches ftplib.FTP so ftp.cwd raises Exception('Failed to change directory'). task_func calls ftp.cwd and on exception raises \"Failed to change to directory /invalid_directory on server ftp.dlptest.com: Failed to change directory\" which exactly matches the test's expected message. The patch target and default ftp_server match, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/101", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nWith the fixed seed the synthetic covariance matrix produced is cholesky-decomposable, seaborn.heatmap returns a matplotlib Axes object, and isinstance(ax, plt.Axes) will be True. No other exceptions are raised, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_heatmap_features": "```plaintext\n[Explanation]\nThe function builds a DataFrame with 13 features plus 'MEDV' (14 columns). The correlation matrix is therefore 14x14, so the heatmap's QuadMesh array has 196 elements, not the 169 (13x13) expected by the test. Thus the assertion on shape (169,) will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_heatmap_values": "```plaintext\n[Explanation]\nThe function generates a synthetic dataset with random covariance using numpy RNG; its correlation matrix will not match the specific expected numeric array. Additionally, the test uses unittest.TestCase.assertAlmostEqual on two lists (which expects numeric scalars), which would also cause the test to error. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_plot_appearance": "```plaintext\n[Explanation]\nThe test expects the returned Axes to have empty xlabel, ylabel, and title. The function returns a seaborn Axes where xlabel and ylabel are empty (so those two asserts pass), but the code sets a title via plt.title('Correlation Heatmap of Boston Housing Dataset'), so ax.get_title() will be that string, not ''. Therefore the third assertion fails.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/239", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function assigns \"stats\" as a dict, which shadows the imported scipy.stats module. When the code reaches \"pdf = stats.norm.pdf(...)\" it will raise an AttributeError ('dict' object has no attribute 'norm'). Even if that were fixed, the function never sets the Axes title (\"Histogram with PDF\"), so the test's title assertion would also fail (it even misuses assertTrue vs assertEqual). Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function shadows the imported scipy.stats module by assigning a local variable named stats (a dict). The subsequent call stats.norm.pdf(...) will raise an AttributeError because the local dict has no attribute 'norm'. This prevents the test from completing. (Even if that were fixed, ax.get_title() is never set so the title assertion would also fail.)\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function assigns a local variable named stats (a dict), which shadows the imported scipy.stats module. Later the code calls stats.norm.pdf(...), causing an AttributeError since the dict has no attribute 'norm'. This exception prevents the function from returning and thus the test will fail. (There is also an incorrect use of assertTrue in the test, but the primary failure is the AttributeError.)\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function assigns stats = {...}, which shadows the imported scipy.stats module. It then tries to call stats.norm.pdf(...), causing an AttributeError because stats is a dict. This error prevents the function from returning the expected values. Additionally, the test expects the Axes title 'Histogram with PDF', but the function never sets a title (though the code fails earlier). Thus the test will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function assigns \"stats\" as a local dict, shadowing the imported scipy.stats module. The code then calls stats.norm.pdf, causing an AttributeError ('dict' object has no attribute 'norm') when plotting the PDF. Also, the test expects a non-empty title but the function never sets one. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/374", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe code imports the third\u2011party module \"regex\" as re at top-level. If the execution environment does not have the \"regex\" package installed, this will raise ModuleNotFoundError before task_func or the tests run. The rest of the function logic correctly replaces \" with \\\" and would satisfy the assertions, but the top-level import makes the test fail in typical environments lacking the \"regex\" package.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe unittest creates Excel files with the expected strings and then directly loads file2.xlsx to check cell values. The provided code only defines task_func (no top-level execution), so it doesn't run or modify the files during the test. Therefore the workbook contents remain as created and the assertions match exactly.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe module's top-level import uses \"import regex as re\" (third-party package \"regex\"). If that package is not available in the test environment, importing the module will raise ModuleNotFoundError and the tests won't run. The test itself only inspects a created workbook and does not call task_func, so the logic of task_func is irrelevant; the failing import prevents the test from executing.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func(directory_path='/invalid/directory/') and expects a FileNotFoundError. The implementation uses glob.glob on the pattern for .xlsx files; glob returns an empty list for a non-existent directory and does not raise an exception. The function will simply return 0 (processed_count) instead of raising FileNotFoundError, so the test's assertRaises will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test creates an empty directory and calls task_func on it. task_func uses glob to find \"*.xlsx\" in the given directory; with no files, excel_files is empty, the loop is skipped, and it returns processed_count which remains 0. This matches the test's expected value.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/502", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates 7 distinct dates, creates rows for each of the five activities with random durations in [0,120], constructs a DataFrame with the required columns, and returns a seaborn/matplotlib Axes and the DataFrame. The DataFrame checks (columns, duration range, unique dates count) are satisfied. The returned ax is a matplotlib Axes and the legend will contain the five activity labels. No issues (such as wrong types or missing legend entries) are present for this test case.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function generates 10 distinct dates, creates rows for each of the 5 activities per date with durations in 0\u2013120 using the given seed, constructs a DataFrame with the expected columns, and plots lines with seaborn using hue='Activity'. The axes object returned is a matplotlib Axes with a legend containing the activity labels. All assertions in the test (columns, duration range, unique dates count, Axes type, and legend labels) are satisfied.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function builds a list of dates using range(days_in_past) so it generates exactly days_in_past distinct dates (end_date minus 0..days_in_past-1 days). The DataFrame contains one row per activity per date, so len(df['Date'].unique()) equals the input days_in_past for all tested values (1,5,10,100,500). The test only checks the number of unique dates, so the code will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function seeds Python's random module at the start of each call and generates rows in a deterministic order (dates then activities). Calling task_func twice with the same seed produces identical DataFrames (same index and values), and using a different seed produces different durations, so the assertions in the test_case_4 will hold. The plotting does not affect the DataFrame generation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test_case_5 expects task_func to raise ValueError when days_in_past is 0 or negative. The provided task_func does not validate days_in_past and will simply create an empty dates list and DataFrame (then attempt to plot), so no ValueError is raised. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/583", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function generates an RSA public key via rsa.newkeys and returns the public key object and a filename string. AES key/nonce sizes and GCM usage are correct; the private key is written to a file. The test only checks that the first return is an rsa.PublicKey instance and the second is a str, which the code satisfies. No runtime errors are apparent for this single test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_creation": "```plaintext\n[Explanation]\nThe function generates RSA keys, creates a 16-byte AES key and 12-byte nonce, encrypts the private key, builds a filename using random bytes, opens the file in binary mode and writes base64-encoded nonce, tag and ciphertext separated by colons. The test only calls task_func() and asserts the returned filename exists. All operations needed to create the file are present and use correct types/lengths (AES key is 16 bytes). Therefore the file will be created and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_file_content": "```plaintext\n[Explanation]\nThe function writes the encrypted private key as base64-encoded parts (nonce:tag:ciphertext) to a file in binary mode. Base64 output is ASCII, so opening the file in text mode and reading it returns a non-empty string. AES key/nonce sizes and rsa usage are correct. Therefore the test's assertion that the file content is truthy will pass.\n\n[Result]\nPASS\n```", "test_key_size": "```plaintext\n[Explanation]\nThe code generates an RSA key pair with rsa.newkeys(2048), so pub_key.n.bit_length() will be 2048. The test asserts the key size equals 512, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_unique_file_per_call": "```plaintext\n[Explanation]\nThe function generates a filename using 8 random bytes from Crypto.Random.get_random_bytes and returns a new random filename each call. The test only asserts that two successive calls produce different filenames. Collisions with 8 random bytes are astronomically unlikely, and there is no deterministic seeding in the code. Other operations (RSA keygen, AES encryption, file writing) do not affect filename uniqueness. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_encryption_decryption": "```plaintext\n[Explanation]\nThe test expects the file to contain a single base64-encoded ciphertext which it decodes and then decrypts using AES.MODE_EAX with the returned nonce. The implementation writes three base64 parts separated by ':' (nonce:tag:ciphertext). b64decode on the whole file will concatenate and decode all parts into nonce||tag||ciphertext. The test then tries to decrypt that concatenated blob with EAX (while the code encrypted with GCM), so the decrypted bytes won't be a valid PKCS#1 private key and rsa.PrivateKey.load_pkcs1 will fail. Mode/format mismatch and extra data cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/763", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function reads the JSON file, computes mean and median using numpy for each key, writes a CSV with columns key, mean, median, and returns the results. The test creates JSON files with a single key each (two integer values), calls task_func on one file, reads the CSV and compares the parsed floats to the expected mean/median. Data types and CSV format match expectations, and there are no path or I/O issues for the temporary output file. Therefore the code will produce the expected values and pass the test.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function reads the JSON file, computes mean and median with numpy, writes them to CSV, and the test reads the CSV as strings and converts to float for comparison. For the provided input (two values for 'key2'), mean and median are 35.0; types and CSV formatting match what the test expects. No ordering dependency or other errors affect this single-case check.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function reads the JSON file with [{'key3': 50}, {'key3': 60}], computes mean and median (55.0 each), writes a CSV with header key,mean,median and the single row for key3. The test reads that CSV and compares floats to the expected 55.0 values. There are no ordering or type issues (floats are converted and compared correctly). numpy usage is acceptable. The code will produce the expected output for the provided test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function reads the JSON file with [{'key4': 70}, {'key4': 80}], computes mean and median (both 75.0), writes them to CSV, and the test reads and compares those values. The code handles extraction, calculation (using numpy), CSV writing, and the assertions match the produced values, so it will pass the provided test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function reads the JSON, computes mean and median with numpy, writes them to a CSV, and returns the results. For the test file containing [{'key5': 90}, {'key5': 100}] it will write 'key5' with mean and median 95.0. The test reads the CSV, converts strings to float, and compares to expected 95.0; all operations and types match and no ordering dependency affects the lookup. Therefore the test should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/916", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns Axes objects and sets the correct titles, so those assertions pass. However the test expects the histogram x-label to be 'closing_price' and its y-label to contain 'Count'. The code sets ax2.set_xlabel('Closing Price') and ax2.set_ylabel('Frequency'), which do not match the expected 'closing_price' and do not contain 'Count'. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_df": "```plaintext\n[Explanation]\nThe function calls ax1.boxplot(df['closing_price']) with an empty series. Matplotlib's boxplot relies on numpy percentile calculations which raise an error for empty input, so the function will raise an exception before returning the axes. The test therefore will not complete and will fail.\n\n[Result]\nFAIL\n```", "test_invalid_column": "```plaintext\n[Explanation]\nThe function accesses df['closing_price'] directly. When the DataFrame only has 'price', this indexing raises a pandas KeyError. The test expects a KeyError, so the behavior matches.\n\n[Result]\nPASS\n```", "test_single_value_df": "```plaintext\n[Explanation]\nThe function returns two matplotlib Axes objects and uses ax.boxplot and ax.hist on the provided single-value Series. Both plotting calls succeed with a single data point, and the returned objects are Axes instances. The test checks isinstance against plt.Axes and that each axis has data via has_data(); both are true after these plotting calls. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_large_values_df": "```plaintext\n[Explanation]\nThe function creates two matplotlib axes via plt.subplots and draws a boxplot and histogram using the provided 'closing_price' Series. The returned objects are AxesSubplot instances (subclasses of plt.Axes), so the isinstance checks pass. Both ax1.boxplot(...) and ax2.hist(...) add artists/data to the axes, so has_data() will return True for each. No errors or mismatches are present for the given input.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1003", "raw_responses": {"test_valid_xml": "```plaintext\n[Explanation]\nThe test configures the mock for urlopen so that read() returns bytes when used as a context manager (mock_urlopen.return_value.__enter__.return_value.read.return_value = valid_xml_data). The function, however, calls response = urllib.request.urlopen(url) and then response.read() without using a with-statement, so response.read() returns a MagicMock (not the bytes). Passing that to etree.fromstring raises an exception caught by the generic except, causing task_func to raise ValueError instead of returning the expected DataFrame. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_empty_xml": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but sets the mock's context-manager return value (.return_value.__enter__.return_value.read.return_value) while the code calls urlopen() without a with-statement. As a result response.read() returns a MagicMock (not bytes), causing etree.fromstring to raise an exception. That exception is caught by the broad except Exception clause and a ValueError is raised. The test expects a ValueError, so the code will raise the expected error and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_different_structure_xml": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but configures the mock for use as a context manager (mock.return_value.__enter__....read). The function calls urlopen() without a with-statement, so response.read() returns an unconfigured Mock object (not the bytes set in the test). etree.fromstring(xml_content) will raise an exception when given that Mock; the function catches all other exceptions and raises ValueError(\"XML structure does not match expected format.\"). The test expects a ValueError, so the function's behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise a generic Exception. In task_func that will be caught by the broad \"except Exception\" clause, which raises a ValueError. The test only asserts that ValueError is raised (not the message), so the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_non_xml_data": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but configures the mock for a context-manager usage (mock.return_value.__enter__...), while task_func calls urlopen() directly (no with). Therefore response.read() will be a MagicMock (not bytes), causing etree.fromstring(...) to raise an exception (TypeError or similar). That exception is caught by the broad except Exception clause, which raises ValueError. The test expects a ValueError, so the function will raise it and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1129", "raw_responses": {"test_download_with_direct_key": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that only provides .content. The code calls response.raise_for_status(), which does not exist on the mock object, causing an AttributeError before the file is written. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_download_with_incorrect_key": "```plaintext\n[Explanation]\nThe function accesses data[unknown_key] directly. Given the JSON only contains \"unknown\", requesting 'nonexistent' will raise a KeyError before any network call or file operations. The test patches requests.get but it isn't reached. Therefore the test's assertRaises(KeyError) will succeed.\n\n[Result]\nPASS\n```", "test_download_with_specified_directory": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that only has a content attribute. The code calls response.raise_for_status() after requests.get; since the mock has no raise_for_status method, an AttributeError will be raised and the file write/assertions won't run. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_download_to_default_directory": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MockResponse that only has a content attribute and no raise_for_status method. The code calls response.raise_for_status(), which will raise an AttributeError on the mock response, so the test will fail.\n\n[Result]\nFAIL\n```", "test_multiple_downloads": "```plaintext\n[Explanation]\nThe test patches requests.get with a mock response that provides only .content. The code calls response.raise_for_status() after requests.get; the mock does not implement raise_for_status, causing an AttributeError before files are written. Therefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/241", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function uses sklearn.preprocessing.StandardScaler to normalize the data, which produces z-scores (mean 0, unit variance). The test expects L2-normalized values ([0.18257419, 0.36514837, 0.54772256, 0.73029674]) corresponding to dividing by the vector's L2 norm. Therefore the normalized_array produced by the code will not match the expected values and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function extracts arr correctly and the plot/title/labels are set as expected. However, the code uses sklearn.preprocessing.StandardScaler (z-score standardization) to produce normalized_array, while the test expects L2 normalization (arr / np.linalg.norm(arr)). These produce different values for the provided input, so the assertion comparing norm_arr to manual_norm will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function uses sklearn.preprocessing.StandardScaler, which for a single-value input produces a standardized value of 0.0 ((x-mean)/scale with scale forced to 1 for zero variance). The test expects the normalized array to be [1.0], so the normalized output will not match. Other assertions (original array, title, labels) would pass, but the normalization check fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function uses sklearn.preprocessing.StandardScaler which performs zero-mean, unit-variance scaling. The test expects L2 (vector) normalization arr / np.linalg.norm(arr). For the constant input [4,4,4,4], StandardScaler produces zeros (or at least not the L2-normalized [0.5,0.5,0.5,0.5]), so the assertion comparing norm_arr to manual_norm will fail. Other checks (array contents, title, labels) would pass, but the normalization check causes failure.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function builds original_array as np.array([]) for an empty input, then calls sklearn.preprocessing.StandardScaler().fit_transform on original_array.reshape(-1,1). Scikit-learn's transformers require at least one sample; fit_transform on an empty array raises a ValueError, so the function will error instead of returning empty normalized array and plot. Therefore the test (which expects no error and empty arrays) will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/108", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function validates the DataFrame and column types correctly for the provided test DataFrame (30 daily integer values). seasonal_decompose is called with period=7 which is valid for 30 observations, and it returns a statsmodels.tsa.seasonal.DecomposeResult. No validation errors are triggered and plotting does not interfere with returning the result. Therefore the test asserting the result is a DecomposeResult should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_input_data": "```plaintext\n[Explanation]\nThe test calls task_func with a DataFrame that lacks the required 'group', 'date', and 'value' columns. The function checks for these columns and raises a ValueError if any are missing, so the expected ValueError will be raised and the test will pass.\n\n[Result]\nPASS\n```", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe function checks the input type immediately: it raises ValueError if df is not a pandas DataFrame. Calling task_func('not a dataframe') (a string) triggers that check and raises ValueError as the test expects. No other code runs before the check that could interfere with the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_model": "```plaintext\n[Explanation]\nThe function explicitly validates decomposition_model and raises ValueError if it's not 'additive' or 'multiplicative'. The provided DataFrame and other parameters are valid, so calling task_func(..., decomposition_model='invalid_model') will raise the expected ValueError.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_frequency": "```plaintext\n[Explanation]\nThe test expects a ValueError when freq='invalid_freq'. The function only checks that freq is a string, not whether it's a valid frequency, so no ValueError is raised. The rest of the function runs (seasonal_decompose with period=7 on positive values 1..30) and returns normally. Therefore the test's assertRaises will fail.\n\n[Result]\nFAIL\n```", "test_insufficient_data": "```plaintext\n[Explanation]\nThe function calls statsmodels.tsa.seasonal.seasonal_decompose with period=7. seasonal_decompose requires at least 2*period observations (>=14) and will raise a ValueError for shorter series. The test provides only 5 rows, so task_func will propagate that ValueError, satisfying the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_components_existence": "```plaintext\n[Explanation]\nThe function validates inputs, sets the datetime index, and calls statsmodels.tsa.seasonal.seasonal_decompose with period=7. The test DataFrame has 30 daily positive numeric values (1..30), so:\n- validation passes,\n- multiplicative decomposition is valid (no zeros/negatives),\n- series length (30) is sufficient for period=7,\n- the DecomposeResult returned contains attributes 'trend', 'seasonal', and 'resid' (even if some entries are NaN at the edges).\nPlotting does not affect the returned result checked by the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_component_shapes": "```plaintext\n[Explanation]\nThe function validates the DataFrame and uses statsmodels.tsa.seasonal.seasonal_decompose with period=7 on the 'value' series (1..30). Values are numeric and positive so multiplicative decomposition is valid. The decomposition result's trend, seasonal, and resid attributes have the same length as the input series (30), so the test assertions comparing shapes will be satisfied. Creating matplotlib figures is harmless in the test context. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_additive_model": "```plaintext\n[Explanation]\nThe function validates inputs, sets the datetime index, and calls statsmodels.tsa.seasonal.seasonal_decompose with period=7 and the additive model. For the provided 30-day linear series (1..30) this produces a DecomposeResult and the seasonal component matches the expected repeating tiny values. There are no blocking issues (dtype checks pass, plotting side-effects are harmless), so the test assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_numeric_values": "```plaintext\n[Explanation]\nThe test sets one 'value' to a string, making the 'value' column non-numeric (object dtype). The function checks pd.api.types.is_numeric_dtype(df['value']) and will raise ValueError when this is False. The test expects a ValueError, so the behavior matches.\n\n[Result]\nPASS\n```", "test_missing_values": "```plaintext\n[Explanation]\nSetting a value to None makes the 'value' column float with NaN, so the numeric-dtype check passes. statsmodels.tsa.seasonal.seasonal_decompose raises a ValueError when the series contains NaN (e.g. \"y contains NaN\"), so task_func will raise ValueError for the provided df_with_missing. The test expects a ValueError, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/587", "raw_responses": {"test_file_encryption": "```plaintext\n[Explanation]\nThe function writes an encrypted file named test_file.txt.encrypted and returns its name. The test creates the file, calls task_func, and asserts that the returned encrypted file exists. The code reads/writes in binary correctly, pads and encrypts the data, and saves iv + ciphertext. RSA encrypting the 32-byte AES key with a 2048-bit key is valid. Top-level imports include os so the test's use of os (in the same execution context) works. Although the tearDown looks for a different AES key filename, that doesn't affect the assertion. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_encrypted_key_file_creation": "```plaintext\n[Explanation]\nThe code depends on external packages (\"rsa\" and \"cryptography.hazmat...\") that are not part of the Python standard library. If those modules are not installed in the test environment, the import at the top will raise ImportError before the test runs. Even though the function logic would create and return the encrypted key file path (which the test checks), the missing external dependencies cause the test to fail at import time.\n\n[Result]\nFAIL\n```", "test_public_key_type": "```plaintext\n[Explanation]\nThe function returns the RSA public key produced by rsa.newkeys, which is an instance of rsa.PublicKey, matching the test's isinstance check. The created filenames do not affect the assertion. Although the code imports third-party modules (rsa, cryptography), the test assumes those imports succeed; given that, the type assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_encrypted_file_content": "```plaintext\n[Explanation]\nThe test's tearDown uses the name os but never imports os in the test module. That will raise a NameError during tearDown (which always runs after setUp/test), causing the test to error out. Although the provided task_func correctly writes an encrypted file different from the original, the missing os import in the test causes the overall test run to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_aes_key_file_content": "```plaintext\n[Explanation]\nThe function writes an encrypted AES key file named \"<file_path>.aes_key.encrypted\" and returns that filename. In the test, task_func('test_file.txt') will create 'test_file.txt.aes_key.encrypted'; os.path.exists(encrypted_key_file) will be True and the file size will be > 0 because rsa.encrypt produces non-empty bytes. The test's tearDown refers to os (which is imported by the provided code), so no NameError occurs. Therefore the assertions in the single test pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test calls task_func('non_existent_file.txt') inside assertRaises(FileNotFoundError). In task_func the rsa keys are generated but the file is opened with open(file_path, 'rb') which will raise FileNotFoundError for a non-existent path and that exception is not caught, satisfying the test. The module-level imports include os so the test's tearDown referencing os will work when the code and test run together.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/765", "raw_responses": {"test_files_with_content": "```plaintext\n[Explanation]\nThe function creates the target directory, iterates the provided mapping, and copies files whose content is not None and whose paths exist. The test creates two actual files in 'test_dir', passes their full paths to task_func with target_dir 'target_dir', and expects both copied. shutil.copy2 will copy them and the returned destination paths start with 'target_dir'. All assertions match, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_files_with_no_content": "```plaintext\n[Explanation]\nThe test creates two files in test_dir: file1.txt with \"Hello\" and file2.txt opened but left empty because its content is None. task_func only copies entries whose value is not None and whose source path exists. It will copy file1.txt (content not None) and skip file2.txt (value is None), producing one copied file in target_dir. The assertions (len==1, file exists, path starts with target_dir) will be satisfied.\n\n[Result]\nPASS\n```", "test_files_do_not_exist": "```plaintext\n[Explanation]\nThe function only copies files when content is not None AND os.path.exists(file_path) is True. The test creates the directories but does not create the files, so the existence check fails for both entries and nothing is copied. The returned list is empty, so len(copied_files) == 0 holds.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_mixed_case": "```plaintext\n[Explanation]\nThe test creates three files in test_dir (one empty because content is None), passes their full paths with corresponding contents to task_func, and expects two files copied to target_dir. The function:\n- creates target_dir,\n- skips items whose value is None,\n- checks file existence (the empty file still exists but is skipped due to None),\n- copies the two non-None files and returns their destination paths.\n\nAll assertions (count == 2, files exist, paths start with target_dir) will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dict": "```plaintext\n[Explanation]\nThe function returns an empty list when given an empty dict. The target directory creation is harmless, the loop does nothing, and copied_files remains []. The test asserts length 0, so it will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/917", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function fits an ARIMA(1,1,1) on the provided closing_price series and returns the forecast as a list and the matplotlib Axes. For the given input values the ARIMA forecast is deterministic and matches the expected numbers within 2 decimal places. The returned ax is an Axes and the plotted first line is the historical series. The final assertion in the test uses assertTrue with the y-data list (a non-empty list is truthy), so it will not fail. Therefore the code will pass the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function fits an ARIMA(1,1,1) to the provided 7-day increasing series, forecasts 7 steps, returns the forecast as a list and the matplotlib Axes. The plotted historical y-data matches the input (returned as floats equal to the integer list). The expected forecast values are consistent with deterministic ARIMA behavior for this simple series and the test allows a tolerance of 2 decimal places. The Axes type and list type checks also pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function likely produces the expected numeric forecasts and returns a matplotlib Axes, so the numeric checks would pass. However, the test's final assertion calls assertAlmostEqual on two lists (lines[0].get_ydata().tolist() and the expected list). unittest.TestCase.assertAlmostEqual expects numeric values and attempts numeric subtraction, so applying it to lists raises a TypeError (or otherwise fails). Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function will return a list of forecasts and a matplotlib Axes; the numeric forecasts should match the expected values within 2 decimal places. However, the provided test erroneously calls assertAlmostEqual on two list objects (lines[0].get_ydata().tolist() and a list of ints), which leads to a TypeError or a ValueError during the comparison (unittest.assertAlmostEqual is intended for scalars). Because of this incorrect assertion, the test will fail regardless of the correctness of the implementation.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function fits an ARIMA(1,1,1) to the provided increasing series, forecasts 7 steps, and returns a list of floats plus a matplotlib Axes. The returned forecast values should match the expected values within 2 decimal places for a simple linear series. The Axes object is returned and the test's final line uses assertTrue incorrectly (it only checks truthiness of a non-empty list), so it won't fail. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/1004", "raw_responses": {"test_word_frequencies": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen expecting it to be used as a context manager (it sets mock_urlopen.return_value.__enter__.return_value.read.return_value). The provided function calls urllib.request.urlopen(url) without a with-statement, so response.read() will return a Mock (not the configured bytes) and response.read().decode('utf-8') will not yield the expected string (it will raise or produce an invalid type for re.findall). Therefore the function will not produce the expected Counter and will fail the test.\n\n[Result]\nFAIL\n```", "test_empty_file": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen but sets the mock's __enter__.return_value.read.return_value = b'' (i.e., for a context-manager use). The function calls urllib.request.urlopen(url) without a with-statement, so response.read() returns a Mock (not bytes) and .decode produces another Mock. Passing that to re.findall causes a TypeError (expected string/bytes). Additionally, the test's tearDown calls plt.clf() but plt is not imported in the test, which would raise a NameError. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```", "test_non_text_file": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise an Exception. task_func calls urllib.request.urlopen as its first action and does not catch exceptions, so the Exception will propagate. The test uses assertRaises(Exception), so this behavior satisfies the test. The plotting cleanup in tearDown uses plt, which is available because the module under test imports matplotlib.pyplot as plt at top level.\n\n[Result]\nPASS\n```", "test_special_characters": "```plaintext\n[Explanation]\nThe test patches urlopen expecting it to be used as a context manager (it sets return_value.__enter__.return_value.read.return_value = b'1234567890'), but task_func calls urllib.request.urlopen(url) directly (not using \"with\"). As a result response.read() returns a Mock (not bytes), so response.read().decode(...) and subsequent re.findall(data) will receive Mock objects and raise a TypeError (or similar). Additionally, the test's tearDown calls plt.clf() but the test file does not import matplotlib.pyplot as plt, which would cause a NameError in tearDown. Therefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```", "test_large_input": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen expecting it to be used as a context manager (it sets return_value.__enter__.return_value.read.return_value), but task_func calls urlopen(url) directly (no with). As a result response.read() returns a Mock (its read.return_value wasn't set), and calling .decode('utf-8') on that Mock raises an AttributeError. Therefore the function will not succeed under the provided test.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/1137", "raw_responses": {"test_phone_numbers_page_1": "```plaintext\n[Explanation]\nThe input file contains \"Phone: +1234567890\". The regex's second alternative matches but the country-code capture excludes the '+' (the \\+? is outside the capturing group), so the code returns \"1234567890\" (and writes that to JSON) rather than the expected \"+1234567890\". Thus the returned/saved list will not match the expected value.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_phone_numbers_page_2": "```plaintext\n[Explanation]\nThe regex captures only digit groups and the code joins non-empty capturing groups, so the leading '+' is dropped (e.g. '+9876543210' becomes '9876543210'). The test expects strings with the '+' preserved, so the returned and saved lists will not match the expected list.\n\n[Result]\nFAIL\n```", "test_phone_numbers_page_3": "```plaintext\n[Explanation]\nThe function logic would return an empty list for the given HTML (no digits present), and it writes [] to the output file. However, the module imports bs4 and requests at top-level; if those external packages are not available in the test environment an ImportError will occur before the test runs. Because the test environment may not provide these third-party packages, the code is not guaranteed to run successfully.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_phone_numbers_page_4": "```plaintext\n[Explanation]\nThe function reads the local file correctly and the regex matches the phone number, but the second alternative captures only digits (the group for the country code excludes the leading '+'). The code concatenates captured groups, producing '919876543210' (no '+'), while the test expects '+919876543210'. Therefore the returned and saved result will not match the expected value.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_phone_numbers_page_5": "```plaintext\n[Explanation]\nThe regex only captures digit groups and not the leading '+'; for numbers like \"+14151234567\" the code assembles groups into \"4151234567\" (or similar) without the '+1' prefix. The test expects strings with the leading '+', so the returned list and saved JSON will not match the expected_numbers.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/15", "raw_responses": {"test_successful_command_execution": "```plaintext\n[Explanation]\nThe function reads the CSV, runs \"echo Hello\" via subprocess, writes stdout (\"Hello\\n\") to an output file and returns its absolute path. The test expects one result and checks the file contains \"Hello\". os.path.join(self.output_dir_path, result[0]) returns result[0] unchanged because it's absolute. No errors or mismatches are present.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe code begins by checking os.path.exists(commands_file_path) and raises FileNotFoundError when the file does not exist. The test calls task_func with a nonexistent CSV path, so the function will raise FileNotFoundError as expected.\n\n[Result]\nPASS\n```", "test_invalid_command": "```plaintext\n[Explanation]\nThe function creates an output file for the single CSV command and, on command failure, writes the command string and the shell stderr into the file. For a nonexistent command the shell returns nonzero exit code and stderr contains \"not found\" on Unix-like systems. The returned path is an absolute path (os.path.join used inside), so opening it succeeds. Therefore the assertions (one result and file contains the command and \"not found\") will be satisfied in the expected environment.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_csv_file": "```plaintext\n[Explanation]\nThe function reads the CSV and iterates over csv.reader. For an empty file the reader yields no rows, so the loop body never runs, output_files stays empty, and the function returns an empty list. The test expects length 0, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_mixed_commands": "```plaintext\n[Explanation]\nThe function creates absolute output file paths, runs each command with shell=True capturing stderr, and writes stderr on failures. The test expects two results and that the second output file contains the invalid command and \"not found\". On a Unix-like system the shell error (e.g. \"/bin/sh: ... not found\") will be captured and written, satisfying the assertions. The path joining in the test works because os.path.join with an absolute path returns that absolute path.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_command_failure_with_specific_exit_code": "```plaintext\n[Explanation]\nThe test expects the output file to contain the string \"Error executing command\". The implementation, for a non-zero return code, writes \"Command failed with exit code {code}\" and details, but never writes \"Error executing command\". Therefore the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/120", "raw_responses": {"test_reproducibility_with_seed": "```plaintext\n[Explanation]\nThe function seeds Python's random module each call (random_seed(seed)) and uses randint with the same range for a fixed number of iterations (366 days for 2020, a leap year), so repeated calls with the same seed produce identical Series (assert_series_equal will pass). The Series of datetime objects will be converted to strings in the form 'YYYY-MM-DD' by dates1.astype(str).tolist(), matching the expected formatting. The generation method (randint with seed 42 over 366 draws) corresponds to the provided expected sequence.\n\n[Result]\n```plaintext\nPASS\n```", "test_series_length": "```plaintext\n[Explanation]\nThe test expects len(dates) == (end_date - start_date).days, which for 2020-01-01 to 2020-01-10 is 9. The function computes total_days = delta.days + 1 (10) and generates that many random dates, returning a Series of length 10. This off-by-one mismatch causes the assertion to fail.\n\n[Result]\nFAIL\n```", "test_invalid_date_types": "```plaintext\n[Explanation]\nThe function explicitly checks types at the start:\n- if not isinstance(start_date, datetime): raise ValueError\n- if not isinstance(end_date, datetime): raise ValueError\nThe test calls task_func with a string for start_date in the first case and a string for end_date in the second, so both calls will raise ValueError as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_start_date_after_end_date": "```plaintext\n[Explanation]\nThe test passes a start_date later than end_date. The function validates inputs and explicitly raises ValueError when start_date > end_date, so the expected exception is raised.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_return_type": "```plaintext\n[Explanation]\nThe function task_func uses valid default datetime objects, validates inputs, and returns pd.Series(random_dates). There are no runtime errors for the default call, and the returned object is an instance of pandas.Series as the test asserts.\n\n[Result]\nPASS\n```", "test_date_within_range": "```plaintext\n[Explanation]\nThe function validates inputs, computes total_days = (end_date - start_date).days + 1, and for each of those days generates a random offset randint(0, total_days-1) producing dates within start_date..end_date inclusive. It returns a pandas Series of datetime-like values. Iterating the Series yields timestamp-like objects that compare correctly with datetime instances, so every produced date satisfies start_date <= date <= end_date. The provided test (start_date 2020-01-01 to 2020-01-05) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/267", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function does add key \"a\" and returns a numpy ndarray for the FFT, so those assertions pass. However the test expects specific plot text:\n- Title expected \"FFT of the Signal\" but code sets \"FFT of the signal\" (case mismatch).\n- X label expected \"Frequency [Hz]\" but code sets \"Frequency (Hz)\".\n- Y label expected \"Frequency Spectrum Magnitude\" but code sets \"Magnitude\".\nThese label/title mismatches will cause the corresponding assertions to fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test expects specific title and axis label strings, but the function sets different ones: it uses 'FFT of the signal' instead of 'FFT of the Signal', 'Frequency (Hz)' instead of 'Frequency [Hz]', and 'Magnitude' instead of 'Frequency Spectrum Magnitude'. Although the key 'a' will be present and fft_result is a numpy array, the mismatched strings cause the assertions to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function adds key 'a' to data and returns an ndarray FFT, so those assertions pass. However the test expects specific plot text:\n- Actual title: 'FFT of the signal' vs expected 'FFT of the Signal' (case mismatch).\n- Actual xlabel: 'Frequency (Hz)' vs expected 'Frequency [Hz]'.\n- Actual ylabel: 'Magnitude' vs expected 'Frequency Spectrum Magnitude'.\nThese string mismatches cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function does add data[\"a\"]=1 and returns an ndarray from fftpack.fft, so the first two assertions would pass. However the plot text does not match the test expected strings: the code sets title \"FFT of the signal\" (test expects \"FFT of the Signal\"), xlabel \"Frequency (Hz)\" (test expects \"Frequency [Hz]\"), and ylabel \"Magnitude\" (test expects \"Frequency Spectrum Magnitude\"). These mismatches cause assertion failures.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function does add key 'a' and returns an ndarray for the FFT, so those assertions would pass. However the test expects specific plot text that does not match the code: the code sets title 'FFT of the signal' (lowercase 'signal') but test expects 'FFT of the Signal'; xlabel in code is 'Frequency (Hz)' but test expects 'Frequency [Hz]'; ylabel in code is 'Magnitude' but test expects 'Frequency Spectrum Magnitude'. These mismatches cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/503", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe code will not pass the test. Two problems cause the assertions to fail:\n1. The DataFrame index is built from string dates (strftime), so df.index[-1] is a str and has no .date() method \u2014 this will raise an AttributeError in the test.\n2. The DataFrame values are numpy.float64, and isinstance(np.float64(...), (int, float)) is False, so the numeric-type assertion would also fail.\n\nTherefore the test will not succeed.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function builds the DataFrame index as strings via strftime('%Y-%m-%d'), so df.index[-1] is a string and has no .date() method. The test calls df.index[-1].date(), which will raise an AttributeError and cause the test to fail. Other assertions (row count and columns) would pass, but the date check fails.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function builds the DataFrame index as strings using strftime, so df.index[-1] is a string (no .date() method) and, moreover, the order produced (i=0 is today) makes the last index an older date, not today. Therefore the test assertion df.index[-1].date() == datetime.now().date() will fail (raising AttributeError or mismatching date).\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects ValueError for days_in_past = -1, 0, and 2.5. The function has no explicit validation:\n- days_in_past = -1: np.random.random(-1) raises a ValueError, so this assertion would pass.\n- days_in_past = 0: range(0) and np.random.random(0) are valid (produce empty arrays); no exception is raised, so the test expecting a ValueError fails here.\n- days_in_past = 2.5: range(2.5) raises a TypeError (not ValueError), so that would also not meet the test's expectation.\n\nBecause the call with days_in_past=0 does not raise ValueError, the test fails overall.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test expects ValueError for empty or invalid stock_names. The provided task_func does not perform any validation: it accepts an empty list (creating an empty DataFrame with a date index) and accepts non-string items as column names. Therefore no ValueError will be raised and the unittest will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_6": "```plaintext\n[Explanation]\nBoth calls with random_seed=42 will produce identical numpy random sequences and thus identical DataFrames (same index, columns, dtypes, and values). The call with random_seed=99 produces different values, so DataFrame.equals returns False against the seed-42 results. The date index formatting is the same across the rapid successive calls, so the test assertions hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function builds a list of dates with length `days_in_past` using range(days_in_past) and generates random price arrays of the same length for each stock. Creating the DataFrame with that index yields `df.shape[0] == days_in_past`, so calling task_func(days_in_past=366) results in 366 rows.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function builds a DataFrame using the provided stock_names as keys in the data dict and returns it. Calling task_func(stock_names=['ABC']) will create a column 'ABC' (with 7 random values by default), so 'ABC' will be in df.columns and the assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/399", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe plot creates two lines, so len(ax.lines) == 2 passes. However, the test checks whether the exact labels 'sin' and 'cos' are present in the list of line labels. The function sets labels like 'sin(1\u03c0t)' and 'cos(1\u03c0t)', not 'sin' or 'cos', so the membership checks for 'sin' and 'cos' fail. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nFor frequency=0 the code generates t as an array and computes sine as np.sin(0) -> all 0.0 and cosine as np.cos(0) -> all 1.0. It plots sine first and cosine second, so ax.lines[0] and ax.lines[1] contain the expected y-data arrays. The test's comparisons (== 0 and == 1) succeed, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function checks inputs at the start: it raises ValueError when frequency < 0 and when sample_size <= 0. The test calls task_func(-1) and task_func(5, -1), both of which trigger these checks and raise ValueError before any plotting occurs. Therefore the test expectations are met.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nInputs are valid (frequency=10, sample_size=5000). The function generates sine and cosine over t in [0,2] with 20 full sine cycles, plots sine then cosine (so ax.lines[0] and ax.lines[1] exist), and returns fig, ax. The sampled y arrays contain both non-negative and non-positive values (including zeros at endpoints and both positive and negative samples), so both assertions in the test are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function calls ax.legend() after plotting two labeled lines, so ax.get_legend() will return a Legend object (not None). No input validation errors occur for frequency=1 and default sample_size, and the plotting code uses valid numpy/matplotlib calls. Therefore the test's assertion that the legend is not None will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/590", "raw_responses": {"test_valid_url": "```plaintext\n[Explanation]\nThe test calls task_func with a live URL and only verifies that a DataFrame with columns 'text', 'href', and 'fetch_time' is returned. task_func requires internet access and the pyquery package to fetch and parse the remote HTML. In a typical isolated test environment this network access is unavailable (urllib.request.urlopen will raise URLError), and/or pyquery may not be installed; either will cause the function to raise an exception and the test to fail. Therefore the code will not pass the provided test case in such an environment.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\ntask_func calls urllib.request.urlopen and, if urlopen fails for the invalid domain, the function catches urllib.error.URLError and re-raises urllib.error.URLError. The test expects a urllib.error.URLError for the invalid URL, so the exception type matches and the test will pass.\n\n[Result]\nPASS\n```", "test_network_error": "```plaintext\n[Explanation]\nThe test patches urllib.request.urlopen to raise urllib.error.URLError. task_func calls urllib.request.urlopen inside a try block; the raised URLError is caught by the except urllib.error.URLError clause and re-raised as urllib.error.URLError. Therefore task_func will raise a URLError as the test expects, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_url": "```plaintext\n[Explanation]\nThe function begins with \"if not url or not isinstance(url, str): raise ValueError(...)\" so calling task_func('') (empty string) will raise ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_dynamic_comparison": "```plaintext\n[Explanation]\nThe function will not match the test's expected DataFrame. The test builds expected rows including anchors with href=None and empty text, while task_func replaces missing href with '' (empty string) and also omits anchors when both text and href are empty. These differences (None vs '' and missing rows) cause the DataFrames to differ, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_fetch_time_format": "```plaintext\n[Explanation]\nThe function sets 'fetch_time' using datetime.now().strftime('%Y-%m-%d %H:%M:%S') and assigns that string to every row in the DataFrame; the test validates parsing each entry with datetime.strptime using the same format. Therefore, given successful URL fetch and available dependencies, the produced 'fetch_time' values match the required format and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/928", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe code builds keys from itertools.permutations(letters, 2), which excludes repeated-letter pairs like 'zz'. As a result result does not contain the key 'zz', so the test's access result['zz'] will raise a KeyError. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe code builds two-letter keys using itertools.permutations(letters, 2), which excludes same-letter pairs like 'aa' or 'bb'. As a result the returned dict has no 'aa' or 'bb' keys, so result['aa'] in the test will raise a KeyError (the expected counts for same-letter pairs are never present). Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function generates keys from itertools.permutations(letters, 2), which excludes repeated-letter pairs like 'zz'. The test accesses result['zz'], which will raise KeyError (the code never includes identical-letter combinations), so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function generates two-letter keys using itertools.permutations(..., 2), which excludes repeated-letter pairs like 'zz'. The test accesses result['zz']; since that key is never created the lookup raises a KeyError (causing the test to fail). Other checked keys exist and have correct counts, but the missing 'zz' key causes failure.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function generates two-letter combinations using itertools.permutations(letters, 2), which only produces pairs of distinct letters (e.g., 'ab') and does not include repeated-letter combinations like 'zz' or 'aa'. For an empty input word the returned dict will therefore contain keys for distinct-letter pairs but not for 'zz'. The test accesses result['zz'], which will raise a KeyError, so the test will not pass.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/771", "raw_responses": {"test_file_creation_and_content": "```plaintext\n[Explanation]\nThe test patches os.listdir but the code uses Path(directory).glob(\"*.csv\") to find files, so the mocked listing is never used and no files will be discovered. As a result the function returns an empty list instead of the expected ['Shan.csv','Shannon.csv']. (Additionally, the test expects open calls without the newline/encoding kwargs, which would also mismatch even if files were found.) Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_empty_directory": "```plaintext\n[Explanation]\nThe test patches os.listdir to simulate an empty directory, but task_func uses pathlib.Path.glob to find CSV files and never calls os.listdir. The mock has no effect, so the function will read the real filesystem at '/mnt/data/test_data'. Because the test expects [] based on the mock, this mismatch will cause the test to fail (unless the real directory happens to be empty, which the test does not guarantee). Therefore the code does not reliably pass the test.\n\n[Result]\nFAIL\n```", "test_non_matching_files": "```plaintext\n[Explanation]\nThe test patches os.listdir to return non-matching filenames, but the function uses Path(directory).glob(\"*.csv\") (not os.listdir). glob will yield no files (especially since no /mnt/data/test_data CSVs are present), so the function returns an empty list. That matches the expected result [].\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_mixed_file_types": "```plaintext\n[Explanation]\nThe test patches os.listdir but the function uses Path.glob(\"*.csv\") (not os.listdir), so the test's mocked file list is not used; glob will inspect the real filesystem and likely find no matching files in the test environment. Even if files were found, the test expects open() to be called as open(path, 'r'), but the function calls open(..., 'r', newline='', encoding='utf-8'), so mock_file.assert_any_call(expected_path, 'r') would not match due to extra keyword arguments. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches os.listdir and builtins.open to force an IOError when files are opened and to simulate a directory containing 'Shan-1.csv'. The function uses pathlib.Path.glob to find CSVs; Path.glob does not rely on the patched os.listdir in this context, so it will not yield the mocked filename and the loop won't call open. Because open is never invoked, no IOError is raised and the test's assertRaises(IOError) fails.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/1006", "raw_responses": {"test_valid_zip_url": "```plaintext\n[Explanation]\nThe patched requests.get returns status_code 200 and content-type application/zip but the content (b'1') is not a valid ZIP. The function writes the file, attempts to open it with ZipFile, catches the BadZipFile exception and returns the error string instead of raising. The test contains no assertions and will pass as no exception is raised.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.RequestException. The function wraps the call in a try/except that catches requests.exceptions.RequestException and returns \"Error: Unable to download the file from the provided URL.\" Therefore the function returns the expected string and the assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_zip_content": "```plaintext\n[Explanation]\nThe test patches requests.get to return status_code 200, headers={'Content-Type': 'text/plain'}, and a .txt URL. The code checks response.headers.get('content-type', '') (lowercase key) and also whether the URL ends with '.zip'. The header lookup returns '' (key case mismatch) so 'application/zip' is not found and the URL does not end with '.zip', causing the function to return \"Error: The URL does not point to a ZIP file.\" which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_download_invald_zip_file": "```plaintext\n[Explanation]\nThe mocked response has status_code 200 and headers indicating a ZIP (the code also accepts .zip URL suffix), so it writes the content to a .zip file. Attempting to open the saved file with zipfile.ZipFile on invalid ZIP bytes will raise BadZipFile, which the function catches and returns \"Error: The downloaded file is not a valid ZIP file.\" This matches the test's expected result exactly.\n\n[Result]\nPASS\n```", "test_general_error": "```plaintext\n[Explanation]\nThe patched requests.get raises RuntimeError('Unexpected error'). The function's try/except catches this in the generic \"except Exception as e\" branch and returns \"Error: Unexpected error\", which satisfies the test's startswith assertion.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/273", "raw_responses": {"test_invalid_content_type": "```plaintext\n[Explanation]\nThe test replaces handler.headers with a plain dict using the lowercase key 'content-type'. The code looks up 'Content-Type' (case-sensitive) so it treats the header as missing/invalid and sends a 400 response via send_response/send_header/end_headers and writes JSON. The test expects send_error to be called with the specific message, but the code never calls send_error in this path. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```", "test_class_properties": "```plaintext\n[Explanation]\nThe function task_func defines and returns PostHandler which is declared as class PostHandler(http.server.BaseHTTPRequestHandler). This is a class object, so isinstance(handler_class, type) is True, and it is a direct subclass of http.server.BaseHTTPRequestHandler, so issubclass(...) is True. There are no runtime errors for this test.\n\n[Result]\n```plaintext\nPASS\n```", "test_valid_json_data": "```plaintext\n[Explanation]\nThe handler checks headers with capitalized names ('Content-Type' and 'Content-Length') using self.headers.get(...). The test sets headers as a dict with lowercase keys ('content-type', 'content-length'), so .get returns None. The handler treats the content type as not 'application/json' and sends a 400 response instead of the expected 200. Therefore the test's assertion send_response.assert_called_with(200) will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_json": "```plaintext\n[Explanation]\nThe test constructs request_handler.headers as a plain dict with lowercase keys ('content-type', 'content-length'), but the handler code looks up headers using 'Content-Type' and 'Content-Length' (case-sensitive). That causes content_type to be None, so the handler returns a 400 for wrong content type instead of attempting to parse JSON and calling send_error(400, 'Invalid JSON'). The test expects send_error to be called with (400, 'Invalid JSON'), so the behavior does not match and the assertion will fail.\n\n[Result]\nFAIL\n```", "test_missing_data_key": "```plaintext\n[Explanation]\nThe test sets headers as a plain dict with lowercase keys ('content-type', 'content-length'), but the handler code calls self.headers.get('Content-Type') / 'Content-Length' (different case), so content_type will be None and the handler will take the \"Content-Type header is not application/json\" branch. In any case, the handler uses self.send_response(...) and writes JSON to wfile for errors (it never calls self.send_error). The test expects request_handler.send_error to have been called with (400, 'No data key in request'), which will not happen. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/123", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function appends 12 to the input list, so for my_list = [1,2,3] the sum becomes 18. The test directory contains only 3 CSV files, so files_to_use has length 3 which is less than 18, causing a FileNotFoundError to be raised. The test expects a concatenated DataFrame, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_list_modification": "```plaintext\n[Explanation]\nThe function appends 12 to my_list (making it [1,2,3,12]) so sum_of_list = 18. The test directory only contains 3 CSV files, so after selecting files_to_use it finds len(files_to_use)=3 < 18 and raises FileNotFoundError. The test only asserts that 12 is in my_list and does not expect an exception, so the raised error causes the test to fail.\n\n[Result]\nFAIL\n```", "test_invalid_input": "```plaintext\n[Explanation]\nThe test calls task_func('not a list', file_dir=self.test_dir) and expects a TypeError. The function immediately checks isinstance(my_list, list) and raises TypeError(\"'my_list' must be a list\") before any file operations, so the expected exception is raised and the test will pass.\n\n[Result]\nPASS\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func([1,2,3], file_dir='./non_existent_dir/'). The function verifies the argument is a list (ok), appends 12, then uses glob to find CSV files in the given directory. For a non-existent directory glob returns an empty list, and the function raises FileNotFoundError when no files are found. This matches the test's assertRaises(FileNotFoundError), so the test will pass.\n\n[Result]\nPASS\n```", "test_correct_file_count": "```plaintext\n[Explanation]\nThe function appends 12 to the input list, so my_list [1] becomes [1, 12] and sum_of_list = 13. The test directory contains only 3 CSV files. The code slices files[:13] (yielding 3) but then checks if len(files_to_use) < sum_of_list (3 < 13) and raises FileNotFoundError. The test expects a DataFrame of length 9, but the function will raise an exception instead.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/401", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a Flask app named 'test_case', sets MAIL_SERVER to 'localhost', MAIL_PORT to int(25) -> 25, MAIL_USE_TLS to False (default 'False'.lower() != 'true'), and MAIL_USERNAME/PASSWORD to None. Initializing Mail(app) will succeed if flask_mail is available. These values match the assertions in the test.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function reads environment variables (with defaults), converts MAIL_PORT to int and MAIL_USE_TLS to a boolean, creates a Flask app with the given name, initializes Mail(app), and returns the Mail instance and app.config. The test patches os.environ with the expected values; the conversions produce 2525 and True, and mail.app.name will be 'test_case_2'. Therefore the assertions should all hold.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function reads environment variables with appropriate defaults and converts MAIL_PORT to int and MAIL_USE_TLS to a boolean. With os.environ patched to only include 'MAIL_SERVER': 'another_server', the code sets MAIL_SERVER to that string, MAIL_PORT to 25, MAIL_USE_TLS to False, and MAIL_USERNAME and MAIL_PASSWORD to None. Initializing Mail(app) requires flask_mail to be importable, but assuming the testing environment provides flask_mail (as flask is already imported in the test), there are no other errors. The test only checks config values, which match the function's behavior.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function reads environment variables with the specified defaults, casts MAIL_PORT to int and converts MAIL_USE_TLS to a boolean by comparing lowercased string to 'true'. With the test's patched os.environ containing MAIL_PORT='3030' and MAIL_USE_TLS='False', the resulting config values are MAIL_SERVER='localhost', MAIL_PORT=3030, MAIL_USE_TLS=False, and username/password None. Initializing Mail(app) requires flask_mail to be available, which is assumed in the test environment. These match the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function sets app.config values using environment variables with the specified defaults. With os.environ patched to {'MAIL_USERNAME': 'username'}:\n- MAIL_SERVER defaults to 'localhost'\n- MAIL_PORT defaults to 25 (int conversion handles the default int)\n- MAIL_USE_TLS defaults to False via the string comparison\n- MAIL_USERNAME is 'username'\n- MAIL_PASSWORD is None\n\nThese match the test assertions, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/509", "raw_responses": {"test_difference_report": "```plaintext\n[Explanation]\nThe test expects the Content column to contain Python tuple-style strings like \"('name', 'age')\", but the implementation compares raw text lines and leaves Content as plain CSV line strings like \"name,age\". The produced df_list will be [\"1, ,name,age\", \"2,-,Alice,30\", \"3,+,Alice,31\", \"4, ,Bob,25\"], which does not match the expected [\"1, ,('name', 'age')\", \"2,-,('Alice', '30')\", \"3,+,('Alice', '31')\", \"4, ,('Bob', '25')\"]. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func with two nonexistent file names and asserts FileNotFoundError. task_func first checks os.path.exists for each file and raises FileNotFoundError if missing, so it will raise the expected exception. The module imports pandas, but the test file also imports pandas, implying it's available in the test environment.\n\n[Result]\nPASS\n```", "test_custom_delimiter": "```plaintext\n[Explanation]\nThe test writes test3.csv and test4.csv with lines containing semicolon-separated values, then calls task_func on those files with delimiter=';'. task_func reads the files, strips line endings, runs difflib.ndiff, and builds a DataFrame of differences. For the given files it will produce a DataFrame (4 rows) without raising exceptions. The test only asserts the return is a pandas DataFrame, so the code will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_file_path": "```plaintext\n[Explanation]\nCalling task_func(123, 456) causes os.path.exists to be evaluated with integer arguments. os.path.exists will not find such paths and the function raises FileNotFoundError (a subclass of Exception). The test expects an Exception to be raised, so the behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_empty_files": "```plaintext\n[Explanation]\nThe test creates two empty files and calls task_func on them. The function checks for empty contents after reading and raises ValueError when both files are empty. The test expects a ValueError, so the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/592", "raw_responses": {"test_csv_file_creation": "```plaintext\n[Explanation]\nThe test expects a file named \"<OUTPUT_DIR>/sensor_data.csv\". The function creates a file with a timestamped name \"sensor_data_{timestamp}.csv\" (e.g. sensor_data_20251231_235959.csv), so the exact path FILE_PATH used in the test (\"./output/sensor_data.csv\") will not exist. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```", "test_csv_file_rows": "```plaintext\n[Explanation]\nThe test expects a file named './output/sensor_data.csv' with 25 lines. The function creates a file named 'sensor_data_{timestamp}.csv' (filename mismatch), so './output/sensor_data.csv' will not exist. Additionally, task_func uses start_time.replace(hour=start_time.hour + i), which can raise ValueError when hour >= 24 for many start times. Either issue causes the test to fail.\n\n[Result]\nFAIL\n```", "test_csv_file_header": "```plaintext\n[Explanation]\nThe test expects a file named output/sensor_data.csv, but task_func creates a timestamped filename like output/sensor_data_YYYYMMDD_HHMMSS.csv. Calling task_func(0) creates the file with the timestamped name and writes the header, but the test opens the fixed path output/sensor_data.csv which does not exist. Therefore the test will fail with FileNotFoundError.\n\n[Result]\nFAIL\n```", "test_file_path_return": "```plaintext\n[Explanation]\nThe function creates a timestamped filename \"sensor_data_{timestamp}.csv\" and returns that full path. The test expects the returned path to be \"./output/sensor_data.csv\" (no timestamp). Therefore the returned string will not match the expected FILE_PATH, causing the assertion to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_no_hours_data": "```plaintext\n[Explanation]\nThe test expects a file named \"./output/sensor_data.csv\". The function creates a file named \"sensor_data_{timestamp}.csv\" (with a timestamp) inside the output directory, so \"./output/sensor_data.csv\" is never created. Thus the test will raise FileNotFoundError when trying to open FILE_PATH.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/777", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists the mocked directory entries, matches each filename with the provided regex, creates extraction dirs using the filename prefix, opens each zip with the expected path and mode, and returns the list of extraction directories in the same order. All mocked calls and assertions in the test align with this behavior, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe code lists the mocked filename 'test_data-123.zip', the regex matches group 'test_data', so it calls os.makedirs once with os.path.join(test_dir, 'test_data') and zipfile.ZipFile once with os.path.join(test_dir, 'test_data-123.zip') and mode 'r'. The mocks in the test provide the needed context-manager extractall, so the assertions will be satisfied.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe filename 'data_test-321.zip' matches the regex, producing prefix 'data_test'. The code calls os.makedirs with os.path.join(test_dir, 'data_test') and exist_ok=True, and opens zipfile.ZipFile with os.path.join(test_dir, 'data_test-321.zip') and mode 'r'. These calls match the test's mocked assertions exactly, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nos.listdir is patched to return an empty list, so the for-loop in task_func does not iterate and none of the code paths that call os.makedirs or zipfile.ZipFile are executed. Therefore both mocks remain uncalled and the test assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lists the patched directory, matches 'test-456.zip' with the given regex (group 1 -> 'test'), creates the expected extraction path, calls os.makedirs with exist_ok=True, opens zipfile.ZipFile with the joined path and mode 'r', and calls extractall on the context manager. All calls match the test's expectations and argument checks, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/942", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns a DataFrame with the required columns and five categories, so those assertions pass. However, the plot title set in the code is 'Sales Report by Category', while the test expects 'Category-wise Sales Trends'; this mismatch causes the test to fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function generates 7 monthly dates using freq='WOM-2FRI' starting at or after 2021-01-01 (first is 2021-01-08), creates all combinations with 5 categories (7*5=35 rows), so df['Date'].min() >= 2021-01-01, df['Date'].nunique() == 7, and len(df) == 35. The plotting side-effects do not affect these assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function generates a DataFrame by making all combinations of the generated dates and the provided categories. Calling task_func(freq='W-TUE', categories=['Books', 'Games']) produces rows for both 'Books' and 'Games', so df['Category'].unique() will contain exactly those two categories and the assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function builds a DataFrame from the supplied categories list and generates one row per (date, category) pair. Calling task_func with categories=['Food','Clothing'] will produce both categories in df['Category'].unique() (length 2), and all unique category values will be in the provided list. The date_range freq 'W-WED' is valid, and no errors are raised, so the assertions in the test will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function generates 13 dates using the default parameters and creates one row per (date, category). With categories=['Electronics'] that yields 13 rows all having Category 'Electronics'. The random Sales column and plotting do not affect these assertions. Therefore both test assertions (all categories equal 'Electronics' and length 13) will be satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1008", "raw_responses": {"test_successful_scrape": "```plaintext\n[Explanation]\nThe test sets mock_response.text but the function reads response.content. Because mock_response.content is a MagicMock (not the HTML string), BeautifulSoup won't find the <table> and table will be None, causing a ValueError. The test expects a DataFrame, so the code will fail this test.\n\n[Result]\nFAIL\n```", "test_table_not_found": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with .text set but not .content. In task_func, response.raise_for_status() is called (exists as a MagicMock and does not raise), and BeautifulSoup is given response.content (another MagicMock). BeautifulSoup will parse that without finding any <table id=\"...\">, so table is None and the function raises ValueError as the test expects. No earlier exception is raised, so the test passes.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_network_error": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.exceptions.ConnectionError when called. task_func calls requests.get and does not catch that exception, so the ConnectionError propagates. The test asserts that calling task_func raises that exception, so it will pass.\n\n[Result]\nPASS\n```", "test_http_error": "```plaintext\n[Explanation]\nThe test patches requests.get so that the mock response's raise_for_status raises requests.exceptions.HTTPError. In task_func, response = requests.get(...) returns that mock and response.raise_for_status() is called, which will raise the HTTPError and propagate. The test expects this exception, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_table": "```plaintext\n[Explanation]\nThe test's mock response sets mock_response.text but not mock_response.content. The function uses response.content when creating the BeautifulSoup object, so BeautifulSoup receives a MagicMock (not the HTML string) and cannot find the <table id=\"table0\"> element. That causes table to be None and the function raises ValueError, while the test expects an empty DataFrame. Therefore the code will not pass the test.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/274", "raw_responses": {"test_invalid_content_type": "```plaintext\n[Explanation]\nThe handler's do_POST uses self.headers['Content-Length'] (capitalized). The test replaces headers with a plain dict containing lowercase keys ('content-length'), so attempting to access 'Content-Length' raises a KeyError. That exception is not caught by the handler, so do_POST will error instead of calling send_response(400) as the test expects.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_missing_key_in_json_data": "```plaintext\n[Explanation]\nThe handler code does int(self.headers['Content-Length']) but the test sets self.handler.headers to a plain dict with key 'content-length' (lowercase). Dictionary access is case-sensitive, so 'Content-Length' raises KeyError and the exception prevents the handler from sending the expected 400 response. Thus the test will fail.\n\n[Result]\nFAIL\n```", "test_valid_json_request": "```plaintext\n[Explanation]\nThe handler expects header 'Content-Length' (capitalized) but the test sets 'content-length' (lowercase) on a plain dict, causing a KeyError. Additionally, even if headers matched, the code calls server.send_message(...) while the test expects instance.sendmail(...) to have been called. These mismatches cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_json_format": "```plaintext\n[Explanation]\nThe handler code uses self.headers['Content-Length'] (capitalization-sensitive), but the test sets headers as a plain dict with lowercase 'content-length'. Accessing the missing 'Content-Length' key raises a KeyError before JSON is parsed, so the test's assertions are not reached and the test will fail.\n\n[Result]\nFAIL\n```", "test_empty_json_data": "```plaintext\n[Explanation]\nThe handler code uses self.headers['Content-Length'] (capitalized) but the test assigns headers as a plain dict with 'content-length' lowercase. This causes a KeyError before JSON parsing, which is not caught, so the handler never sends the expected 400 response and the test will fail.\n\n[Result]\nFAIL\n```", "test_email_sending_exception": "```plaintext\n[Explanation]\nThe handler will not produce the expected 535 response. Two problems:\n1) The test sets headers with lowercase 'content-length' but the code accesses 'Content-Length' -> KeyError before JSON parsing.\n2) Even if headers matched, the test configures a side effect on instance.sendmail, but the code calls server.send_message (different method), so the SMTPAuthenticationError would not be raised and the code would send a 200 response.\n\nTherefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/124", "raw_responses": {"test_output_types": "```plaintext\n[Explanation]\nThe function appends 12 so actual_size = 18 and generates 18 random integers with seed 100 and then plots a histogram using bins=range(1,102), which produces 100 bins (one per integer 1..100). The test expects 20 bar heights (length-20 list), but ax.containers[0] will contain 100 rectangles, so the bar_heights list length and contents will not match the expected list. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe function itself raises TypeError for non-list inputs as required, but the module imports matplotlib.pyplot at top-level. If matplotlib is not available in the test environment, importing the module will raise ImportError before the test can call task_func, causing the test to fail. Therefore the code will not reliably pass the provided test case.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_list_elements": "```plaintext\n[Explanation]\nThe function iterates over my_list and raises ValueError if any element is not int or float. Given input [1, 2, 'three'], the string 'three' triggers that check, so task_func raises ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_plot_title": "```plaintext\n[Explanation]\nThe function appends the title 'Histogram of Random Numbers' to the Axes via ax.set_title and returns the Axes. The test checks ax.get_title() against that exact string, so it will pass (no type or value errors occur for the given input).\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_time_measurement": "```plaintext\n[Explanation]\nThe function accepts a list, appends 12, sums to 18 for input [1,2,3], and generates 18 random integers. time_taken is computed as end_time - start_time, which is always >= 0. Type and value checks pass. The histogram creation and return of (time_taken, ax) do not affect the asserted condition. Therefore the test assertion that time_taken >= 0 will hold.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_size_limit": "```plaintext\n[Explanation]\nThe function appends 12 to [30,40,50] giving sum 132, so actual_size = min(132, 100) = 100 and it generates 100 random ints successfully. The test then computes np.histogram(..., bins=20) which returns a counts array of length 20, so the assertion len(n) == 20 holds. No other operations in the test cause errors.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/409", "raw_responses": {"test_normal_functionality": "```plaintext\n[Explanation]\nThe function constructs the correct path, reads the Excel file, selects the 'Sales' column, and computes mean/median/std using numpy. For the sample data the mean is 200, median 200, and numpy's default population std (np.std) yields 70.71067811865476, matching the test's expected values. Data types (float vs int) do not cause assertion failures because numeric equality holds. No other errors (file path, column name, keys) occur for the provided test.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe test calls task_func with a filename that does not exist in the prepared directory. task_func constructs the full path and explicitly checks os.path.exists(...) and raises FileNotFoundError when the file is missing. Therefore the expected exception is raised and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_column_name": "```plaintext\n[Explanation]\nThe test creates an Excel file 'test_file.xlsx' containing a 'Sales' column. The test calls task_func with a non-existent column name 'NonexistentColumn'. task_func reads the Excel file successfully and checks for the column; since the column is absent it raises ValueError as expected by the test. No other errors are triggered.\n\n[Result]\nPASS\n```", "test_empty_excel_file": "```plaintext\n[Explanation]\nThe test writes an empty Excel file (no columns). task_func reads the file, finds the specified column missing, and raises ValueError as expected by the test. File existence and Excel I/O succeed under normal pandas/openpyxl usage, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_file_with_different_data_types": "```plaintext\n[Explanation]\nThe test creates an Excel file where the 'Sales' column contains mixed types (ints and strings). task_func reads the column and calls numpy.mean on it; numpy.mean on an object array with ints and strings raises a TypeError (unsupported operand for addition). The test expects a TypeError, so the code will produce that exception and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/511", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function computes the Salary column statistics correctly from the DataFrame and returns the expected dictionary values: sum 500000, mean 100000.0, min 50000, max 150000. The pie chart creation does not affect the returned stats. The types (numpy scalars) compare equal to the expected Python numeric values in the unittest assertion. No exceptions are raised for the given valid column and non-empty data.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function computes statistics using a pandas Series for the 'Experience' column: sum = 41, mean = 8.2, min = 3, max = 13. The returned dict values may be numpy numeric types but compare equal to the expected Python numbers, so the unittest assertEqual will succeed. No other errors are present for this input.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function computes statistics from the 'Age' column correctly (sum 185, mean 37.0, min 27, max 47). pandas/numpy scalar types compare equal to Python numeric literals in dict equality, and plotting code does not affect the asserted result. Therefore the test's assertEqual will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns the expected dict structure for empty data, but the numeric NaN values (np.nan) are not equal to each other (np.nan != np.nan). unittest's assertEqual uses equality to compare dict values, so the dicts will not compare equal and the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function correctly handles a non-empty single-row input. It builds a DataFrame, computes sum/mean/min/max (numeric results may be numpy scalars but compare equal to Python ints/floats), and returns a matplotlib Axes from plt.subplots after drawing a pie. The test's assertions (dictionary equality and isinstance check) will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function immediately checks if the provided column is in ['Age', 'Salary', 'Experience'] and raises KeyError if not. The test calls task_func('InvalidColumn', data) and expects a KeyError, so the behavior matches and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function creates a pie chart with one wedge per data point by calling ax.pie(column_data, labels=df['Age'].tolist(), ...). That produces len(data) wedge patches and adds the age labels as text objects. The test asserts the number of patches equals the number of data points and that each age string appears among ax.texts; both conditions are satisfied by the implementation.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/593", "raw_responses": {"test_dataframe_content": "```plaintext\n[Explanation]\nThe test expects the function to:\n- Write a CSV named exactly OUTPUT_DIR/traffic_data.csv, but the code writes a timestamped filename (traffic_data_YYYYMMDD_HHMMSS.csv). file_path comparison will fail.\n- Call pandas.read_csv(FILE_PATH) and csv.writer, but the code uses df.to_csv and never calls read_csv or csv.writer, so those patched functions will not be called as asserted.\n- Call matplotlib.pyplot.show(), but the code only creates the plot and does not call plt.show(), so that assertion will fail.\nThe only assertion likely to pass is that randint was called, but multiple other assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_empty_dataframe_on_zero_hours": "```plaintext\n[Explanation]\nThe test calls task_func(0) and asserts that the returned ax is None. In the provided code, even when hours=0 the function still creates a matplotlib figure and axes (fig, ax = plt.subplots(...)) and returns that axes object. Therefore ax will not be None and the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_directory_creation": "```plaintext\n[Explanation]\nThe test patches os.makedirs and expects it to be called with os.path.dirname(FILE_PATH). The task_func calls os.makedirs(output_dir, exist_ok=True), so the mock is invoked with the keyword argument exist_ok=True. The test's mock_makedirs.assert_called_with(os.path.dirname(FILE_PATH)) does not account for this keyword and will fail due to the mismatched call signature.\n\n[Result]\n```plaintext\nFAIL\n```", "test_plot_generation": "```plaintext\n[Explanation]\nThe test patches plt.show and asserts that it is called when task_func(1) is executed. The provided task_func creates and configures a plot but never calls plt.show(), so the mocked plt.show will not be called and the assertion fails.\n\n[Result]\nFAIL\n```", "test_task_func_runs_without_error": "```plaintext\n[Explanation]\nThe test only checks that calling task_func(1) runs without raising an exception. The function creates the output directory, builds a small DataFrame, writes a CSV file, and creates a matplotlib plot (without calling plt.show). The test patches plt.show and does not assert file names. Required modules (os, pandas, matplotlib) are imported in the same module, so the patch target resolves. There is no obvious runtime error for hours=1. Therefore the function should execute successfully and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/785", "raw_responses": {"test_archive_txt_files": "```plaintext\n[Explanation]\nThe function will create /tmp/archive, glob the provided pattern './test_files/*.txt' to get the two txt files, run the system tar command to create the archive, and then remove the original files. The test then checks that the archive file exists and that no txt files remain. In a typical Linux environment with 'tar' available and normal permissions for /tmp, the code performs these steps correctly, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_archive_image_files": "```plaintext\n[Explanation]\nThe function will find the two .jpg files in ./test_files, create /tmp/archive if needed, run tar to produce the archive and then remove the original files. The test checks the archive exists and that no .jpg files remain. Assuming a typical Unix-like environment with tar available (so subprocess.run succeeds), the code satisfies the test assertions and tearDown can remove the created archive directory.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_no_matching_files": "```plaintext\n[Explanation]\nThe test calls task_func with a pattern that matches no files. The function returns None when no files are found, but the test expects the string 'No files found matching the pattern.' Therefore the assertion will fail. (Additionally, tearDown removes ARCHIVE_DIR which won't exist unless created, but the primary failure is the return value mismatch.)\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_multiple_archiving_unique_naming": "```plaintext\n[Explanation]\nThe first call to task_func finds the two .txt files, creates the archive and deletes those originals. Subsequent calls with the same pattern find no matching files and return None. The test compares the two returned values (both None) so the assertion passes. The code uses the system 'tar' command; assuming a standard environment with tar available, no exceptions occur.\n\n[Result]\nPASS\n```", "test_archiving_in_nonexistent_directory": "```plaintext\n[Explanation]\nThe test calls task_func('/tmp/archive/*.txt') after ensuring ARCHIVE_DIR does not exist. task_func creates ARCHIVE_DIR, then glob.glob finds no matching files and the function returns None. The test then does os.path.isfile(archive_path) where archive_path is None; os.path.isfile(None) raises a TypeError (it expects a path-like object), causing the test to error/fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/1012", "raw_responses": {"test_successful_download_and_extraction": "```plaintext\n[Explanation]\nThe test calls task_func with a real Google Drive URL but does not mock requests, so it requires network access and a direct downloadable zip response. In typical test environments network access is unavailable or the Drive link returns an HTML confirmation (not a zip), causing requests or zipfile to raise and the function to return an Error tuple. Therefore the assertion for a successful download/extraction will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to return a mock with status_code 404, but the mock's raise_for_status() does not raise, so the code proceeds and attempts to write response.content (a MagicMock) to a binary file, causing an exception. The function returns an error message starting with \"Error:\" rather than containing \"Download failed\" as the test expects. Although the returned list is [], the first assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_non_200_http_response": "```plaintext\n[Explanation]\nThe test patches requests.get and sets only return_value.status_code = 404. The code calls response.raise_for_status(), but the MagicMock's raise_for_status() does not raise an exception, so execution continues. Writing response.content (a MagicMock) in binary mode raises a TypeError which is caught; the function returns a message starting with \"Error:\" and an empty list. The test expects the message to contain \"Download failed\" and [] as the second element. The returned message does not match the expected substring, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_network_error": "```plaintext\n[Explanation]\nThe test patches requests.get to raise a requests.exceptions.ConnectionError. task_func calls requests.get inside a try/except that catches Exception and returns ('Error: ...', []). The test checks that the first element contains 'Error' and the second is an empty list \u2014 both conditions are satisfied. No filesystem side-effects interfere with the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_corrupted_zip_file": "```plaintext\n[Explanation]\nThe test patches requests.get, builtins.open, and zipfile.ZipFile. requests.get returns a MagicMock response whose raise_for_status() won't raise; builtins.open is mocked so writing the (mock) content succeeds. zipfile.ZipFile is configured to raise zipfile.BadZipFile when called, so the with-statement triggers an exception which is caught by the code, returning a message containing \"Error\" and an empty list. This matches the test assertions.\n\n[Result]\n```plaintext\nPASS\n```", "test_request_exception": "```plaintext\n[Explanation]\nThe test method references requests.exceptions.RequestException but the test file never imports the requests name into its namespace. The decorator @patch('requests.get') patches the requests.get attribute but does not create a requests symbol for the test body, so mock_get.side_effect = requests.exceptions.RequestException will raise a NameError before exercising task_func. Therefore the test will error/fail regardless of the implementation.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/19", "raw_responses": {"test_single_file_zip": "```plaintext\n[Explanation]\nThe function creates 'files.zip' in the given directory containing all files (non-recursive) and returns its path. The test creates one file in a temporary directory, calls task_func on that directory, and asserts the returned zip path exists. The code checks directory existence, finds the file, writes the zip, and returns the zip path \u2014 so the zip will be created and the existence check will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_multiple_files_zip": "```plaintext\n[Explanation]\nThe function checks the directory exists, collects files (non-recursively), returns None only if no files. The test creates 5 files in a temp directory, so files is non-empty. The function creates \"files.zip\" in that directory and returns its path. The zip file will exist, so os.path.exists(zip_path) will be True.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_directory": "```plaintext\n[Explanation]\nThe temporary directory is empty. glob.glob will find no files, so the function returns None as coded. The test asserts None, so it will pass.\n\n[Result]\nPASS\n```", "test_non_existent_directory": "```plaintext\n[Explanation]\nThe function checks os.path.exists(directory) and raises FileNotFoundError if it does not exist. The test calls task_func('/non/existent/directory') and expects a FileNotFoundError, so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```", "test_exclusion_of_subdirectories": "```plaintext\n[Explanation]\nThe function lists entries with glob(... '*') then keeps only those where os.path.isfile is True, so the subdirectory is excluded. The zip file is created after collecting the file list, so files.zip itself won't be included. With one top-level file present, the zip will contain exactly one entry, satisfying the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_file_integrity_in_zip": "```plaintext\n[Explanation]\nThe function collects files in the given directory, creates files.zip and writes each file into the archive using its basename. The test creates one text file, calls task_func, then opens the zip and reads the file contents \u2014 which will match. No errors or edge cases affect this test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/129", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe test patches requests.get but only sets mock_get.return_value.text and .ok. The function calls response.raise_for_status() (a Mock, so no exception) and then uses BeautifulSoup(response.content, ...). The mock has no .content set, so response.content is a Mock whose string does not contain the HTML table; soup.find('table') returns None and the function raises ValueError(\"No table found on the page\"). The test expects a DataFrame, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.ConnectionError. task_func directly calls requests.get and does not convert exceptions, so requests.ConnectionError will propagate. The test uses self.assertRaises(ConnectionError) (the built-in ConnectionError), but requests.ConnectionError is a different exception class (not a subclass of the built-in), so the raised exception will not match the expected type. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_empty_page": "```plaintext\n[Explanation]\nThe test patches requests.get to return an object with empty content. task_func calls response.raise_for_status() (a Mock, so no exception) and parses response.content with BeautifulSoup. No <table> will be found, so the function raises ValueError(\"No table found on the page\") as the test expects.\n\n[Result]\nPASS\n```", "test_non_html_content": "```plaintext\n[Explanation]\nThe test patches requests.get to return a Mock with .ok and .text set. task_func calls response.raise_for_status() (which is a no-op on the Mock) and then parses response.content with BeautifulSoup. The Mock's .content is another Mock (or becomes a string via str), which will not contain a <table>, so soup.find('table') is None and the function raises ValueError as the test expects.\n\n[Result]\nPASS\n```", "test_http_error": "```plaintext\n[Explanation]\nThe test patches requests.get to return a mock response whose raise_for_status has side_effect = requests.HTTPError. task_func immediately calls response.raise_for_status() after requests.get, so this call will raise requests.HTTPError as expected. The test asserts that requests.HTTPError is raised, so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```", "test_return_type_with_complex_data": "```plaintext\n[Explanation]\nThe test's mocked response sets .text but not .content. The function calls response.content when creating the BeautifulSoup object, so BeautifulSoup will not receive the provided HTML and will not find a <table>. That causes the function to raise ValueError(\"No table found on the page\") instead of returning a DataFrame. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/287", "raw_responses": {"test_single_file_few_words": "```plaintext\n[Explanation]\nThe function writes the JSON to os.path.join(directory, filename) (./testdir/test_output.json) but the test opens 'test_output.json' in the current directory \u2014 path mismatch causes FileNotFoundError. Even if the path matched, the JSON contains a top-level filename key {'single_file.txt': {...}} while the test expects just the word-count dict {'hello':2,'world':1}. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_multiple_files": "```plaintext\n[Explanation]\nThe test will fail. task_func writes the JSON to os.path.join(directory, filename) (i.e., ./testdir/test_output.json), but the test attempts to open 'test_output.json' in the current working directory, causing FileNotFoundError. Even if the path were correct, the code writes per-file word-count dictionaries (keys are filenames), whereas the test expects a single aggregated word-count dict across all files. Although the returned total_words (7) matches the test's second assertion, the JSON file check will fail.\n\n[Result]\nFAIL\n```", "test_empty_files": "```plaintext\n[Explanation]\nThe test expects a JSON file named 'test_output.json' in the current working directory containing an empty dict {}. The implementation writes the JSON to os.path.join(directory, filename) (i.e., './testdir/test_output.json'), so the test's open('test_output.json', ...) will raise FileNotFoundError. Even if the test opened the file in the directory, the code would write {\"empty_file.txt\": {}} (it includes the empty file as a key) which does not match the expected {}.\n\n[Result]\nFAIL\n```", "test_files_with_special_characters": "```plaintext\n[Explanation]\nThe test writes a file in './testdir/' and then calls task_func('test_output.json', self.test_dir). The function writes the JSON to os.path.join(directory, filename) => './testdir/test_output.json', but the test later opens 'test_output.json' in the current working directory, so the file it tries to read does not exist (FileNotFoundError). Even if the test opened the file written by the function, the function writes a mapping keyed by the filename ('special_chars.txt': { ... }), whereas the test expects a flat dict of word counts for the file. Therefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```", "test_nested_directories": "```plaintext\n[Explanation]\nThe test writes a .txt file into a nested directory and then calls task_func('test_output.json', nested_dir). The test then opens 'test_output.json' from the current working directory and expects a flat JSON mapping words to counts. The implementation writes the JSON into the provided directory (nested_dir/test_output.json) and writes a mapping from filename to its word-count dict ({\"nested_file.txt\": {\"hello\":2,\"world\":1}}). Therefore the file path and the JSON structure do not match the test's expectations, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/417", "raw_responses": {"test_model_type": "```plaintext\n[Explanation]\nThe function creates and returns a keras.models.Sequential instance (model) and an Axes object. The test only checks that the returned model is an instance of Sequential. The model is constructed with Sequential() in the function, so the assertion will succeed. No other test constraints are present that would cause an error here.\n\n[Result]\nPASS\n```", "test_axes_type": "```plaintext\n[Explanation]\nThe function returns the Axes object created by plt.subplots() and the test only checks that ax is an instance of plt.Axes. The code constructs and returns (model, ax) correctly; shapes and training call are valid for the provided 4-sample input. Therefore the assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_axes_title": "```plaintext\n[Explanation]\nThe function explicitly calls ax.set_title('Model loss') before returning the Axes object. Given the provided inputs, model training and plotting should run without affecting the Axes title, so the test asserting ax.get_title() == 'Model loss' will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_axes_xlabel": "```plaintext\n[Explanation]\nThe function sets the x-axis label with ax.set_xlabel('Epoch') and returns the Axes object. The test only checks ax.get_xlabel() == 'Epoch', which will be true. Training and plotting complete without affecting the label.\n\n[Result]\nPASS\n```", "test_axes_ylabel": "```plaintext\n[Explanation]\nThe function creates a matplotlib Axes and explicitly sets the y-axis label with ax.set_ylabel('Loss'). The test only checks ax.get_ylabel() == 'Loss', so this will pass. The model training and plotting should run without affecting the label retrieval.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_model_output_shape": "```plaintext\n[Explanation]\nThe model is constructed with Dense(1, input_dim=2), compiled and trained, so its output shape is (None, 1). Nothing in the code modifies this, and the fit call ensures the model is built. The test checks model.output_shape == (None, 1), which will be True.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_model_weights": "```plaintext\n[Explanation]\nThe function builds a Sequential model with a single Dense layer Dense(1, input_dim=2), so model.get_weights() returns two arrays (kernel and bias). The provided data shapes are compatible, training runs without needing to display the plot, and the test only checks len(weights) == 2. Therefore the code will satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_model_loss": "```plaintext\n[Explanation]\nThe function compiles the model with loss='binary_crossentropy', so model.loss will contain that string. The test checks that 'binary_crossentropy' is in model.loss; given typical Keras behavior (model.loss storing the passed loss name), this assertion will succeed. Other parts (data split, training, plotting) do not interfere with the check.\n\n[Result]\n```plaintext\nPASS\n```", "test_model_optimizer": "```plaintext\n[Explanation]\nThe function compiles the model with an SGD instance (sgd = SGD(...)) and assigns it to the model via model.compile(..., optimizer=sgd). Therefore model.optimizer will be an instance of SGD, satisfying the test's isinstance check. No other errors affect this test.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/800", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function checks os.path.exists for the default 'match_data.csv'; if it doesn't exist (as in this test), it skips reading and only sums the provided empty dictionaries, producing zeros. It returns a Counter({'goals': 0, 'penalties': 0}) which matches the expected result.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function defaults to reading 'match_data.csv' but the test does not create or require that file. If the CSV does not exist, the code simply sums the provided dictionaries: goals 3+2=5 and penalties 1+2=3, producing Counter({'goals': 5, 'penalties': 3}) which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function checks for the default CSV file 'match_data.csv'. In the test no file is created and both input dicts are empty, so os.path.exists returns False, no file is read, totals remain 0, and the function returns Counter({'goals': 0, 'penalties': 0}) which matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function defaults to reading 'match_data.csv'; if the file does not exist it skips reading and sums only the provided dictionaries. For the test inputs goals sum to 5 and penalties sum to 3, and the function returns Counter({'goals': 5, 'penalties': 3}). Counter compares equal to an equivalent dict, so the assertion comparing to {'goals': 5, 'penalties': 3} will succeed.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function reads 'match_data.csv' only if it exists; the test does not create that file, so the CSV read is skipped. It then sums the provided dict values: goals = -2 + 3 = 1, penalties = 1 + (-2) = -1. The returned Counter matches the expected Counter({'goals': 1, 'penalties': -1}).\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/594", "raw_responses": {"test_task_func_checks_backup_directory_exists": "```plaintext\n[Explanation]\nThe test patches os.path.exists and then calls task_func(1), expecting os.path.exists to have been called with the backup directory path. The implementation uses os.makedirs(..., exist_ok=True) to create directories and never explicitly calls os.path.exists to check the backup directory. Because os.path.exists is not invoked inside task_func, the mock has no calls (mock_exists.call_args is None), so the test's attempt to read mock_exists.call_args[0][0] will fail. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_task_func_copies_to_backup_directory": "```plaintext\n[Explanation]\nThe test patches shutil.copy but the function uses shutil.copy2, so the patched mock_copy is never called. The test then tries to index mock_copy.call_args, which will be None, causing the test to fail (error/Assertion not reached). Therefore the code does not pass this test.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_task_func_writes_correct_header": "```plaintext\n[Explanation]\nThe test patches builtins.open with mock_open so file writes are captured. The code uses csv.DictWriter.writeheader(), which invokes the file's write method; mock_file_open().write will therefore contain a call that includes both 'Time' and 'Condition'. Other patches (os.makedirs, os.path.exists) do not prevent the function from executing the write path for hours=1. The use of shutil.copy2 (not patched) does not affect the captured write calls. Thus the test's assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_backup_file_creation": "```plaintext\n[Explanation]\nThe test patches shutil.copy and asserts it was called with (FILE_PATH, BACKUP_PATH). The code instead calls shutil.copy2 and copies to a backup file path that includes a timestamped filename (not the backup directory). Therefore the patched mock is never called and the expected call arguments don't match. The test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_csv_writing": "```plaintext\n[Explanation]\nThe test patches csv.writer and calls task_func(1). csv.DictWriter internally calls csv.writer when initialized, so the patched csv.writer will be invoked exactly once. The function writes one row (hours=1) and creates a backup; no errors arise for this test. The patch target is reachable because the test imports symbols from the module, so csv is available to patch. Therefore the assertion mock_csv_writer.assert_called_once() will succeed.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1013", "raw_responses": {"test_empty_page": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with text set to '<html></html>'. The function uses response.content (a MagicMock) which BeautifulSoup will handle without raising\u2014ultimately producing no <a> tags\u2014so absolute_links is empty and the function returns 0. The CSV file is written (header only) and removed in tearDown. The returned value matches the expected 0.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_link": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with the HTML assigned to its text attribute, but the function uses response.content. Since .content was not set, BeautifulSoup receives a MagicMock (not the HTML string) and finds no <a> tags. The function returns 0 instead of the expected 1, so the test will fail.\n\n[Result]\nFAIL\n```", "test_multiple_links": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with a .text attribute containing the HTML, but the code uses response.content. Since .content is not set, BeautifulSoup receives a MagicMock object (not the HTML string) and will not parse the expected <a> tags, so the function returns 0 instead of 2. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_duplicate_links": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with the HTML set on the .text attribute, but the code reads response.content. The MagicMock has no real .content bytes containing the HTML (it will be another MagicMock), so BeautifulSoup won't parse the intended HTML and no <a> tags will be found. The function will return 0 instead of the expected 1, so the test will fail.\n\n[Result]\nFAIL\n```", "test_external_links": "```plaintext\n[Explanation]\nThe test patches requests.get to return a MagicMock with only the text attribute set. The function uses response.content (not response.text), so BeautifulSoup receives a MagicMock object (its string representation does not contain the HTML anchor). As a result no <a> tags are found and the function returns 0, while the test expects 1. Therefore the test will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/513", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly validates input, builds a DataFrame, checks for negative values, computes the statistics for 'Steps' (sum 16500, mean 5500.0, min 5000, max 6000), and creates an Axes with title 'Line Chart of Steps'. The numeric results (numpy scalars) compare equal to the expected Python numbers, and the test only checks equality of the dict and the plot title. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function validates input, constructs a DataFrame, checks for negative values, computes statistics for 'Calories Burned', and plots the chart with the expected title. For the provided data (250, 275, 300) the sum, mean, min, and max match the expected values (825, 275.0, 250, 300). The matplotlib Axes title is set correctly. pandas/numpy return types compare equal to Python numeric literals, so the dict equality assertion will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function computes statistics using pandas on the provided data. For the test, 'Distance Walked' values are 3.6..4.5 (10 values), whose sum is 40.5, mean 4.05, min 3.6, max 4.5. The code validates inputs, computes these stats with pandas (matching the expected numeric values), and sets the plot title to 'Line Chart of Distance Walked'. All assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe input data is non-empty and the column 'Steps' is valid. The DataFrame is created correctly from datetime and zero values. The non-negative check passes (zeros are not negative). Calculated stats produce sum=0, mean=0.0, min=0, max=0 (numpy scalar comparisons equal the expected Python ints/floats). The function returns an Axes with title 'Line Chart of Steps'. All assertions in the test match the actual outputs.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe input data is non-empty and the requested column 'Calories Burned' is in the valid_columns list. The DataFrame creation and non-negative checks pass (all values >= 0). The statistics computed (sum 30000, mean 10000.0, min 10000, max 10000) match the expected dict. The code sets the plot title to 'Line Chart of Calories Burned', matching the test assertion. No other errors are triggered for this test case.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function checks if the provided column is in valid_columns and raises a KeyError if not. For 'Invalid Column' this happens immediately (before any DataFrame or plotting code), and KeyError is an Exception, so the test's assertRaises(Exception) will succeed.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe input data contains a negative value for 'Steps'. The function validates numeric columns and will raise ValueError when any numeric column has a negative entry (it checks df['Steps'] < 0 and .any()). The test expects a ValueError, so the code will raise it and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function correctly handles a single-row input: it creates a DataFrame, validates non-negative numeric values, computes sum, mean, min, and max for 'Steps' (sum and min/max may be numpy integer types but compare equal to Python ints), and returns the expected dictionary. The plotting does not affect the returned stats. No errors are raised for the provided test input.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_9": "```plaintext\n[Explanation]\nThe function computes statistics using pandas on the provided data. For the 'Steps' column it returns sum, mean, min, max which match the expected values (16500, 5500.0, 5000, 6000). Although pandas returns numpy scalar types, they compare equal to Python numeric literals used in the test. The data order does not affect these aggregates. Plot creation does not interfere with the test. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_10": "```plaintext\n[Explanation]\nThe test provides an empty data list and expects an exception. The function begins with \"if not data: raise ValueError('Data list cannot be empty')\", so it will raise a ValueError (a subclass of Exception). Therefore the with self.assertRaises(Exception) will catch it and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_11": "```plaintext\n[Explanation]\nThe function creates a DataFrame from the provided data, computes stats, and plots Date vs Steps. It sets the title to \"Line Chart of Steps\", x-label to \"Date\", and y-label to \"Steps\". The provided data are valid (non-empty, non-negative numeric values), so no exceptions are raised. The matplotlib Axes returned will have the expected title and labels, so the assertions in the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_12": "```plaintext\n[Explanation]\nThe function constructs a DataFrame from the provided data preserving order, validates non-negative numeric values, and plots df['Distance Walked'] against the Date. The plotted Line2D's y-data will be the array [1.0, 2.0], matching the test's expected_y. No errors (column validity, empty data, or negatives) occur for the given input, so the assertion should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/302", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nThe function computes the Pearson correlation of the Value lists, which yields a 3x3 matrix of 1.0s. The expected_result in the test is a 3x3 matrix of 1s; DataFrame.equals treats these as equal (values match), so result.equals(expected_result) is True. The test asserts False(result.equals(...)), causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test provides an empty DataFrame. The function begins by checking `if df.empty:` and raises `ValueError(\"DataFrame is empty\")`. The test expects a ValueError via `assertRaises(ValueError)`, so the function behavior matches the test's expectation. No other code paths are executed for an empty DataFrame.\n\n[Result]\nPASS\n```", "test_plot_generation": "```plaintext\n[Explanation]\nThe function creates a correlation heatmap and returns (corr_matrix, ax) when plot=True. For the given DataFrame the 'Value' entries are lists, so validation passes. seaborn.heatmap returns a matplotlib Axes and the code sets its title to 'Correlation Heatmap', matching the test assertions. The test unpacks the return and checks the Axes type and title, so the code will pass this test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_data": "```plaintext\n[Explanation]\nThe test provides a DataFrame whose 'Value' entry is a list of strings ['a','b','c']. The function only checks that df is non-empty and that df['Value'].iloc[0] is a list \u2014 it does not validate element types. It then builds a DataFrame from the list and calls value_df.corr(), which with non-numeric (object) data yields an empty correlation DataFrame rather than raising ValueError. Therefore the function returns a DataFrame instead of raising the expected ValueError, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_plot_data_correlation": "```plaintext\n[Explanation]\nThe function correctly converts the 'Value' lists into columns, computes the Pearson correlation (all 1.0 for the provided data), and when plot=True creates a seaborn heatmap with annotations. The test reads the annotation texts from the axes, converts them to floats, and compares them to the correlation matrix; the annotations and matrix values match (and formatting/parsing to float succeeds). No input validation or plotting behaviors block this test.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/17", "raw_responses": {"test_process_not_found_starts_process": "```plaintext\n[Explanation]\nThe test patches psutil.process_iter to return an empty list, so the loop finds no process and process_found remains False. The code then calls subprocess.Popen(process_name) (which is patched) and returns the exact expected message. The mock assertion matches the single call with the given process name. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_process_found_restarts_process": "```plaintext\n[Explanation]\nThe test patches psutil.process_iter to return a MagicMock process whose name() is set, but the code checks proc.info['name']. The MagicMock has no .info['name'] equal to 'notepad', so the code treats the process as not found, starts it and returns \"Process not found. Starting notepad.\" The test expects \"Process found. Restarting notepad.\" and that terminate() was called, so the outputs and calls won't match.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_process_terminates_and_restarts_multiple_instances": "```plaintext\n[Explanation]\nThe code checks proc.info['name'] but the test's MagicMocks set proc.name.return_value; proc.info['name'] will be a MagicMock (not equal to the string), so process_found remains False. The function will take the \"not found\" branch, start the process, return \"Process not found. Starting multi_instance.\", and will not call terminate on the mock processes. This does not match the test's expected \"Process found. Restarting multi_instance.\" and the terminate call assertions.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/418", "raw_responses": {"test_return_types": "```plaintext\n[Explanation]\nThe function splits 4 samples with test_size=0.3, producing a test set of 1 sample. sklearn.metrics.roc_curve requires both positive and negative samples in y_true; with a single test sample only one class is present, causing roc_curve to raise a ValueError. Therefore the test will not complete successfully.\n\n[Result]\nFAIL\n```", "test_model_type": "```plaintext\n[Explanation]\nThe function builds and trains a Keras Sequential model and returns it. For the provided small dataset (X shape (4,2), Y shape (4,)), Keras accepts a 1D label array and model.predict outputs probabilities which sklearn.metrics.roc_curve accepts. The function returns the trained Sequential model (which has a 'fit' attribute) and an Axes object; thus the test that only checks for the presence of 'fit' will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_model_output_shape": "```plaintext\n[Explanation]\nThe model definition does produce output_shape (None, 1). However, with 4 samples and test_size=0.3, train_test_split yields a very small test set (1 sample). roc_curve requires both positive and negative labels in y_true; with a single test sample it will raise a ValueError (\"Only one class present in y_true. ROC is not defined in that case.\"), so the function will error before returning the model. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_model_loss": "```plaintext\n[Explanation]\nThe function compiles the Keras model with loss='binary_crossentropy', which matches the unit test's expectation. The input shapes and subsequent operations do not affect this assertion. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_model_optimizer": "```plaintext\n[Explanation]\nThe function compiles the Keras model with sgd_optimizer = keras.optimizers.SGD(learning_rate=0.1) and passes that instance to model.compile, so model.optimizer will be an instance of tensorflow.keras.optimizers.SGD. The rest of the code (train_test_split, predict, roc_curve, plotting) should run without raising an exception for the provided 4-sample input, so the unittest assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_plot_axes": "```plaintext\n[Explanation]\ntrain_test_split with test_size=0.3 on 4 samples produces a very small test set (1 sample). With random_state=42 the single test sample has label 1, so Y_test contains only one class. sklearn.metrics.roc_curve raises an error when y_true has only one class (\"Only one class present in y_true. ROC is not defined in that case.\"), so the function will raise an exception and the unittest will not pass its assertions.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/139", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe function selects numeric columns (A and B), creates a matplotlib Axes for each via plt.subplots, appends and returns them. The test checks that each returned item is an instance of plt.Axes \u2014 which will be true. No errors (DataFrame non-empty, numeric columns present), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_input_empty_dataframe": "```plaintext\n[Explanation]\nThe test calls task_func(pd.DataFrame()), i.e. an empty DataFrame. The function checks df.empty and raises ValueError(\"DataFrame cannot be empty\") for empty DataFrames. This matches the test's expected exception, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_input_type": "```plaintext\n[Explanation]\nThe test calls task_func('not a dataframe'). The function's first check is \"if not isinstance(df, pd.DataFrame): raise ValueError(...)\", so passing a string raises ValueError as expected. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_no_numeric_columns": "```plaintext\n[Explanation]\nThe test passes a DataFrame with only a text column. task_func checks for numeric columns using df.select_dtypes(include=[np.number]) and raises ValueError when none are found. That matches the test's assertRaises expectation, so the test will pass.\n\n[Result]\nPASS\n```", "test_histograms_count": "```plaintext\n[Explanation]\nThe DataFrame in the test has two numeric columns ('A' and 'B') and one non-numeric column. task_func selects numeric columns via select_dtypes(include=[np.number]), creates one Axes per numeric column, and returns a list of those Axes. No errors are raised, so len(axes) will be 2, matching the test expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_plot_labels": "```plaintext\n[Explanation]\nThe function selects numeric columns (A and B), creates a Matplotlib Axes for each, sets xlabel to 'Value' and ylabel to 'Frequency', and returns the axes. The test checks that those exact substrings are present in the axis labels \u2014 which they are. No other errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_correctness_of_histogram_lines": "```plaintext\n[Explanation]\nThe function creates histograms for numeric columns 'A' and 'B' and sets the plot titles to the column names. Matplotlib's hist (via numpy.histogram) uses the data min and max as the outer bin edges, so the leftmost patch x and the rightmost patch x+width should equal the column data min and max within floating precision. The test compares those edges to the data min/max with a tolerance of 0.01. There are no errors in selecting numeric columns or in axis/title assignment, so the checks should pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/618", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function generates 5 rows (TEAMS length) with keys in the order 'Team', 'Goals', 'Penalty Cost', so df.shape == (5,3) and columns match. Goals are randint(0,5) so <=5, penalty cost is randint(0,3)*1000 so <=3000. The function returns two Matplotlib axes objects from plt.subplots, which satisfy isinstance(..., matplotlib.axes.Axes). No other failing issues in this test context.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nFor task_func(0, 5): randint(0, 0) yields 0 for all teams so df['Goals'] are all 0, satisfying the first assertion. Penalties are randint(0,5) so penalty costs are multiples of 1000 between 0 and 5000, satisfying the second assertion. No other exceptions are raised by the code in this test.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nFor penalties=0, randint(0, 0) yields 0 for every team so 'Penalty Cost' is 0 for all rows (0 * PENALTY_COST). Goals are generated with randint(0, 10) so every 'Goals' value is <= 10. Both assertions in the test will be satisfied.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nCalling task_func(0, 0) makes randint(0, 0) return 0 for every team, so 'Goals' are all 0 and 'Penalty Cost' is team_penalties * 1000 = 0. The DataFrame will therefore satisfy both assertions. The plotting calls are unlikely to raise errors given the test already imports matplotlib (and typical test environments use a non-interactive backend), so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function uses randint(0, goals) and randint(0, penalties). For the test inputs (goals=2, penalties=1) Goals will be in {0,1,2} and Penalty Cost will be team_penalties * 1000, i.e. either 0 or 1000. Both assertions (Goals <= 2 and Penalty Cost <= 1000) will always hold. Creating plots does not affect these values or raise errors in the given test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/826", "raw_responses": {"test_files_moved": "```plaintext\n[Explanation]\nThe regex pattern r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b' matches the provided matching filenames (file1.txt, document1.doc, notes.docx). The function iterates files, checks isfile, uses pattern.match (which succeeds for these names), moves them to the target directory, and increments the counter. Non-matching files keep their extensions and remain in the source. Therefore the returned count and file locations will meet the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_no_files_moved": "```plaintext\n[Explanation]\nThe test passes a custom pattern '\\.pdf$'. task_func compiles that regex and uses pattern.match(item), which attempts to match at the start of the filename. The pattern requires a leading dot before \"pdf\" so it will not match any of the filenames in the source (none are PDFs or start with a dot). Therefore no files are moved, the returned count is 0, and the target directory stays empty \u2014 matching the test assertions.\n\n[Result]\nPASS\n```", "test_directory_does_not_exist": "```plaintext\n[Explanation]\nThe test removes the source directory and then calls task_func. The function calls os.listdir(source_dir) without catching errors, so os.listdir will raise FileNotFoundError when the source directory does not exist, matching the test's expected exception.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_source_directory": "```plaintext\n[Explanation]\nThe test clears the source directory so there are no files. task_func iterates os.listdir(source_dir) (empty), does not move anything, and returns moved_files_count = 0. It also ensures the target directory exists but does not add files. Therefore the returned value is 0 and the target directory remains empty, satisfying both assertions.\n\n[Result]\nPASS\n```", "test_target_directory_creation": "```plaintext\n[Explanation]\nThe function creates the target directory, compiles the regex, and moves files from source to target when the filename matches r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'. The test removes the target, places three matching files (file1.txt, document1.doc, notes.docx) and three non-matching files in the source, then calls task_func and asserts the target exists and that at least one matching file was moved. The regex and match logic accept the provided filenames, so the directory will be created and matching files moved. Therefore the code satisfies the test conditions.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/945", "raw_responses": {"test_with_default_parameters": "```plaintext\n[Explanation]\nThe function returns a NumPy array from sklearn's predict with length equal to the input periods (13). When called with default parameters, sales_data is None, so the function generates deterministic random data (it resets the seed inside), fits a LinearRegression model, and predicts 13 future values. The test only checks the return type is np.ndarray and that its length is 13 \u2014 both conditions are satisfied.\n\n[Result]\nPASS\n```", "test_with_custom_parameters": "```plaintext\n[Explanation]\nThe function accepts the provided sales_data list of length 10, fits a LinearRegression on indices 0..9, predicts for indices 10..19, and returns a numpy array of length 10. The test only checks that the return is an ndarray and has shape (10,), which is satisfied. No errors are triggered for the given inputs.\n\n[Result]\n```plaintext\nPASS\n```", "test_with_random_sales_data": "```plaintext\n[Explanation]\nThe test only checks that task_func(periods=5) returns a numpy array of length 5. The function, when sales_data is None, generates sales (it resets the RNG to seed 42 internally, but the test does not depend on random values), fits a LinearRegression model, and predicts 5 future values. The returned forecast is a numpy.ndarray with shape (5,), so both assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_forecasted_values_increasing": "```plaintext\n[Explanation]\nThe provided sales_data = [100,150,200,250,300] is perfectly linear and increasing. task_func fits a linear regression to these points and predicts the next five values, yielding an increasing sequence (e.g., [350, 400, 450, 500, 550]). The test does np.assertFalse(all(forecasted[i] <= forecasted[i+1] ...)), which expects that NOT all consecutive forecasts are non-decreasing. Since the forecasts are strictly increasing, the all(...) is True and assertFalse(True) fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_with_specific_sales_data": "```plaintext\n[Explanation]\nThe test provides explicit sales_data = [100,200,300,400,500]. task_func uses that data (converted to a numpy array), fits LinearRegression on X = [0..4], then predicts for indices [5..9], producing a numpy array of length 5. The test only checks the return type is np.ndarray and that its shape[0] == 5, both of which are satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1015", "raw_responses": {"test_valid_webpage_url": "```plaintext\n[Explanation]\nThe patched requests.get returns a MagicMock with .content set to the HTML bytes; calling response.raise_for_status() on that MagicMock does nothing (no exception). lxml.html.fromstring accepts bytes and pandas.read_html can parse the provided simple HTML table into a DataFrame with one row. The DataFrame is written to SQLite and the function returns len(df) == 1, matching the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_local_file_url": "```plaintext\n[Explanation]\nThe patched builtins.open returns the provided HTML string for the file:// path. task_func reads that string, pd.read_html parses one-row table, writes it to SQLite, and returns len(df) == 1. No errors are raised, so the returned value matches the test expectation.\n\n[Result]\nPASS\n```", "test_invalid_url": "```plaintext\n[Explanation]\nThe test patches requests.get to raise requests.RequestException. In task_func, the requests.get call is inside a try/except that catches requests.RequestException and re-raises it. Therefore calling task_func('http://invalid-url.com') will raise requests.RequestException as expected by the test. No database actions occur, so no other errors are triggered.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_table": "```plaintext\n[Explanation]\nThe mocked requests.get returns a response with content b'<html><body><table></table></body></html>' and a callable raise_for_status (MagicMock), so no exception is raised. pandas.read_html on this HTML will either return an empty DataFrame (which makes df.empty true) or raise a ValueError which the code catches and treats as no tables. In both cases the function returns 0 as expected by the test. No database operations occur. Therefore the test's assertion of 0 will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_database_error": "```plaintext\n[Explanation]\nThe test patches requests.get to return HTML bytes containing a table and patches sqlite3.connect to raise sqlite3.DatabaseError. The function will parse the HTML (pd.read_html finds a non-empty DataFrame) and then call sqlite3.connect, which (due to the patch) raises sqlite3.DatabaseError. The function catches and re-raises that DatabaseError, so the test's with self.assertRaises(sqlite3.DatabaseError) condition is satisfied.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/34", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function removes URLs using the regex and then generates and returns a WordCloud object. For the test input, the URL \"https://www.example1.com\" will be removed by re.sub, so it will not appear in result.words_. The function returns the WordCloud instance created by WordCloud.generate, so isinstance check passes. There are no other errors (imports available in the same module), so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe regex removes the URL from the input text, leaving other words; words list is non-empty so a WordCloud is generated and returned. The returned object is an instance of WordCloud, and the URL string will not be present in result.words_. Therefore both assertions in the test will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function removes URLs, splits the input into words, ensures words remain, generates a WordCloud, and returns the WordCloud object. For the input 'There is no url in this sentence.' the words list is non-empty and WordCloud.generate will run, so the returned value is an instance of WordCloud. The matplotlib plotting (plt.show()) does not alter the return value and should not affect the isinstance check.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe regex will remove the entire URL \"https://www.example4.com\", leaving no words. The function then raises ValueError with the exact message 'No words available to generate a word cloud after removing URLs.', matching the test expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function removes URLs with the regex, splits and verifies there are remaining words, generates a WordCloud (WordCloud.generate returns the WordCloud instance), and returns it. For the given text both URLs will be removed, so result.words_ will not contain 'https://www.example51.com'. The plotting call (plt.show()) does not prevent the function from returning the WordCloud object in typical test environments.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/147", "raw_responses": {"test_return_type": "```plaintext\n[Explanation]\nThe test patches socket.socket so each created socket is a MagicMock; methods like settimeout, connect_ex and close are available (created dynamically), so threads run without raising. IPv4Network('192.168.0.0/24') produces hosts to scan, threads join, and the function returns a dict. The test only asserts the return is a dict, so the code will pass.\n\n[Result]\nPASS\n```", "test_open_port": "```plaintext\n[Explanation]\nThe test patches socket.socket to a MagicMock. The code calls sock.connect_ex, which (not being explicitly set) is itself a MagicMock; calling it returns a MagicMock, and comparing that to 0 yields another MagicMock. MagicMock instances are truthy in boolean context, so any(result.values()) will be True. No exceptions occur (settimeout/close are mocked), so the test assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_closed_port": "```plaintext\n[Explanation]\nThe test patches socket.socket but sets a side_effect on connect (not connect_ex). The code calls sock.connect_ex, which (with the patched socket) returns a MagicMock that is not equal to 0, so each scanned IP is recorded as False. IPv4Network('192.168.0.0/30') yields the expected hosts and threads join normally. Therefore all results are False and the test assertion (not any(result.values())) passes.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_all_ips_checked": "```plaintext\n[Explanation]\nThe function iterates over network.hosts(), which excludes the network and broadcast addresses. The test's expected_ips uses IPv4Network(ip_range) (iterating all addresses, including network and broadcast). Therefore result.keys() will be missing those two addresses for a /30, causing the assertion to fail.\n\n[Result]\nFAIL\n```", "test_return_value_structure": "```plaintext\n[Explanation]\nThe test patches socket.socket, so sock.connect_ex returns a MagicMock by default. The code assigns results[str(ip)] = (result == 0), but (MagicMock == 0) yields another MagicMock object, not a bool. The unittest asserts each value is an instance of bool, so the types won't match. Thus the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/424", "raw_responses": {"test_normal_functionality": "```plaintext\n[Explanation]\nThe function reads the test JPEG, converts BGR->RGB, applies KMeans with 3 clusters and returns the original and segmented numpy arrays. The test creates the image, calls task_func('test_image.jpg', 3), and checks shapes and exact pixel arrays that match the behavior of OpenCV + sklearn KMeans used in the code. There are no obvious errors (file exists, n_clusters validated, proper conversions, and outputs cast to uint8). Assuming the standard testing environment provides OpenCV and scikit-learn (as expected), the code will produce the arrays the test asserts.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_non_existent_file": "```plaintext\n[Explanation]\nThe function correctly raises FileNotFoundError for 'non_existent.jpg'. However, the provided unittest's tearDown uses the name \"os\" without importing it, causing a NameError during tearDown. That error causes the test run to error (not pass), so the code will not pass the single test case.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_n_clusters": "```plaintext\n[Explanation]\nThe function immediately checks n_clusters after confirming the file exists and will raise ValueError for n_clusters = -1, which matches the test's expectation. The dummy image is created in setUp so the existence check passes, and since the provided code imported os in the same execution namespace, tearDown's use of os will not error. Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_n_clusters_as_non_integer": "```plaintext\n[Explanation]\nThe function explicitly checks \"if not isinstance(n_clusters, int) or n_clusters <= 0: raise ValueError\". Calling task_func('test_image.jpg', 'three') passes a string for n_clusters, so the check will raise ValueError as the test expects. The image file exists (created in setUp) and required imports (os) are present at module scope, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_single_cluster_returns_original_image": "```plaintext\n[Explanation]\nThe function uses KMeans with n_clusters=1; that produces a single centroid equal to the mean color of all pixels, so the segmented image will be a uniform color (the centroid) rather than the original image. The test asserts the original and segmented numpy arrays are identical for n_clusters=1, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/526", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe input JSON for the tested file is a list with one dict {'a':2,'b':3,'c':4}. The function collects numeric values per key, computes mean and median with numpy, constructs a DataFrame indexed by keys and sorts them. This yields index ['a','b','c'] and mean/median 2.0 for 'a', satisfying the assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe test uses the second JSON file which contains [{'a': 1}]. The function reads the list, collects numeric values for key 'a', computes mean and median as 1.0, returns a DataFrame indexed by 'a'. This matches the asserted index and numeric values in the test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe input JSON for the tested case is [{'a': 1.5}, {'b': null}]. The implementation only adds keys to key_values when a numeric value is encountered. Because 'b' has a non-numeric None value it is never added, so the resulting DataFrame will only contain index 'a' (with mean/median 1.5). The test expects both 'a' and 'b' with NaN for 'b', so the index assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe JSON file contains an empty list. The function iterates over records to collect numeric values; with no records key_values remains empty, so results is empty and pd.DataFrame.from_dict returns an empty DataFrame. Its number of rows is 0, matching the test assertion.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe input file contains [{'a': 1.5, 'c': 4}, {'b': None}]. The function only appends values when isinstance(value, (int, float)), so 'b' with None is never added to key_values and therefore never appears in the resulting DataFrame index. The test expects the index to include 'b' with NaN statistics, so the assertion on df.index (and subsequent checks for 'b') will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function filters values using isinstance(value, (int, float)), so strings and None are ignored. For the test data, numeric values collected are a:[5], b:[4], c:[7]; means are 5.0, 4.0, 7.0. The DataFrame is sorted by index to ['a','b','c']. All assertions in the test match these results.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function reads the JSON list, collects only numeric (int/float) values per key (ignoring None), and computes mean/median using numpy. For the test data 'a' contains 1..100 (mean 50.5) and 'b' contains 4,8,...,200 (mean 102.0), matching the assertions. The code's type checks and NaN handling correctly exclude nulls, so the test expectations are met.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function only adds keys to key_values when a numeric (int/float and not NaN) value is encountered. For the test input both 'a' and 'b' have string values, so key_values (and thus results) stays empty and the returned DataFrame has no 'a' or 'b' index. The test then tries to access df.loc['a','mean'], which raises a KeyError instead of returning NaN. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function only collects values that are int/float and not NA. For the test data:\n- a collects [10, 5] -> mean 7.5\n- b collects [20, 25] -> mean 22.5\n- c collects [30] -> mean 30.0\nThese match the asserted values and the DataFrame indexing/formatting is correct, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/636", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nFor rows=0 the function creates an empty DataFrame (df.empty is True), so the first assertion passes. However, non_zero_counts becomes a Series of zeros for each of the five columns, and ax.bar(...) still creates bar Rectangle patches (with height 0) for each column. Therefore len(ax.patches) == 5, not 0, and the second assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe DataFrame length check passes (rows=1 gives len(df)==1). However, ax.patches includes the Axes background Rectangle in addition to the bar Rectangles, so after plotting 5 bars len(ax.patches) will be 6 (axes patch + 5 bars). The test expects 5, so it will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function creates a DataFrame with 10 rows and 5 columns, so len(df) == 10. ax.bar adds one Rectangle patch per column (5 bars), and the annotations/text do not add patches, so len(ax.patches) == 5. No errors are raised in the code path for this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function calls np.random.randint with size=(rows, len(COLUMNS)). Passing rows = -5 gives a negative dimension, causing numpy to raise ValueError (\"negative dimensions are not allowed\") before any DataFrame or Axes are returned. The test expects an empty DataFrame and zero patches, but the exception prevents those assertions from running. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a DataFrame with the given number of rows (np.random.randint shape=(rows, 5)), so len(df) == 1000. It then plots one bar per column (5 columns), producing 5 Rectangle patches in ax.patches. The annotations and tick labels do not add extra patches, so len(ax.patches) == 5.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/845", "raw_responses": {"test_case_identical_strings": "```plaintext\n[Explanation]\nFor inputs 'test' and 'test', the cleaned token lists are identical, producing term-frequency vectors [1] and [1], so cosine similarity = 1.0. Levenshtein.ratio('test','test') = 1.0. The returned tuple equals (1.0, 1.0). Types (numpy.float64 and float) compare equal in unittest.assertEqual, so the test will pass assuming required imports (numpy, Levenshtein) are available.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_different_strings": "```plaintext\n[Explanation]\nThe function logic produces cosine_similarity = 0.0 for 'test' vs 'different' (no shared tokens) and computes a Levenshtein ratio of 4/13 = 0.3076923076923077, matching the expected numeric result. However, the code imports Levenshtein.ratio from the external 'python-Levenshtein' package; if that third-party package is not installed in the test environment the import will raise ImportError and the test will fail. Because the presence of that dependency is not guaranteed, the code is not reliably runnable in the given test environment.\n\n[Result]\nFAIL\n```", "test_case_empty_strings": "```plaintext\n[Explanation]\nThe function logic returns (0.0, 1.0) for two empty strings (empty token vectors give cosine 0.0; Levenshtein.ratio('', '') == 1.0). However, the module attempts \"from Levenshtein import ratio\" at import time. In typical test environments the third-party \"python-Levenshtein\" package is not installed, causing a ModuleNotFoundError before the test runs. Therefore the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_similar_strings": "```plaintext\n[Explanation]\nThe cosine similarity is 0.0 as expected, but the code uses Levenshtein.ratio(text1, text2), which returns the similarity (13/21 \u2248 0.6190476190476191). The test expects 0.38095238095238093 (which equals the normalized distance 8/21), so the returned second value will not match the expected value. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_numerical_strings": "```plaintext\n[Explanation]\nThe cosine similarity will be 0.0 for '123' vs '321' (different tokens) which matches the test. However, the code uses Levenshtein.ratio(text1, text2). The python-Levenshtein ratio returns a similarity of (len1+len2-distance)/(len1+len2) which for '123' vs '321' (distance=2, sum lengths=6) yields 0.6666666667, not the 0.3333333 expected by the test. Therefore the Levenshtein assertion will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/952", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function sets 'Due Date' as datetime.date objects (datetime.now().date()), while the test compares those values to a string produced by datetime.today().strftime('%Y-%m-%d'). The element-wise comparison of date objects to strings yields False, so the assertion all(df['Due Date'] == self.today_str) fails. Other checks (columns, length, task name underscores) would pass.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function creates a pandas DataFrame with columns 'Task Name', 'Assigned To', and 'Due Date', uses random.seed when provided, and selects exactly n_tasks items via random.choices. For the given test (n_tasks=2, seed=123, tasks contain spaces/special chars), it will return a DataFrame instance with the expected column set and length 2. The module-level import \"from datetime import datetime\" is present in the same code, so the test's use of datetime is available. No errors are expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns a pandas DataFrame with columns 'Task Name', 'Assigned To', and 'Due Date', and it creates exactly n_tasks rows using random.choices (with replacement) and sets the seed when provided. For the tested n_tasks values (2, 10, 20, 100) it will produce a DataFrame instance whose columns match the expected set and whose length equals n_tasks. The test does not assert types or values of 'Due Date' entries beyond presence, so the date objects are acceptable. Therefore the code meets the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func with n_tasks = -1 and expects a ValueError.\nThe function checks \"if n_tasks < 0: raise ValueError(...)\" at the start, so it will raise ValueError as expected. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nFor n_tasks=0 the function returns a pandas DataFrame constructed from empty lists, producing zero rows and the exact columns {'Task Name', 'Assigned To', 'Due Date'}. The function checks n_tasks is non-negative, handles k=0 correctly via random.choices, and returns a DataFrame instance. The test's assertions (instance type, column set, and length 0) will be satisfied, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function calls random.choices(sanitized_tasks, k=n_tasks). With task_list empty and n_tasks=2 this attempts to sample from an empty population and raises an exception (IndexError). The test expects a DataFrame of length 0, so the function will not return and the test will fail.\n\n[Result]\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function uses random.seed(seed) (when seed provided) and random.choices(employees, k=n_tasks) to select the 'Assigned To' values. Since the test passes a custom_employees list, every assigned employee will be chosen from that list, so the assertion that all assigned employees are in custom_employees will hold. No other parts of the function affect this check.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function seeds Python's global random with the provided seed, uses random.choices to pick tasks and employees with replacement, and constructs DataFrames with the same index and due-date type. Calling task_func with the same seed (0) produces identical DataFrames so assert_frame_equal(df1, df2) passes. Using a different seed (100) yields a different random sequence, so df1.equals(df3) is False. There are no other issues that affect this test case.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function replaces spaces with underscores, so ['Task One','Task Two'] becomes ['Task_One','Task_Two']. With seed=42 the random selection (random.choices with k=2) yields both sanitized tasks (not duplicates), so the set of 'Task Name' equals {'Task_One','Task_Two'}. The test's assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_10": "```plaintext\n[Explanation]\nThe function returns a DataFrame of length n_tasks (2) and sanitizes names by replacing spaces with underscores. Given input ['Task', 'Task'] (no spaces), the 'Task Name' column will contain only \"Task\", so set(df['Task Name']) == {'Task'}. The seed and due date handling do not affect these assertions. Therefore the test assertions will be satisfied.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1019", "raw_responses": {"test_successful_ocr_extraction_and_encoding": "```plaintext\n[Explanation]\nThe patched pytesseract.image_to_string returns a non-empty string 'Extracted Text in cp1251', so the OCR branch is taken. Encoding to cp1251 then to utf8 succeeds (the text is ASCII-compatible), and the function returns the same string. The Image.open mock configuration (setting __enter__) is unnecessary but does not affect the outcome. The returned value matches the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_ocr_fails_comment_extraction_succeeds": "```plaintext\n[Explanation]\nThe test patches PIL.Image.open but only sets its return_value.__enter__.return_value to the mock image; the code calls Image.open(...) without a with-statement, so img becomes mock_open.return_value, not the provided self.mock_image. As a result img.info.get(...) yields a Mock, not the expected bytes, and the function ultimately returns a Mock object instead of the decoded string. Therefore the assertion comparing to the expected decoded string will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_ocr_succeeds_encoding_fails": "```plaintext\n[Explanation]\nThe test expects a ValueError when OCR text encoding fails. In the code, text.encode(invalid_encoding) raises a LookupError which the inner except converts to ValueError. However that ValueError is caught by the outermost except Exception block, so the function proceeds to the fallback branch and does not propagate the ValueError. Additionally, the test configures Image.open to return the mock via a context manager (__enter__), but the code calls Image.open() directly, so the image.info used in fallback is a different Mock; no decoding error occurs and the function returns a Mock instead of raising. Therefore the test's assertRaises(ValueError) will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_ocr_and_comment_extraction_fail": "```plaintext\n[Explanation]\nThe patched Image.open return value is not configured to be self.mock_image (the test only sets its __enter__.return_value), and pytesseract.image_to_string raises an Exception so execution goes to the fallback. In the fallback the mock Image.open(...) returns a Mock whose info.get(...) yields another Mock (truthy). The code then tries to encode/decode that Mock, producing and returning a Mock object rather than the expected empty string. Therefore the function returns a Mock, not '', and the assertion fails.\n\n[Result]\nFAIL\n```", "test_ocr_extraction_succeeds_no_encoding_needed": "```plaintext\n[Explanation]\nThe patched pytesseract.image_to_string returns a non-empty UTF-8 string. The function encodes/decodes with from_encoding='utf8' and to_encoding='utf8' without errors and returns the same string. The Image.open mock details (context manager) are irrelevant because OCR is used and the OCR mock return value is independent. Therefore the function returns the expected result.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/82", "raw_responses": {"test_app": "```plaintext\n[Explanation]\nThe function task_func creates and returns a Flask app instance (Flask(__name__)) and the test only checks that the returned object is an instance of Flask. task_func runs without raising exceptions during app creation. Although the test references Flask without importing it, the submission code imports Flask at module level so Flask is available in the shared execution namespace used by the test harness. Therefore the assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_protected_route_access": "```plaintext\n[Explanation]\nThe protected route is decorated with @login_required, so an unauthenticated GET to /protected will not show any \"Logged in as:\" text. The test creates a templates/login.html in the current working directory's templates folder, and render_template('login.html') will find it. Even if Flask-Login returns a 401 instead of redirecting, the response body still will not contain \"Logged in as:\". Thus the assertion assertNotIn('Logged in as:', ...) will pass.\n\n[Result]\nPASS\n```", "test_secret_key": "```plaintext\n[Explanation]\nThe task_func sets app.config['SECRET_KEY'] = secret_key and returns the Flask app. The test only asserts that this config value equals the provided secret_key. task_func executes without errors during setup, so the assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_login_page_accessibility": "```plaintext\n[Explanation]\nThe /login route handles GET by creating a LoginForm and rendering 'login.html'. The test creates a templates directory at the current working directory (./templates) and writes login.html there. Although task_func ignores the template_folder parameter (it sets app.config['TEMPLATES_FOLDER'] instead of app.template_folder), Flask's default template folder is './templates' relative to the app root, so render_template finds the file and returns 200. No POST or CSRF behavior is exercised. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_login_manager_initialization": "```plaintext\n[Explanation]\nThe function calls login_manager.init_app(app) inside task_func. The test patches flask_login.LoginManager.init_app and then calls task_func; the patched method will be the one bound/invoked, so mock_init_app will be called once with the created app. The setUp call to task_func runs before the patch (so unmocked) but the test's assertion checks only the patched call. No other errors in the code prevent this test from running.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_logout_route_redirects_to_login": "```plaintext\n[Explanation]\nThe test expects a protected template showing \"Logged in as: testuser\" and that logout redirects to the login page. The app's protected and home routes render 'protected.html' and 'home.html' which the test does not create (only 'login.html' is created). The function also sets app.config['TEMPLATES_FOLDER'] instead of configuring Flask's template_folder, so templates may not be found as expected. As a result the /protected request will attempt to render a missing template (raising TemplateNotFound / returning an error) and /logout redirects to home (not to the login page). The behavior does not match the test assertions.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/161", "raw_responses": {"test_log_to_csv_content": "```plaintext\n[Explanation]\nThe function writes CSV with columns 'type','timestamp','message' (lowercase) and the timestamp column contains datetime values serialized by pandas; the test expects columns named 'Type','Timestamp','Message' with timestamp strings. Column name (and casing) mismatch (and possibly dtype) causes pd.testing.assert_frame_equal to fail.\n\n[Result]\nFAIL\n```", "test_no_log_entries": "```plaintext\n[Explanation]\nThe test patches builtins.open to return an empty file (read_data=''). task_func will read no valid log lines, so log_data remains empty and the function raises ValueError(\"No valid log entries found\"), which matches the test's assertRaises(ValueError). No other code paths (like CSV writing) are reached.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_incorrect_format_log": "```plaintext\n[Explanation]\nThe patched open returns a single line that does not match the expected log regex. The function will skip the line, find no valid entries, and raise ValueError(\"No valid log entries found\"). The test expects a ValueError via assertRaises, so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```", "test_partial_correct_log": "```plaintext\n[Explanation]\nThe function correctly parses lines matching the specified log pattern, ignores non-matching lines, writes the parsed entries to 'log_data.csv', and returns that filename. For the provided test input (one valid and one invalid line) it will create the CSV and contain exactly one row, so the assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_malformed_timestamp": "```plaintext\n[Explanation]\nThe patched open returns a log with month \"00\", which matches the regex but is invalid for datetime.strptime. The function catches the parsing error and raises ValueError. The test expects a ValueError, so the code will raise it and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/443", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe input P is the 3x3 identity, so result equals T. T contains only 0s and 1s, giving 27 samples with two distinct values. KMeans with n_clusters=3 on 1D data with duplicate points can still produce 3 label classes (it will split identical-value points across centers), and sklearn's KMeans will run without error for 27 samples and k=3. Therefore the function will produce 3 unique cluster labels and the test's assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function flattens the 4x3x3 result into 36 scalar samples (shape (36,)) and runs KMeans on 36 one-dimensional points. The test expects 12 samples (self.P.shape[0] * n_clusters = 4*3 = 12) corresponding to 12 three-dimensional samples obtained by reshaping tensordot(...).swapaxes(...).reshape(-1, 3). Because the shapes and clustering inputs differ, the label arrays have incompatible shapes (36 vs 12) and the equality/assertions will fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function computes result shape (4,3,3) -> flattened length 36 and plots 36 scatter points. The test expects P.shape[0]*3 = 12 points. Because 36 != 12, the assertion comparing number of plotted points will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function produces a (4,3,3) result via einsum, flattening to 36 samples and reshaping to (36,1). KMeans with n_clusters in {1,3,5} is valid because 36 >= n_clusters, and sklearn's KMeans will produce the requested number of clusters (it handles empty-cluster cases by reinitializing centroids). The matplotlib plotting does not affect clustering and is safe in the test (plots are closed in tearDown). Therefore the assertions on unique cluster counts will be satisfied.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function calls KMeans(...).fit_predict(...) on the reshaped data. For n_clusters values -1 and 0, scikit-learn's KMeans will raise a ValueError (n_clusters must be > 0). For the string 'invalid', KMeans will raise an error when validating/converting the parameter (ValueError/TypeError). These exceptions occur before plotting, so the test's with self.assertRaises(Exception) will be satisfied for each invalid value.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function directly uses np.einsum('ij,jkl->ikl') without input validation. For the first call P.shape (2,2) vs T.shape (3,3,3) the contracted dimension mismatches; for the second call T is 2D but einsum expects 3D. np.einsum raises ValueError in both cases and the function does not catch it, so both assertRaises checks succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_7": "```plaintext\n[Explanation]\nThe test passes random_state='invalid' (a string). KMeans uses sklearn's random-state checking (via check_random_state) during fit_predict, which raises a ValueError for invalid types. The call to kmeans.fit_predict(reshaped_result) will therefore raise ValueError, satisfying the test's assertRaises.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/528", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function reads all CSV rows (including header) and uses Counter to count duplicates. For the test data, ('Alice','25','New York') appears 3 times and ('Bob','30','London') 2 times, so duplicates becomes {(...):3, (...):2}. The test compares this dict to a Counter with the same items; dict equality with a Counter having identical key-value pairs is True. The matplotlib pyplot object (plt) is available from the submitted code so test teardown works. Therefore the test should pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test asserts the plot title equals 'Duplicate Entries', but the implementation sets the title to 'Duplicate Rows Count'. Although the Axes object and ylabel match, the mismatched title causes the assertion to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns an empty dict {} for no duplicates ({} == Counter() is True), but it always creates and returns a matplotlib Axes object even when there are no duplicates. The test expects ax to be None, so the assertion self.assertIsNone(ax) will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function immediately checks the file extension and raises ValueError if it does not end with '.csv'. The test calls task_func('sample_data.txt') inside assertRaises(ValueError), so the expected ValueError will be raised. The test's tearDown references plt, which is defined because the submitted code imports matplotlib.pyplot as plt at module scope. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function first checks the .csv extension (the test filename ends with .csv) then attempts open(file_path, 'r'). Opening a non-existent file raises FileNotFoundError, which satisfies the test's assertRaises. The matplotlib.pyplot instance (plt) is imported in the provided code, so tearDown's plt.close('all') will not cause an unrelated NameError.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/637", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test expects df.shape == (10, 5), but the function creates 5 course columns then adds a 'Student_ID' column, so df.shape will be (10, 6). The plot title matches the expected string, but the DataFrame shape assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function adds a 'Student_ID' column to the DataFrame after creating five course columns, so for num_students=50 the DataFrame shape is (50, 6). The test expects shape (50, 5), causing the first assertion to fail. (The plot title check would pass, but the shape mismatch causes the test to fail.)\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe DataFrame created has 5 course columns plus an added 'Student_ID' column, so its shape is (100, 6). The test expects (100, 5). The plot title matches, but the mismatched DataFrame column count causes the test to fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function adds a 'Student_ID' column to the DataFrame after creating 5 course columns. For num_students=1 the DataFrame shape becomes (1, 6), but the test expects (1, 5). The axis title check would pass, but the DataFrame shape assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe DataFrame is created with 5 course columns and then a 'Student_ID' column is added, so for num_students=5 the DataFrame shape becomes (5, 6). The test expects (5, 5), so the assertion on shape will fail. (The plot title would match.)\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/854", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns factorial sums [3,3] for permutations of [1,2]; permutations produced are [(1,2),(2,1)]. The test checks equality of result (order-sensitive) and uses assertCountEqual for permutations (order-insensitive), so both assertions succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function correctly validates input, generates all permutations of [1,2,3], and computes the sum 1!+2!+3! = 9 for each permutation, producing [9,9,9,9,9,9] and the six expected permutations. The test uses assertEqual on the results list and assertCountEqual on permutations (order not required). No errors or edge cases apply for this input.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nFor input [1], the function validates the list, generates permutations -> [(1,)], and computes factorial sums: math.factorial(1) = 1. reduce on a single-element list returns that element, so factorial_sums = [1]. The function returns ([1], [(1,)]), which matches expected result and expected permutation (assertCountEqual accepts the tuple). No errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nCalling task_func([]) passes input validation and triggers the explicit empty-list branch which returns ([], []). The test expects result == [] and perm to be an empty sequence; both assertions (assertEqual on result and assertCountEqual on perm) succeed. No exceptions occur.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_5": "```plaintext\n[Explanation]\nAll test inputs are invalid types or contain invalid elements. The function first rejects any non-list input (raising TypeError), then checks each list element for being an int and non-negative (raising TypeError or ValueError). Each assertRaises call therefore receives an exception as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/955", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lowercases and uses re.findall(r'[a-zA-Z0-9_]+') which strips punctuation, so 'world!' becomes 'world' (test expects 'world!'). Also the code appends modified_strings (['hello']) to the word list, causing 'hello' to be counted twice, so the first bar height is 2 rather than the expected 1. Therefore the xtick label and bar height assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function lowercases and tokenizes the text using the regex r'[a-zA-Z0-9_]+' which splits \"Hello world!\" into ['hello', 'world']. It then appends the modified string 'hello_world' (no '!') resulting in x-tick labels ['hello', 'world', 'hello_world']. The test expects the first x-tick label to be 'hello_world!' which does not match the function's actual first label 'hello'. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function lowercases the input text and uses a regex that strips punctuation, so x-tick labels become ['hello', 'world'] (lowercase, no '!'). The test expects 'Hello' and 'world!' to appear, so those assertions will fail. Although the bar heights would be correct (1), the label checks cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function lowercases the text and tokenizes it, then appends the modified string 'Lorem_ipsum' (from mystrings) unchanged, so the plot's x-tick labels include 'Lorem_ipsum'. The returned object is a matplotlib Axes. Therefore both assertions (instance check and presence of 'Lorem_ipsum' label) succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lowercases the text and extracts words with the regex, producing xtick labels: 'hello', 'world', and 'hello_world' (without punctuation). The test expects a label 'hello_world!' (with an exclamation mark), which will not be present. (The bar height check would pass, but the label assertion fails.)\n\n[Result]\nFAIL\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function produces x-axis labels: ['hello', 'world', 'Hello_World'] (the modified string has no trailing '!'). The test expects 'Hello_World!' to be among the tick labels, which is not true. Although the first bar height equals 2, the label assertion fails.\n\n[Result]\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function appends the modified string 'not_in_text' to the words list before counting and building unique_words, so actual_words includes 'not_in_text' and it is set as an x-tick label. The test expects 'not_in_text' to be absent, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function checks \"if not text\" and raises ValueError(\"Input text cannot be empty\") for an empty string. The test calls task_func([], '') inside assertRaises(Exception), and ValueError is a subclass of Exception, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function lowercases the input text, extracts word tokens (alphanumeric and underscores) which yields ['test','123','is','fun'], then appends the modified input strings ['test_123', '#$%!'] so the x-axis tick labels become ['test','123','is','fun','test_123','#$%!']. Both 'test_123' and '#$%!' are therefore present in ax.get_xticklabels() texts. The plotting calls return an Axes object, so the test's membership assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_10": "```plaintext\n[Explanation]\nThe function lowercases the text yielding three 'duplicate' tokens, but then appends the two entries from mystrings to all_words before counting. That makes the Counter report 5 occurrences (3 from text + 2 from mystrings) so the plotted bar height is 5, while the test expects 3.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/1020", "raw_responses": {"test_get_data_with_default_parameters": "```plaintext\n[Explanation]\nThe test patches requests.get and chardet.detect. requests.get.return_value.content is set to cp1251-encoded JSON bytes and chardet.detect returns {'encoding': 'cp1251'}. The function decodes using the detected 'cp1251', re-encodes/decodes to utf8, and json.loads correctly returns the expected dict. No exceptions or mismatches occur in this scenario.\n\n[Result]\nPASS\n```", "test_get_data_with_custom_url_and_encodings": "```plaintext\n[Explanation]\nThe test patches requests.get and sets its return_value.content to bytes encoded in latin1. task_func is called with from_encoding='latin1' so the function decodes the bytes correctly, re-encodes to utf8, decodes back to string, and json.loads returns the expected dict. The mock patch target 'requests.get' affects the same requests module used by task_func, so the call is intercepted. No errors expected.\n\n[Result]\n```plaintext\nPASS\n```", "test_get_data_with_empty_response": "```plaintext\n[Explanation]\nThe test patches requests.get so that response.content is b''. The function calls requests.get(...), reads response.content, and returns {} when content is empty (if not content: return {}). That matches the expected_output {}. No other code paths are invoked, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_get_data_with_invalid_json": "```plaintext\n[Explanation]\nThe test expects task_func() to raise json.JSONDecodeError for invalid JSON bytes. However, the implementation catches json.JSONDecodeError around json.loads(...) and returns an empty dict instead. Therefore no JSONDecodeError is propagated and the test's assertRaises will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_get_data_with_different_valid_encoding": "```plaintext\n[Explanation]\nThe test patches requests.get to return a mock whose .content is UTF-8 encoded JSON bytes. task_func is called with from_encoding='utf8', so it decodes the bytes correctly, re-encodes/decodes with utf8, and json.loads returns the expected dict. No network calls are made and no errors are raised.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_get_data_with_undetectable_encoding": "```plaintext\n[Explanation]\nThe mocked requests.get returns non-empty bytes and chardet.detect returns {'encoding': None}. The function checks for a detected encoding and raises ValueError(\"Unable to detect encoding for non-empty content\") for non-empty content when encoding is None. The test expects that ValueError and checks the message substring, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/37", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function trains a RandomForestClassifier, builds a DataFrame of feature importances, sorts it descending, and plots a horizontal barplot with x=importance and y=feature. The axis title and labels are set as required. Because the plotted bar widths are taken directly from the sorted importances DataFrame, the list of bar widths matches the model.feature_importances_ sorted in descending order. The returned model is the classifier and ax is the matplotlib Axes, satisfying the test assertions.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function trains a RandomForestClassifier, computes feature importances, sorts them descending, and plots a horizontal barplot with the specified labels and title. The seaborn barplot uses the sorted dataframe so ax.patches widths match the sorted importances. The returned types and axis properties match the test assertions. The random_state ensures deterministic importances.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function trains a RandomForestClassifier on the provided DataFrame, computes feature importances, sorts them descending, and draws a seaborn horizontal barplot with x as importance and y as feature. The title and axis labels are set as required. The bar widths correspond to the sorted importances, so the test's comparison of descending importances to ax.patches widths should match. The returned model and axes types are correct.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function trains a RandomForest on the dataframe and plots feature importances. For the given test, there is only one feature ('N'), so model.feature_importances_ == [1.0] and the seaborn horizontal bar will have a single patch with width 1.0. The labels and title are set as required. The imports (RandomForestClassifier, plt) are present in the provided code, so the isinstance checks succeed. Therefore all assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function trains a RandomForestClassifier, builds a DataFrame of feature importances, sorts it descending, and plots a horizontal seaborn barplot with x='importance' and y='feature'. The test checks the model type, Axes type, labels/title (which match exactly), and that the list of bar widths equals the model's importances sorted descending. Since the plotted bars are produced from the same importance values (in the sorted DataFrame), the widths will match the sorted feature_importances_. The returned ax is a matplotlib Axes and the model is the expected class, so the test case will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/162", "raw_responses": {"test_histogram_content": "```plaintext\n[Explanation]\nThe function extracts alphabetic words and computes word lengths; for the provided text this matches the test's word list. The test then calls ax.hist(...) on the returned Axes with an explicit bins array (np.arange(max(...) + 2) - 0.5). matplotlib returns the same bins array that was passed in, so the assertion comparing bins to expected_bins succeeds. The test's use of np and re is satisfied because the module code already imported numpy as np and re in the same namespace. No other errors occur.\n\n[Result]\n```plaintext\nPASS\n```", "test_empty_text": "```plaintext\n[Explanation]\nFor an empty input string the function finds no words, takes the else branch setting axis limits and returns the Axes object. The test then calls ax.hist([], bins=[]), which with matplotlib produces no bars (empty patches list). No exceptions are raised by task_func, so the assertion len(patches) == 0 holds.\n\n[Result]\n```plaintext\nPASS\n```", "test_single_word": "```plaintext\n[Explanation]\nThe function finds the word \"Hello\" and plots a histogram with a bin covering length 5. The test then calls ax.hist([5], bins=[4.5, 5.5]) on the returned Axes, which will produce one patch and count 1 (n[0] == 1). There are no conflicts or type issues that would cause the assertions to fail.\n\n[Result]\n```plaintext\nPASS\n```", "test_histogram_bin_counts": "```plaintext\n[Explanation]\nThe function extracts alphabetical words and creates a histogram of their lengths. For the provided input the word lengths are [3,3,5,4,4,3,5,5,4,3]. Calling ax.hist(...) in the test on the returned Axes uses explicit bins [2.5,3.5,4.5,5.5], which will yield counts [4,3,3] and three patches. Nothing in task_func interferes with this subsequent hist call, so the assertions match.\n\n[Result]\nPASS\n```", "test_rwidth_parameter_effect": "```plaintext\n[Explanation]\nThe function calls ax.hist(word_lengths, ..., rwidth=rwidth, ...) when words exist. For the provided text the word_lengths list is non-empty, so hist is called. The test patches plt.Axes.hist (which is the method used by the Axes instance created in task_func), captures the call, and asserts the 'rwidth' kwarg equals 0.5. That matches the call in task_func, so the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/308", "raw_responses": {"test_dataframe_structure": "```plaintext\n[Explanation]\nThe function builds a DataFrame with columns equal to FIELDS in order and then adds 'Average Grade'. With default additional_fields=[] the resulting columns are ['Physics','Math','Chemistry','Biology','English','History','Average Grade'], matching the test's expected_columns. random.seed only affects values, not column names, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_additional_fields": "```plaintext\n[Explanation]\nThe function builds a DataFrame with columns in the order of FIELDS + additional_fields + ['Average Grade']. It fills subject grades with random.randint(0,100) for 100 students, computes per-student averages, and appends an 'Average' row whose values are the mean of each subject (also within 0\u2013100). The test seeds the RNG, checks the DataFrame type, exact column order (which matches insertion order), and that all non-'Average Grade' columns have values between 0 and 100 (student scores and subject averages satisfy this). No errors or type issues occur, so the test will pass.\n\n[Result]\nPASS\n```", "test_grades_range": "```plaintext\n[Explanation]\nThe function generates integer grades using random.randint(0, 100) for each subject and student, and the test sets random.seed(0) before calling it. The appended \"Average\" row is computed as the mean of student grades (and the overall mean), which yields floats within 0\u2013100. The test checks all subject columns except 'Average Grade' for values between 0 and 100 inclusive, so all entries (student grades and the average row) satisfy this.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_average_grade": "```plaintext\n[Explanation]\nThe function generates deterministic random grades after random.seed(0) and computes each student's average using statistics.mean. Pandas' per-row mean (row[:-1].mean()) will match the previously stored 'Average Grade' (both use IEEE double arithmetic on small integer sums), and the test skips the appended 'Average' row. No type or indexing mismatches occur, so the assertion should hold for every student row.\n\n[Result]\nPASS\n```", "test_subject_average": "```plaintext\n[Explanation]\nThe function computes each subject's mean over the student rows, appends that mean as the 'Average' row, and then the test compares that appended value to df[column].mean(). Appending the mean to a set doesn't change the mean ((sum + mean) / (n+1) == mean), so df[column].mean() equals the previously computed subject average. No other errors (random seeding, lengths, types) affect this test. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_non_negative_grades": "```plaintext\n[Explanation]\nrandom.seed(0) is set before calling task_func(). The function generates grades using random.randint(0, 100) (no negatives) and computes averages (means) of those non-negative numbers, so all DataFrame entries are >= 0. The test checks (df >= 0).all().all(), which will be True.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/445", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a Voronoi object from the given points (with tiny jitter), plots it using matplotlib, and returns (vor, ax). For the provided 4 points the Voronoi construction succeeds, vor.point_region has length 4, and voronoi_plot_2d adds plot elements so ax.get_children() is non-empty. The test's checks for isinstance(vor, Voronoi), matching lengths, and ax being a matplotlib Axes are satisfied.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function sets the numpy RNG with the given seed and applies a tiny deterministic jitter to the input points before calling scipy.spatial.Voronoi. Calling task_func twice with the same seed produces identical jittered points (hence identical vor.ridge_points), while a different seed produces different jitter and different ridge_points. The function returns (vor, ax) as expected and uses pyplot already imported at module scope so plt.close in teardown is available. Therefore the provided test assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function jittered the nearly-collinear input by a larger amount (1e-10) with a fixed seed, so the points are non-degenerate for Qhull. Voronoi(jittered_points) should succeed and produce a Voronoi object whose point_region length equals number of input points. voronoi_plot_2d returns an Axes instance and adds plot children, so the assertions (instance checks and children > 0) will pass. No other errors are expected.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test expects task_func to raise an exception for two points. SciPy's Voronoi called with only two 2D points raises a QhullError, so the with self.assertRaises(Exception) will succeed. However, the test's tearDown calls plt.close('all') and the test file does not import matplotlib.pyplot as plt, so plt is undefined in the test namespace; tearDown will raise a NameError after the test body, causing the test to error/fail overall.\n\nTherefore the provided code will not pass the given test case.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test passes a 1D numpy array (shape (3,)) to task_func. The function does not validate shape and will proceed; calling Voronoi on a 1D array (or the subsequent voronoi_plot_2d which expects 2D) will raise an exception (ValueError/QhullError or an indexing/plotting error). The test uses assertRaises(Exception), so any such exception satisfies it. The matplotlib.pyplot name used in tearDown is available because the submitted code imports plt at module level. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nCalling task_func('Not valid points') causes an AttributeError when the code tries to access points.shape (a str has no .shape). AttributeError is an Exception, so the test's with self.assertRaises(Exception): will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/530", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly identifies duplicate names (Alice appears twice), converts ages to integers, builds a Counter {25: 2}, and returns an Axes with title and labels set to 'Age' and 'Count'. All checks (non-empty, columns present, no negative ages) pass for the provided df_valid. The histogram creation and returned ax satisfy the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function checks for negative ages with \"if (df['age'] < 0).any(): raise ValueError\". The test DataFrame contains -1, so the function raises ValueError as expected by the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe input DataFrame has no duplicate names; the function detects zero duplicate_names and returns an empty Counter() and None for the axes. All prechecks (columns, non-empty, non-negative ages) pass, so the test_case_3 assertions will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function correctly handles the provided DataFrame: it is non-empty, ages are non-negative and converted to ints, duplicate names are detected (Alice appears 3 times), and the age distribution Counter becomes Counter({25: 3}). A matplotlib Axes is created and its title and axis labels are set as expected. Therefore the assertions in test_case_4 (counter equality and plot checks) will be satisfied.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function correctly identifies duplicate names (Alice and Bob), collects their ages [25,26,25,27], produces Counter({25:2, 26:1, 27:1}) and returns a matplotlib Axes with the title and 'Age'/'Count' labels. These match the test expectations, so the test_case_5 will pass.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function floors float ages and finds duplicate names. For df_floats the floored ages are [25,26,25,27,26]; duplicates (Alice and Bob) produce ages [25,26,25,27], giving Counter({25:2, 26:1, 27:1}) as expected. The function returns a matplotlib Axes with title, xlabel 'Age' and ylabel 'Count', satisfying the plot checks. No errors are raised for this input.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe test calls task_func with df_empty which is an empty DataFrame. The function begins by checking if df.empty and raises ValueError(\"DataFrame cannot be empty\") in that case. The test expects a ValueError, so the behavior matches and the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/857", "raw_responses": {"test_successful_transfer": "```plaintext\n[Explanation]\nThe test patches glob.glob (so the function will process the specified source file paths) and creates the source files in setUp. The code uses shutil.copy2 (not patched) to copy the actual files, which exist, into the destination directory; copies succeed and filenames are appended in the same order as the provided extensions ('.txt', '.csv'). Patching shutil.move has no effect. The returned list matches the expected ['file1.txt', 'file2.csv'].\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_source_directory": "```plaintext\n[Explanation]\nThe test patches glob.glob to always return an empty list, so task_func will find no files for the given extensions and return an empty list. Although the test patches shutil.move, the implementation uses shutil.copy2; this does not matter because no copy calls occur when glob returns [], so the returned value matches the expected [].\n\n[Result]\n```plaintext\nPASS\n```", "test_invalid_file_extensions": "```plaintext\n[Explanation]\nThe test patches glob.glob to return an empty list and calls task_func with extensions that don't match any files. The function iterates extensions, finds no files (glob returns []), performs no copy operations, and returns an empty list. This matches the asserted result [].\n\n[Result]\nPASS\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches glob.glob and shutil.move and expects that moving the file raises an exception and a warning containing \"Unable to move file\" is issued, and that no files were transferred. The provided function uses glob.glob (so that part is patched) but uses shutil.copy2 to copy files, not shutil.move. Because copy2 is not patched, it will successfully copy the mocked file from the real temporary source to destination, so transferred_files will contain the filename. Additionally, the function issues a warning with text \"Could not transfer file ...\" on exceptions, not \"Unable to move file\". Both the returned list and the warning text therefore do not match the test expectations.\n\n[Result]\n```plaintext\nFAIL\n```", "test_all_extensions": "```plaintext\n[Explanation]\nThe test patches glob.glob to return the same list of four file paths for every extension, but the implementation calls glob.glob once per extension and copies every returned path each time. That causes each of the four filenames to be appended once per extension (4 extensions \u00d7 4 files = 16 entries), whereas the test expects only the four matching filenames. Also the test patches shutil.move (mock) but the code uses shutil.copy2, so the mock_move is unused \u2014 though this doesn't prevent copying, it reinforces that the test's intention (per-extension matching via glob) is broken by the mock returning all files for every call. Therefore the returned list won't match the expected list.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/654", "raw_responses": {"test_return_types": "```plaintext\n[Explanation]\nThe test provides a numpy array of strings (e.g. '332', '1', '33', ...). task_func extracts x_values and y_values from that array but does not convert them to numeric types. scipy.optimize.curve_fit will call the model function which performs numeric operations (np.exp, multiplication) on these string arrays, causing a TypeError/ValueError before any return. Therefore the function will raise an exception and not produce the expected (coeffs, ax) outputs, so the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_target_value_found": "```plaintext\n[Explanation]\nThe test provides a numpy array of strings. task_func extracts x_values and y_values from the array without converting to numeric types, so x_values and y_values are string arrays (e.g., x_values = ['332','332','332']). scipy.optimize.curve_fit will call the model function with these string arrays, causing numeric operations (like -b * x and np.exp) to raise a TypeError. As a result the function will raise an exception and not return coefficients, so the test will fail.\n\n[Result]\nFAIL\n```", "test_target_value_not_found": "```plaintext\n[Explanation]\nThe test passes a numpy array of strings and looks for a target '999' that does not occur. task_func filters rows and produces empty x_values and y_values. Calling scipy.optimize.curve_fit with empty data raises a ValueError, which matches the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_not_enough_points": "```plaintext\n[Explanation]\nThe test passes a small array with a single column. After filtering, filtered_data has shape (1,1), so extracting y_values = filtered_data[:, 1] raises an IndexError (second column doesn't exist). The test expects a ValueError to be raised, so the raised IndexError does not satisfy the assertion. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_functionality": "```plaintext\n[Explanation]\nThe test provides a numpy array of strings but the code filters rows by matching the string '332' in the first column, so filtered rows are found. The x and y arrays become string representations of numbers but scipy.optimize.curve_fit will convert them to floats, so fitting proceeds and returns popt with length 3 (a, b, c). The function returns (popt, ax), and the test only checks popt.shape == (3,), which will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/963", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function creates a zip at the target path and walks the source directory. For an existing empty source directory, os.walk yields no files, so the zip is created with zero entries. The test counts entries with the given extensions and expects 0. No errors occur (source exists, target dir created), so the test will pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe source directory exists and contains only files with extensions '.pdf' and '.gif', none of which are in the function's valid_extensions list ('.txt', '.docx', '.xlsx', '.csv'). task_func will create the target directory, create an empty zip (no files match), and return its path. The test counts matching entries in the zip (zero), so the assertion self.assertEqual(..., 0) will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function zips files with extensions .txt and .docx (among others). The test creates a source directory 'some_matching_files' containing c.txt, d.docx, and e.png, then calls task_func on that directory. The code walks the directory, includes c.txt and d.docx in the zip, excludes e.png, and the test counts 2 matching entries. All checks (existence, target creation, relative arcname) are correct, so the produced zip will have 2 matching files.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function walks the given source directory, adds files with extensions ['.txt', '.docx', '.xlsx', '.csv'] into a zip at the target location, and returns the zip path. The test creates a directory 'all_matching_files' containing exactly four files with those extensions. task_func will include all four files in the zip and zip_file_count will return 4. No errors or edge cases affect this test.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function walks the given source directory recursively and adds files whose extensions are in ['.txt', '.docx', '.xlsx', '.csv'] to the zip. For the test, source_directory points to '.../nested_dir' which contains nested/j.txt and nested/k.docx (l.png is ignored). The zip will contain those two matching files, so the counted matches equal 2 as asserted. No errors or mismatches occur.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe function walks the given source directory and adds only files whose lowercased names end with one of the valid extensions ['.txt', '.docx', '.xlsx', '.csv'] into the zip. The test points the source to the 'mixed_extensions' directory which contains o.txt, p.docx, q.unknown, r.csv \u2014 three matching files. The zip creation and counting logic align, so the test's expected count of 3 will be produced.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function walks the given source directory recursively, adds only files with extensions .txt, .docx, .xlsx, .csv, and writes them into the zip using paths relative to the source. The test calls task_func on the 'subdirs_with_files' directory which contains exactly four files with those valid extensions, so the created zip will contain those four entries and the test's assertion matches.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1022", "raw_responses": {"test_valid_input": "```plaintext\n[Explanation]\nThe function filters rows where the date column equals today's date, but the test CSV contains only future dates (today+1, +2, +3). Therefore the returned DataFrame will be empty and the test's assertion that the DataFrame is not empty will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe test calls task_func('non_existing.csv', 'Date') and expects a FileNotFoundError. The function immediately checks os.path.exists(csv_file_path) and raises FileNotFoundError if the file does not exist. The test's setUp creates only 'valid.csv', so 'non_existing.csv' does not exist and the correct exception will be raised.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_column_not_found": "```plaintext\n[Explanation]\nThe test creates a CSV whose header does not include the column 'Date' (it uses 'NotDate'), writes it to 'invalid.csv', and then calls task_func expecting a ValueError. The function reads the CSV and explicitly raises ValueError if the requested column is missing. Therefore the function will raise ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_file": "```plaintext\n[Explanation]\nThe test creates an empty file and calls task_func on it. The function checks existence, then pd.read_csv on an empty file raises pandas.errors.EmptyDataError, which the code catches and returns an empty DataFrame. The test asserts df.empty, so the function behavior satisfies the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_no_future_dates": "```plaintext\n[Explanation]\nThe function reads the CSV, converts the 'Date' column to datetime using format '%Y-%m-%d', and returns only rows where the date equals today. The test creates a CSV with past dates (2020-01-01 and 2020-01-02), so no rows match today and the returned DataFrame is empty. No exceptions are raised, so the assertion df.empty passes.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/89", "raw_responses": {"test_original_data_unchanged": "```plaintext\n[Explanation]\nThe function creates original_data = data.copy() and does not modify the input array in-place (np.delete returns a new array). Therefore the returned original_data is equal to the passed-in self.data, satisfying the assert_array_equal check.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_data_without_outliers": "```plaintext\n[Explanation]\nThe z-scores for column 1 of the test data [2,4,6,1000] do not exceed the threshold 3.0 (the extreme value yields a z \u2248 1.73), so outlier_indices is empty and filtered_data has the same length as the input. The test only asserts len(filtered_data) <= len(self.data), which will be True. Plotting does not affect the return values.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_return_type": "```plaintext\n[Explanation]\nThe function executes without errors given standard scientific Python packages. For the test data column (values [2,4,6,1000]) the Z-scores computed by scipy.stats.zscore do not exceed the threshold 3.0, so no indices are removed. The function returns a tuple: original_data (ndarray), filtered_data (ndarray), and outlier indices as a tuple \u2014 exactly what the unit test checks. The plotting calls do not affect the returned types.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_no_plotting": "```plaintext\n[Explanation]\nThe test patches matplotlib.pyplot.show and calls task_func. The function creates plots and calls plt.show(), so the patched mock will be invoked and mock_show.assert_called() will succeed. No errors (e.g., division by zero) occur for the provided data, so the function runs to completion.\n\n[Result]\n```plaintext\nPASS\n```", "test_no_change_in_data_dimension": "```plaintext\n[Explanation]\nThe function removes rows whose Z-score in column 1 exceeds 3.0. For the test data, the outlier is the last row [1000, 1000], so filtered_data has shape (3,2) \u2014 the number of columns remains 2. No errors expected from zscore or plotting, so the assertion comparing number of columns will pass.\n\n[Result]\nPASS\n```", "test_plot_titles": "```plaintext\n[Explanation]\nThe function creates a matplotlib figure with two subplots and sets their titles to 'Data with Outliers' and 'Data without Outliers'. The test patches plt.show so the figure remains accessible, and plt.gcf() will return the created figure. The titles match exactly, so the assertion will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/177", "raw_responses": {"test_word_frequencies": "```plaintext\n[Explanation]\nThe function lowercases all content words before counting, so keys and counts are aggregated in lowercase (e.g., 'data': 2, 'what': 2). The test's expected dictionary uses mixed-case keys and splits counts between cases (e.g., 'Data': 1 and 'data': 1, 'What': 1 and 'what': 1). Because of these casing and count differences, the returned dict will not equal the expected dict.\n\n[Result]\nFAIL\n```", "test_no_matching_articles": "```plaintext\n[Explanation]\nThe function filters rows whose Title matches the word-boundary regex for \"like\" or \"what\" (case-insensitive). For the test DataFrame provided in test_no_matching_articles none of the titles match, so filtered_df is empty, the loop is skipped, and the function returns the initialized empty dict {} as expected. There are no other checks or errors triggered for this input.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test calls task_func with an empty DataFrame (columns present but no rows). The function checks \"if df.empty or 'Title' not in df.columns ...\" and raises ValueError for an empty DataFrame. The test expects an empty dict {} instead of an exception. Therefore the test will fail.\n\n[Result]\nFAIL\n```", "test_case_sensitive_handling": "```plaintext\n[Explanation]\nThe function lowercases all content words before counting, so the returned keys are lowercase (e.g., \"technological\", \"these\"), but the test's expected dictionary uses original capitalization for \"Technological\" and \"These\". This mismatch in key casing causes the equality assertion to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_invalid_columns": "```plaintext\n[Explanation]\nThe test creates a DataFrame with columns 'Headline' and 'Body' (no 'Title' or 'Content'). The function begins by raising ValueError if 'Title' or 'Content' are missing. Therefore task_func(df_invalid) will raise ValueError as expected and the test will pass. No other errors occur.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/310", "raw_responses": {"test_file_creation": "```plaintext\n[Explanation]\nThe provided task_func imports random at the top-level, generates and writes the CSV file, and returns the absolute path. The test sets random.seed(0) (random is available from the module-level import), calls task_func to create 'test_file_creation.csv', and asserts the returned path exists. The file is created and tearDown removes it. No errors are expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_content_rows": "```plaintext\n[Explanation]\nThe function writes 1 header row + 100 people rows + 1 averages row = 102 rows. The test seeds random (random is already imported in the provided code) and only asserts the row count, so the produced file will have 102 rows and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_averages_calculation": "```plaintext\n[Explanation]\nThe test seeds random, calls task_func to create a CSV with 100 integer records and an averages row, then reads back the individual values and recomputes means and compares to the written averages. task_func uses the same RNG, writes exact integer fields for individuals and computes averages with statistics.mean; the CSV writer writes float values using Python's repr that round-trips. The sums involved are small integers (well under 2**53), so floating-point summation and division produce identical results in both computations. File creation and cleanup match the test's usage. Therefore the computed and read-back averages will be equal and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_header": "```plaintext\n[Explanation]\nThe function writes the CSV header using the constant COLUMNS = ['Name', 'Age', 'Height', 'Weight']. The test checks only that header. The test calls random.seed(0) \u2014 the module already imported random, so that name is available in the test's global namespace when calling task_func. The file is created and the header matches exactly, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_average_row_label": "```plaintext\n[Explanation]\nThe test calls random.seed(0) but does not import random in the test file, which will raise NameError before the test can invoke task_func. Although the function writes an 'Average' label correctly as the first element of the final CSV row, the missing import in the test causes the test run to error out, so it will not pass.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/454", "raw_responses": {"test_move_no_files": "```plaintext\n[Explanation]\nThe function checks that both TemporaryDirectory paths exist (they do), uses glob to find \"*.txt\" in the empty source directory (returns an empty list), performs no moves, and returns an empty list. The test asserts len(files_moved) == 0, so the code meets the test's expectation.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_extension": "```plaintext\n[Explanation]\nWith ext = '' the search pattern becomes \"*.\" so glob.glob finds no files (the created 'test.txt' does not match), moved_files stays empty and the function returns an empty list. That satisfies the test assertion that zero files are moved.\n\n[Result]\n```plaintext\nPASS\n```", "test_move_specified_extension_files": "```plaintext\n[Explanation]\nThe test patches glob.glob to return two source files and os.path.exists to indicate the source and destination directories exist and that file1 already exists in destination. The function skips file1 and moves file2, calling shutil.move with ('/fake/source/file2.txt','/fake/destination/file2.txt') and returning ['/fake/destination/file2.txt']. The test accepts either destination-only or full-destination path for the move call (it tries the former and falls back to the latter), and asserts that one file was moved and that '/fake/destination/file2.txt' is in the returned list. All assertions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_no_files_moved_with_different_extension": "```plaintext\n[Explanation]\nThe function searches for files matching \"*.txt\" in the temporary source directory. The test creates a single file \"test_file.md\" (different extension), so glob finds no files, moved_files remains empty, and the function returns an empty list. The assertion expecting zero moved files is satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_exception_raised_when_dirs_do_not_exist": "```plaintext\n[Explanation]\nThe function checks existence of src_dir and dest_dir and raises FileNotFoundError if either does not exist. The test first cleans up the source directory and calls task_func \u2014 this raises FileNotFoundError as expected. It then cleans the destination (the source is already removed) and calls task_func again; this call also raises FileNotFoundError (due to the missing source), and the test only asserts that a FileNotFoundError is raised, not which path caused it. The unused os import in the test helper is harmless because the helper is never called. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/532", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns Counter(values) which for the input [1,2,3,4,5] yields Counter({1:1,2:1,3:1,4:1,5:1}). The test asserts the counter equals Counter() (empty), so this assertion will fail. The plotting checks would pass, but the counter mismatch causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function returns Counter({1:5}) for the input series and constructs a matplotlib Axes with the title and axis labels set as expected. While fitting a normal distribution to constant data yields sigma=0 which produces degenerate PDF values, these become scaled by a bin width of 0 and may produce NaNs but matplotlib handles plotting them without raising an exception. The tests check the counter and the plot's type/title/labels, which the function satisfies.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test file references plt (e.g., plt.Axes and plt.close) but only imports \"matplotlib\", not \"matplotlib.pyplot as plt\". That causes a NameError in the test (plt is undefined), so the test will fail regardless of the task_func behavior. Although task_func correctly handles an empty DataFrame, the test itself is invalid and will not pass.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function returns a Counter counting all values including 4:1, but the test asserts the Counter equals Counter({5:4, 1:4, 2:3, 3:2}) (which omits the value 4). Thus the asserted equality will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function calls ax.hist(values, bins=bins) which creates one Rectangle patch per bin, so len(ax.patches) will equal the requested bins (for the provided continuous random data). The histogram and axis labels/title are set as expected. Therefore the test that checks len(ax.patches) for bins in [2,10,20] will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe DataFrame's 'value' column contains strings. task_func performs numeric operations (ax.hist and norm.fit) which will raise a TypeError when given non-numeric data. The test expects a TypeError, so the function call triggers the expected exception.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/655", "raw_responses": {"test_extract_topics": "```plaintext\n[Explanation]\nThe function logic correctly preprocesses texts, vectorizes with TF-IDF, runs NMF with n_components=2 (valid for 3 documents), and returns a list of topic-word lists, so it would satisfy the test assertions if all dependencies are available. However, at import time the code calls nltk.download('stopwords') and then immediately accesses nltk.corpus.stopwords.words('english'). In many isolated test environments the NLTK stopwords corpus is not present and network access for downloading is restricted, causing a LookupError or download failure at runtime. This would make the test fail before reaching the assertions.\n\n[Result]\nFAIL\n```", "test_invalid_num_topics": "```plaintext\n[Explanation]\nThe test calls task_func with num_topics = 0 and expects a ValueError. The code passes n_components=0 to sklearn.decomposition.NMF; scikit-learn validates n_components and raises a ValueError for non-positive values. Therefore the function invocation will raise ValueError and satisfy the test.\n\n[Result]\n```plaintext\nPASS\n```", "test_empty_texts": "```plaintext\n[Explanation]\nThe test calls task_func([], 1) and expects two values (topics, ax). The provided task_func returns only one value (topics), so the test will fail when trying to unpack the return. Additionally, with an empty list of texts the code will attempt to fit TfidfVectorizer on an empty corpus, which would raise an exception before producing any result. Either way, the function does not meet the test's expectations (returning a two-value tuple and handling empty input), so it fails the test.\n\n[Result]\nFAIL\n```", "test_single_text": "```plaintext\n[Explanation]\nThe function returns one topic per NMF component. For the single input document and num_topics=1, TF-IDF produces a non-empty vocabulary (many non-stopword tokens remain), and NMF with n_components=1 fits and yields one component; the code then appends one topic to the list. Therefore len(topics) == 1 and the test's assertion passes.\n\n[Result]\n```plaintext\nPASS\n```", "test_all_stopwords": "```plaintext\n[Explanation]\nThe test calls task_func(...) expecting two return values (topics, ax) but the provided task_func returns only a single value (topics), so unpacking will fail. Additionally, for an input of only stopwords the preprocessing yields empty documents and TfidfVectorizer.fit_transform will raise a ValueError (\"empty vocabulary\"), so the function will error before returning. Either error causes the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```"}}
{"id": "BigCodeBench/865", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function converts the input to a DataFrame, applies scipy.stats.zscore to 'Count' (which produces values whose mean is effectively 0) and sklearn's MinMaxScaler to 'Weight' (which yields values in [0,1]). The item order is preserved by the DataFrame. For the provided input, the z-score mean is within the 0.001 tolerance and the normalized weights are within [0,1], so the assertions will succeed.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function uses scipy.stats.zscore for counts and sklearn MinMaxScaler for weights. For the input counts [-100, -200, -150], zscore yields values that sum to 0 (mean ~ 0 within float tolerance), satisfying the mean assertion. For weights [-0.5, -0.1, -0.2], MinMaxScaler maps them to [0, 0.75, 1], so min >= 0 and max <= 1. There are no other issues in the code for this test case.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe input has identical counts (100,100,100) and identical weights (0.5,0.5,0.5). scipy.stats.zscore on a constant array yields NaNs (division by zero), so 'Normalized Count' will be all NaN, satisfying the isna() check. sklearn's MinMaxScaler maps a constant feature to 0.0 for all entries, so 'Normalized Weight' will be all 0.0. The assignments to DataFrame columns work as expected. Therefore both assertions in the test pass.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function computes z-score for counts and min-max scaling for weights. For the input counts (1e6, 2e6, 1.5e6) the z-scores are [-1.2247, 1.2247, 0.0], whose mean is 0 (within the 0.001 tolerance). The weights (0.5, 0.6, 0.7) scale to [0.0, 0.5, 1.0], so min >= 0 and max <= 1. Thus all assertions in the test pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nFor a single-row input, scipy.stats.zscore returns NaN because the standard deviation is zero, and sklearn's MinMaxScaler maps a constant feature to 0.0. The test asserts the normalized weight equals 0.0 and the normalized count is NaN, so the function's outputs match the expectations.\n\n[Result]\nPASS\n```", "test_return_value": "```plaintext\n[Explanation]\nThe input list contains a tuple with 4 elements ('C', 999, 3, 0.7) while the code expects tuples of length 3 and calls pd.DataFrame(data, columns=['Item','Count','Weight']). pandas will raise a ValueError about the mismatch in number of columns, so the function will not produce the expected DataFrame and the test will fail.\n\n[Result]\nFAIL\n```", "test_large_data_amount": "```plaintext\n[Explanation]\nThe function constructs a DataFrame from the input, applies scipy.stats.zscore to the counts and sklearn MinMaxScaler to the weights, and returns the columns in the same order the test expects. The test computes the expected normalized values using the same zscore and MinMaxScaler calls on the same input. Indexes and column order match, and dtype checking is disabled in the assertion, so the DataFrames should be equal. Therefore the code will pass the test.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/964", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function will find the single 'sample.txt' file in the temporary source directory, read it with UTF-8, create a DataFrame with a 'Text' column, write 'sample.csv' into the target directory and return converted_count == 1. No errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function supports '.docx' files, reads the document paragraphs, creates a DataFrame with the combined text, writes sample.csv to the target directory, and returns 1. The test creates a single .docx file and expects 1 converted file and that sample.csv exists \u2014 both conditions are met.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test creates a single .xlsx file and expects task_func to convert it to sample.csv and return 1. The function handles '.xlsx' by reading with pandas.read_excel, converting to a single 'Text' column, writing CSV, and incrementing converted_count. The output filename is sample.csv in the target directory. There are no logic errors in this path, so the function should succeed (assuming pandas/openpyxl are available, which the test environment uses). \n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test creates a single CSV file in the source directory and calls task_func. The function supports '.csv', reads it with pandas, converts to a single-column DataFrame and writes a CSV to the target directory, incrementing converted_count. No exceptions should occur for this simple CSV case, so num_converted will be 1 and the output file will exist.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test creates an empty source directory and calls task_func. The function checks existence, iterates os.listdir on the empty directory (no files), never increments converted_count, and returns 0. This matches the test's expected value.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function immediately checks os.path.exists(source_directory) and raises FileNotFoundError if it does not exist. The test calls task_func with a clearly non-existent path '/path/does/not/exist' and expects FileNotFoundError, so the behavior matches the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_7": "```plaintext\n[Explanation]\nThe function lists files in the provided source directory and only processes files with extensions in {'.txt', '.docx', '.xlsx', '.csv'}. The test places a single unsupported.pdf file in the source directory. The code will skip that file, not increment converted_count, and return 0 as expected. No errors are raised.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function processes .txt, .docx, .xlsx and .csv in the source dir and increments converted_count for each supported file even though all outputs use the same base name (they overwrite, which the function allows). The test creates one of each extension named \"sample\" and only asserts the returned count equals 4. No apparent runtime errors (assuming pandas, python-docx and openpyxl are available), so it should return 4.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function only iterates top-level entries in source_directory and skips directories. The test places sample.txt inside a nested subdir, so task_func will not see or convert it and will return 0 while the test expects 1.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/553", "raw_responses": {"test_non_empty_lists": "```plaintext\n[Explanation]\nThe function creates a 3x3 DataFrame and calls df.plot(kind='bar'), which returns a matplotlib Axes object (AxesSubplot, a subclass of matplotlib.axes.Axes). For the provided inputs (lists of length 3) this yields an Axes instance, so the isinstance check against matplotlib.axes.Axes will succeed (assuming numpy, pandas, and matplotlib are available in the test environment).\n\n[Result]\nPASS\n```", "test_empty_a_list": "```plaintext\n[Explanation]\nThe function handles an empty 'a' by creating a DataFrame with shape (0, 3) and calling df.plot(kind='bar'). pandas.plot returns a matplotlib Axes (AxesSubplot) even for an empty DataFrame, so isinstance(ax, matplotlib.axes.Axes) is True. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_b_list": "```plaintext\n[Explanation]\nWhen b is empty, num_cols becomes 0 and the DataFrame has shape (3, 0) (no columns). Calling df.plot(kind='bar') on an empty DataFrame raises a ValueError (\"No numeric data to plot\") instead of returning a matplotlib.axes.Axes. Therefore the test asserting an Axes instance will not be reached.\n\n[Result]\n```plaintext\nFAIL\n```", "test_both_lists_empty": "```plaintext\n[Explanation]\nWith a=[] and b=[] the function creates a DataFrame with shape (0, 0) (no columns). Calling df.plot() on an empty DataFrame raises an error (pandas reports no numeric data to plot) instead of returning a matplotlib.axes.Axes, so the test expecting an Axes instance will fail.\n\n[Result]\nFAIL\n```", "test_a_list_longer_than_columns": "```plaintext\n[Explanation]\nThe function creates a DataFrame with shape (6,2) and calls df.plot(kind='bar'), which returns a matplotlib Axes (AxesSubplot), a subclass of matplotlib.axes.Axes. The isinstance check in the test will succeed. There are no operations that would raise errors for the given inputs.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1039", "raw_responses": {"test_file_found": "```plaintext\n[Explanation]\nThe test expects that after wrapping the socket the code will call secure_socket.send(...) and secure_socket.close(), but task_func never sends any data or closes the secure socket. Therefore the assertions mock_secure_socket.send.assert_called() and mock_secure_socket.close.assert_called() will fail. Other parts (SSL context, recv, file read and hash) align with the test.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_file_not_found": "```plaintext\n[Explanation]\nThe function returns 'File not found' when os.path.exists is False (matching the first assertion) and correctly wraps the socket via the mocked SSL context. However, the test also expects the function to send the error message over the secure socket and close it (mock_secure_socket.send(...) and mock_secure_socket.close() to have been called). The implementation does not call secure_socket.send(...) or secure_socket.close(), so those assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe function catches the raised exception and returns the expected error message, and the test patches SSLContext so wrapping works. However, the test also asserts that secure_socket.close() was called. The implementation never closes the wrapped socket in the normal or exception path, so mock_secure_socket.close.assert_called() will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_task_func_empty_file": "```plaintext\n[Explanation]\nThe test patches ssl.SSLContext (so ssl.create_default_context will instantiate the mocked SSLContext), socket.socket, os.path.exists, and builtins.open. The function uses create_default_context which will return the mocked context, wrap_socket returns the mocked secure socket, and recv returns the encoded file path. os.path.exists is patched to True and open is mocked to return empty bytes. The code computes SHA256 over the (empty) file correctly and returns the hex digest; it also opens the file in 'rb' as the test asserts. No mismatched calls or exceptions occur under these mocks.\n\n[Result]\nPASS\n```", "test_task_func_large_file": "```plaintext\n[Explanation]\nThe function uses ssl.create_default_context which instantiates ssl.SSLContext; the test patches ssl.SSLContext so create_default_context returns the mocked context (mock_context) and wrap_socket returns mock_secure_socket as set. The mocked secure socket's recv returns the requested path, os.path.exists is patched True, and builtins.open is mocked with the large binary content; iter(lambda: file.read(4096), b\"\") will read the content and stop on b'' so the SHA256 of the provided bytes matches the expected value. All calls the test asserts (including open(..., 'rb')) align with the code, so the test should pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/458", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns a single-row DataFrame created from the normalized dictionary: the 'a' list stays as a single list value in one cell, while expected_output has three rows (one per element of 'a') with 'b' and 'c' repeated. Shapes and contents differ, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\njson_str is '{}' which loads to an empty dict. The function builds normalized_dict = {} and then returns pd.DataFrame([normalized_dict]). pd.DataFrame([{}]) produces a DataFrame with one index (shape (1,0)), whereas expected_output is an empty DataFrame (shape (0,0)). assert_frame_equal will detect the shape/index mismatch, so the test will fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function normalizes the JSON correctly (a -> [2, 'apple', 6], b -> 9.8, c -> 10.0, d -> 'banana') but creates the DataFrame with pd.DataFrame([normalized_dict]) which produces a single-row DataFrame whose 'a' cell is a list. The test expects a 3-row DataFrame with 'a' expanded into rows and other scalar values broadcast to length 3. Shapes differ, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function parses the JSON and converts string numbers using regex to floats, then doubles them. For the input, 'a' -> float(1)*2 = 2.0, 'b' -> 5.0, 'c' remains 'string'. The resulting DataFrame has values [2.0, 5.0, 'string'], which matches the expected DataFrame when check_dtype=False (integer vs float dtype difference is ignored).\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function leaves list values as a single list element in one row (it builds the DataFrame with pd.DataFrame([normalized_dict])), so for input '{\"a\": [1, 2, {\"b\": 3}], \"c\": 4.9}' it produces one row with a = [2, 4, {'b': 3}] and c = 9.8. The test expects three rows (a column expanded into rows with c repeated). Shapes/values differ, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/313", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function extracts the target text from the filename (removing bracketed parts in the filename), not from the file contents. The test creates a file named \"test_file1.txt\" whose contents start with \"example[content]\". task_func will therefore use \"test_file1.txt\" as the target subdirectory key, not \"example\". The test expects a key \"example\" in moved_files, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe test writes the strings \"example[content]\" etc. into the file contents, but the function extracts the target text from the filename (not the file's content). Filenames are \"test_file1.txt\", \"test_file2.txt\", \"test_file3.txt\", so moved_files will contain keys for those filenames rather than \"example\" and \"sample\". The test expects keys \"example\" and \"sample\", so assertions will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_3": "```plaintext\n[Explanation]\nThe function determines target subdirectory from the filename (it strips bracketed parts from the filename, not file contents). The test file is named \"test_file1.txt\" (its content is \"[example]content\" but the code doesn't read file contents), so the created key in moved_files will be \"test_file1.txt\", not \"content\". Therefore 'content' is not in moved_files and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe code always extracts the first non-bracketed token from the filename; for 'test_file1.txt' this is 'test_file1.txt', so the function creates a subdirectory named 'test_file1.txt', moves the file into it, and returns moved_files = {'test_file1.txt': ['test_file1.txt']}. The test expects moved_files to be {}, so the output will not match.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function extracts text from the filename (not the file content). For the test, the filename is \"test_file1.txt\" which yields target_text \"test_file1.txt\"; the file will be moved into a subdirectory named \"test_file1.txt\" and recorded in moved_files. moved_files will be non-empty, so the assertion len(moved_files) > 0 passes.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/184", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation/numbers, removes the provided stopwords (including \"this\", \"is\", \"a\", \"and\", \"are\", \"for\"), and then CountVectorizer builds token counts. The resulting tokens are exactly {'analysis','cool','nltk','python','sklearn','test','text','useful'} and CountVectorizer/get_feature_names_out returns them in alphabetical order, producing the same DataFrame values and dtypes as expected. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function lowercases, removes non-letters (so \"GPT-4\" -> \"gpt\"), removes stopwords (removes \"is\" and \"with\"), then uses CountVectorizer which yields alphabetically ordered features ['amazing','chat','chatgpt','gpt','hello','world']. The resulting counts for the three documents match the expected DataFrame exactly, so the test will pass.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function lowercases, removes non-letter characters, strips stopwords (including \"is\", \"the\", \"with\"), and vectorizes the remaining words. For the three input sentences this yields tokens: \n1) openai, develops, cool, models\n2) deep, learning, future\n3) stay, updated, latest\nCountVectorizer produces an alphabetically ordered feature matrix with counts matching the expected DataFrame.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation, removes stopwords (including \"the\" and \"over\"), and vectorizes the remaining words. For the test rows the tokens become:\n1) quick, brown, fox\n2) jumps, lazy, dog\n3) lorem, ipsum, dolor, sit\nCountVectorizer will produce the vocabulary in alphabetical order ['brown','dog','dolor','fox','ipsum','jumps','lazy','lorem','quick','sit'], matching the expected columns and counts. Data types and indices align, so the produced DataFrame equals the expected one.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lowercases, strips punctuation, removes stopwords (including \"you\", \"are\", \"a\"), and vectorizes the remaining words. For the input rows it produces 'hello there', 'general kenobi', 'bold one'. CountVectorizer yields features in alphabetical order ['bold','general','hello','kenobi','one','there'] and the resulting counts match the expected DataFrame (same values, column order, and indices). No edge cases prevent matching.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/870", "raw_responses": {"test_default_data": "```plaintext\n[Explanation]\nThe function computes column-wise means for the default data: Position 0 has only strings -> NaN; Position 1 mean of [1,2,3,4,5] = 3.0; Position 2 mean of [2.1,3.2,4.3,5.4,6.5] = 4.3. The test assertions (np.isnan for Position 0, equality checks for Positions 1 and 2) match the returned DataFrame. No errors expected.\n\n[Result]\nPASS\n```", "test_custom_data": "```plaintext\n[Explanation]\nThe function transposes the tuples and computes means ignoring non-numeric entries. For the test data:\n- Position 1 has values (10, 20, 30) \u2192 mean 20.0\n- Position 2 has values (20.5, 40.6, 60.7) \u2192 mean 40.6\nThe DataFrame uses indices 'Position 0', 'Position 1', 'Position 2', so df.loc[...] accesses are valid and the numeric comparisons succeed.\n\n[Result]\nPASS\n```", "test_incomplete_data": "```plaintext\n[Explanation]\nThe function uses itertools.zip_longest(fillvalue=np.nan) to align positions and then filters values with isinstance(x, (int, float)) and not np.isnan(x). For the test input:\n- Position 1 collects [1,2,4,5] -> mean 3.0 (exact).\n- Position 2 collects [3.2,5.4,6.5] -> mean 15.1/3 = 5.033333333333333, which is close to 5.0333333.\nIndexes are named 'Position i' and the column 'Mean Value' matches the test lookups. Therefore the assertions will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_data": "```plaintext\n[Explanation]\nWhen called with an empty list the function returns pd.DataFrame(columns=['Mean Value']), which has zero rows and thus df.empty is True. Therefore the test assertion will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_non_numeric_data": "```plaintext\n[Explanation]\nThe function transposes the input and computes means only from values that are instances of int or float. For the test input (all strings), no numeric values are found for any position, so each mean becomes np.nan. The DataFrame will therefore contain only NaNs, making df.isna().values.all() evaluate to True.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/969", "raw_responses": {"test_incremental_values": "```plaintext\n[Explanation]\nThe function computes cumulative sums per column and applies MinMaxScaler, matching expected transformed values: A -> [0.0, 0.4, 1.0], B -> [0.0, 0.66666667, 1.0]. Input has no NaNs and all numeric, so no exceptions. The output dtype and values meet the test tolerance.\n\n[Result]\nPASS\n```", "test_negative_numbers": "```plaintext\n[Explanation]\nThe function computes per-column cumulative sums and applies MinMax scaling. For the provided input, column A cumsum is [-1, -3, -6] which scales to [1.0, 0.6, 0.0], and column B cumsum is [-3, -5, -6] which scales to [1.0, 0.33333333, 0.0]. The function checks for emptiness/NaNs and numeric types (all satisfied), uses a copy so the original DataFrame isn't modified, and returns the DataFrame matching the expected values within the tolerance.\n\n[Result]\n```plaintext\nPASS\n```", "test_all_zeros": "```plaintext\n[Explanation]\nThe input DataFrame has only zeros. For each column cumsum is [0,0,0]; sklearn's MinMaxScaler converts constant features to 0.0, so the function returns a DataFrame of zeros with the same columns. No checks (empty, NaN, non-numeric) fail, and dtype check is ignored in the test, so the output matches the expected DataFrame.\n\n[Result]\n```plaintext\nPASS\n```", "test_same_numbers": "```plaintext\n[Explanation]\nThe function computes cumulative sums per column and applies MinMax scaling. For the test input A: [5,5,5] -> cumsum [5,10,15] -> scaled [0.0,0.5,1.0]; B: [2,2,2] -> cumsum [2,4,6] -> scaled [0.0,0.5,1.0]. The DataFrame shapes, column order, and values match the expected output; dtype check is disabled in the assertion. No errors (empty/NaN/non-numeric) occur for this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_non_numeric_data_raises": "```plaintext\n[Explanation]\nThe test passes a DataFrame with column 'A' of strings and column 'B' numeric. The function first checks empty/NaN (neither true) and then uses pd.api.types.is_numeric_dtype on each column; because 'A' is non-numeric this check fails and the function raises TypeError as expected by the test.\n\n[Result]\nPASS\n```", "test_nan_values_raise": "```plaintext\n[Explanation]\nThe provided DataFrame contains a NaN. The function immediately checks df.isnull().any().any() and raises ValueError(\"DataFrame contains NaN values\") before any further processing. The test expects a ValueError, so the behavior matches.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test calls task_func with an empty DataFrame. The code immediately checks df.empty and raises ValueError(\"DataFrame is empty\"), which matches the test's expectation. No other code is executed for this test, so it will pass.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/324", "raw_responses": {"test_empty_file_list": "```plaintext\n[Explanation]\nWith an empty file_list the function's loop that starts threads does nothing, so no subprocess.Popen is invoked and it returns the initialized empty list. The patched subprocess.Popen is therefore not called and the result is [].\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_valid_files": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and sets mock_popen.return_value.poll.return_value = 0, but the code calls process.wait() and then reads process.returncode. The mock's returncode was not set to 0, so the function will append Mock objects (not integers 0) to exit_codes. Therefore the assertion comparing to [0, 0] will fail, though the call_count check would pass.\n\n[Result]\nFAIL\n```", "test_valid_directory_and_files": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and sets mock_popen.return_value.poll.return_value = 0, but the code calls process.wait() and then reads process.returncode. The mock's returncode is not configured, so process.returncode will be a Mock object (not integer 0). Thus task_func returns a list of Mock objects instead of [0, 0], causing the equality assertion to fail. (mock_popen.call_count would be 2, but the main assertion on return values fails.)\n\n[Result]\nFAIL\n```", "test_process_still_running": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and sets poll.return_value = None, but the code calls process.wait() and then reads process.returncode. The mock's returncode was never set, so process.returncode is a Mock object (not None). The returned list will not equal [None], so the assertion fails.\n\n[Result]\nFAIL\n```", "test_multiple_processes_with_different_exit_codes": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and sets poll.side_effect expecting the code to use poll to obtain exit codes for three separate subprocesses. The implementation calls process.wait() and then reads process.returncode instead of using poll. Also the patch leaves mock_popen.return_value the same for all calls (no side_effect), so all threads receive the same mock process object. As a result the returned exit_codes will be mock objects (and identical) rather than [0, 1, None]. Therefore the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```"}}
{"id": "BigCodeBench/1040", "raw_responses": {"test_queue_empty_condition": "```plaintext\n[Explanation]\nThe server may enqueue a response for the first client and then the test's client immediately closes. If the server later tries to send that queued response in the writable loop, sock.send can raise an OSError/BrokenPipeError because the peer closed; the writable-handling code has no try/except, so such an exception will terminate the server thread. If the server thread dies, the subsequent new_client.connect will fail (connection refused) and the test will not observe the expected echoed response. Thus the code is likely to fail this test.\n\n[Result]\nFAIL\n```", "test_server_response": "```plaintext\n[Explanation]\nThe server accepts connections, reads data, queues a timestamped response, and sends it via select-driven writable handling. The test connects, sends a short message, and blocks on recv until the server replies. There are no blocking or logical errors that would prevent the server from echoing the message in this scenario, so the assertion should succeed.\n\n[Result]\nPASS\n```", "test_multiple_connections": "```plaintext\n[Explanation]\nThe server implements a non-blocking select-based echo that appends a timestamp. The test starts the server thread, connects five times (sequentially), sends \"Test\", and reads the response. The server accepts connections, reads data, enqueues a timestamped response, and sends it back. There are no protocol mismatches, and the response format includes the original \"Test\" string. Timing (1s startup and sequential client interactions) is sufficient. No evident errors for this test case, so it should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_no_data_received": "```plaintext\n[Explanation]\nThe server accepts connections but only sends data in response to received data; it does not send any initial greeting or close the client socket. The client connects and calls recv with a 2s timeout, so with no data sent by the server recv will block until the timeout and raise socket.timeout. There are no actions in the server that would close the connection or send data before the client's recv timeout, so the test's expectation is met.\n\n[Result]\nPASS\n```", "test_server_closes_after_duration": "```plaintext\n[Explanation]\nThe server is started with run_duration=10s. The test waits 5s (server still running), connects a client, and calls recv with a 2s timeout without sending any data. The server only echoes data when it receives some, so no data will be sent and client.recv will raise socket.timeout within 2s. This matches the expected exceptions in assertRaises, so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_large_data_transfer": "```plaintext\n[Explanation]\nThe server accepts the connection, reads the 1000-byte payload (recv buffer 1024), queues a response containing the original data plus a timestamp, and sends it back. The client waits until it observes the original large_data substring in the received data. Although the server does not handle partial sends by re-queuing unsent tails, on local loopback a ~1000-byte message will normally be sent in one send() call, so the client will receive the complete original data and the assertion will succeed. The server runs for 10 seconds (enough for the exchange) and the test's timing/synchronization is adequate.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/187", "raw_responses": {"test_default_parameters": "```plaintext\n[Explanation]\nThe test fixes np.random.seed(42) and expects coordinates generated by alternating random draws per city (lon1, lat1, lon2, lat2, ...). The implementation draws all longitudes first and then all latitudes (consuming the RNG in a different order), so the generated Point coordinates won't match the expected list. Therefore the equality assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_custom_cities": "```plaintext\n[Explanation]\nThe function logic is correct for the test: it generates two rows and includes the provided city names. However, the code imports non-standard libraries geopandas and shapely at the top. In typical execution environments for these exercises those packages are not installed, causing an ImportError before the test runs. Therefore the test will not pass.\n\n[Result]\nFAIL\n```", "test_invalid_dic": "```plaintext\n[Explanation]\nThe function would raise ValueError for a non-tuple 'Lon' as the test expects, but the module does top-level imports of geopandas and shapely.geometry. If those third-party packages are not installed, the import will fail before the test calls task_func, causing the test to error out. Because the environment is unlikely to have geopandas/shapely available, the test will not pass.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_coordinate_ranges": "```plaintext\n[Explanation]\nThe function validates the input ranges, uses numpy.uniform to generate longitude/latitude values within the provided ranges, creates shapely Point(lon, lat) objects, and returns them in a GeoDataFrame column 'Coordinates'. The test checks that all point.x and point.y are between 0 and 10; numpy.uniform with those bounds produces values in that interval, and the Points' .x/.y accessors work. Therefore the code satisfies the test.\n\n[Result]\nPASS\n```", "test_return_type": "```plaintext\n[Explanation]\nThe module imports geopandas as gpd and shapely Point, task_func constructs and returns a gpd.GeoDataFrame. The test calls task_func() and checks isinstance(gdf, gpd.GeoDataFrame). Given the imports succeed, the returned object is a GeoDataFrame, so the assertion will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/461", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function checks the script exists, launches it with bash, and monitors CPU/memory via psutil while the script runs. The test creates an executable script that sleeps 5s; task_func's default timeout is 10s so the script will finish normally. The function returns a dict containing 'CPU Usage' and 'Memory Usage', which satisfies the test assertions. No errors are expected in this environment.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function checks the script exists, launches it with bash, and monitors the process with psutil until it finishes (default timeout 10s > sleep 5s). cpu_usage accumulates floats from psutil.cpu_percent() and memory_usage accumulates integers from mem_info.rss, matching the test assertions (CPU Usage is float, Memory Usage is int). No other errors or early exits occur for this test case.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function immediately checks os.path.exists(script_path) and raises FileNotFoundError if the path does not exist. The test calls task_func('non_existent_script.sh'), which does not exist, so the function will raise FileNotFoundError as expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function runs the provided bash script and samples psutil.Process.cpu_percent() and memory_info repeatedly until the process exits. Although the first cpu_percent() call can return 0.0, the loop samples immediately and then again after 0.1s; the provided script (10000 echo iterations) will almost certainly run longer than 0.1s on the test environment, so subsequent cpu_percent() calls will be >0 and the accumulated 'CPU Usage' will be greater than 0. No other errors (file exists, bash present, psutil usage) are expected for the given test.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe test only asserts that 'Memory Usage' is >= 0. The function returns a dict with 'Memory Usage' initialized to 0 and then increased by psutil.Process(...).memory_info().rss while the subprocess runs. Even if the process finishes before monitoring, the value will be 0, which satisfies the assertion. Therefore the test will pass.\n\n[Result]\nPASS\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function checks the script exists, launches it with subprocess.Popen(['bash', script_path]), monitors via psutil, and always returns a dict with 'CPU Usage' and 'Memory Usage'. The test provides an existing script that sleeps 5 seconds; this will run without errors and the function will return a dict. No conditions in the test assert specific metric values, only that the return is a dict. Therefore the code will pass this test.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function launches the script, monitors it with psutil, and always returns a dict with 'CPU Usage' and 'Memory Usage'. With timeout=0 the loop runs one monitoring iteration, then the timeout check triggers a terminate and the function proceeds to return the metrics dictionary. No exceptions are raised in this flow, so isinstance(resources, dict) is True.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_8": "```plaintext\n[Explanation]\nThe test creates a short bash script that blocks on \"read\" (waiting for stdin). task_func launches the script via \"bash <script>\", monitors it with psutil, and enforces the provided timeout (5s). On timeout it terminates the subprocess and returns a dictionary with 'CPU Usage' and 'Memory Usage'. The test only asserts that the returned value is a dict, which the function returns unless an import/OS error occurs. Given a typical Linux test environment with bash and psutil available, the function will return a dict and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe test calls task_func with a path containing an embedded null byte ('\\x00'). The function starts by calling os.path.exists(script_path). On CPython, filesystem calls with embedded null bytes raise ValueError (\"embedded null character in path\") (os.stat/os.path.exists do not return False). The test expects a FileNotFoundError, but the code will raise ValueError before reaching any FileNotFoundError, so the assertion fails.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_case_10": "```plaintext\n[Explanation]\nThe test creates a short script that immediately exits with code 1 and calls task_func on it. The function checks the path, launches bash, and since the process exits immediately the monitoring loop is skipped and the function returns the initialized dict {'CPU Usage': 0.0, 'Memory Usage': 0}. No exceptions are raised during this flow, so isinstance(resources, dict) will be True. (This assumes the psutil import is available in the environment.)\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/560", "raw_responses": {"test_basic_functionality": "```plaintext\n[Explanation]\nThe function correctly parses the input, extracts year \"2022\", months [1,2,3], and values [100,200,150]. It sets the x-label, y-label, and title as required and creates three bars (one per entry), so ax.get_xlabel(), ax.get_ylabel(), ax.get_title() match the expected strings and len(ax.patches) == 3. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_full_year_data": "```plaintext\n[Explanation]\nThe function plots 12 bars from the provided 12 monthly entries. However, matplotlib's Axes.patches includes the Axes background patch in addition to the bar rectangles, so len(ax.patches) will be 13 (1 background + 12 bars). The test expects 12, so the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_partial_year_data": "```plaintext\n[Explanation]\nThe function parses three entries, converts months and values to integers, and calls ax.bar(...) once with three x positions, producing three Rectangle patches. Setting x-ticks doesn't add patches. No errors are raised by the parsing or plotting steps, so len(ax.patches) will be 3.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_incorrect_data_format": "```plaintext\n[Explanation]\nThe function only reads the year from the first entry and never checks that all entries have the same year. Given the input includes entries from 2022 and 2023, the function will parse them without raising an exception and return an Axes object. The test expects a ValueError for multiple years, so the code will not meet that expectation.\n\n[Result]\n```plaintext\nFAIL\n```", "test_empty_data": "```plaintext\n[Explanation]\nFor data == '' the code does entries = ['']; in the loop entry == '' so parts = [''] and accessing parts[1] raises IndexError before any ValueError is raised. The test expects a ValueError, so the raised IndexError will cause the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/657", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function tokenizes and lowercases the provided texts, removes the supplied stopwords (the test passes stopwords_mock so nltk stopwords are not needed), and trains gensim Word2Vec with min_count=1. \"Python\" becomes \"python\" and appears in the vocabulary. The model returned is a Word2Vec instance. The incidental nltk.download call in an except block does not affect behavior when stopwords are provided. Therefore the assertions should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function cleans the input, uses the provided stopwords list (so nltk stopwords aren't required), and trains gensim Word2Vec with min_count=1. For the test texts the processed sentences become [['hello'], ['machine','learning'], ['python']], so 'python' is included in the vocabulary and the returned object is a Word2Vec instance. No errors should occur in this test environment.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe test passes an empty texts list. task_func builds processed_texts as an empty list and then calls Word2Vec(processed_texts, ...). Gensim's Word2Vec raises an error when given an empty corpus (empty vocabulary), so the function will raise an exception instead of returning a Word2Vec instance. Therefore the assertion expecting a Word2Vec instance will not be reached.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function cleans the two input sentences, removes the provided stopwords (so 'long' remains), tokenizes into lists of words, and trains gensim.models.Word2Vec with min_count=1, so the token 'long' will be in model.wv.key_to_index. The returned object is a Word2Vec instance. Therefore the test's assertions should succeed (assuming required libraries/data are available).\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function lowercases and tokenizes the three single-word texts into [['bonjour'], ['hola'], ['ciao']] using the provided stopwords list (which does not remove these words). It then trains gensim.models.Word2Vec with min_count=1 so all three tokens are in the vocabulary. model is a Word2Vec instance and 'bonjour' will be present in model.wv.key_to_index. No errors arise for this test input.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/971", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function lists files in the given directory and appends one tuple per file. The test creates exactly five files in a TemporaryDirectory and then asserts the returned list length equals the number of files. The implementation uses os.listdir and os.path.isfile, so all created files will be counted. No other checks in the test depend on ordering or timestamp formatting. Therefore the code should return 5 items and the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe test calls task_func with a non-existent path '/path/to/non/existing/directory' and expects a ValueError. The function checks os.path.exists(directory_path) and raises ValueError if it does not exist, so the expected exception will be raised. The setUp files are irrelevant to this test method.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function returns an empty list when the directory exists but has no entries. The test creates a temporary empty directory and asserts len(result) == 0, which matches the implemented behavior.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function lists files in the provided directory, checks isfile, uses os.stat().st_size for sizes, and returns tuples including file name and size. The test creates files with specific sizes, calls task_func, and only verifies sizes by mapping file names to sizes from the result. Nothing in the function filters out regular files or alters sizes, so all created files should be present with correct st_size. Edge conditions (ordering, timestamps) do not affect this test.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function lists all entries in the provided directory and appends tuples only for items where os.path.isfile is True. The test creates five regular files in a temporary directory and only asserts that the returned file names (sorted) match the expected names (sorted). The implementation will include all those files, so the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nThe function uses stat().st_ctime as \"creation time\". On Unix-like systems st_ctime is the last metadata-change time (not true creation time) and can be greater than st_mtime, so creation_time <= modification_time is not guaranteed. The test asserts creation_datetime <= modification_datetime for each file; this can fail on typical Linux environments. Other parts (ISO formatting and parsing) are fine.\n\n[Result]\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nThe function lists directory entries and appends only those where os.path.isfile(item_path) is True. The test creates five files in the top-level temp directory and one file inside a subdirectory. The subdirectory entry is skipped by the isfile check, so result will contain exactly the five top-level files. No other checks in the test would fail (times and sizes aren't asserted), so the code will pass this test.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe function lists files in the provided directory and appends each filename to the result tuples. The test creates several regular files in a TemporaryDirectory and only asserts that those filenames appear in the returned list of names. The code uses os.listdir and os.path.isfile to include the created files; no behavior in the function prevents those files from being detected. Time formatting and sizes are irrelevant to the test. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe test passes a path to a regular temporary file and asserts task_func raises ValueError. The function checks existence (True) then checks if the path is a directory; for a file os.path.isdir(...) is False, so it raises ValueError. Therefore the test's assertRaises condition is satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_10": "```plaintext\n[Explanation]\nThe function returns UTC-aware ISO-formatted timestamps using datetime.fromtimestamp(..., tz=timezone.utc) and .isoformat(), producing strings like \"YYYY-MM-DDTHH:MM:SS+00:00\". datetime.fromisoformat can parse those and yields tzinfo matching timezone.utc. The test sets before_creation just before file writes and after_creation after calling task_func, so both creation and modification times should fall within that window. There are no other mismatches (file names/sizes not asserted, order irrelevant), so the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/1042", "raw_responses": {"test_successful_email_send": "```plaintext\n[Explanation]\nThe test patches getpass.getpass and smtplib.SMTP but does not patch builtins.input. The code calls input() twice to get sender and recipient emails, so during the test those input() calls will block or raise EOFError (no mocked input provided). The test's mock_getpass.side_effect seems intended to supply sender, recipient, and password, but getpass.getpass is only called once in the code, so it cannot substitute for the two input() calls. Therefore the function will not reach the patched smtplib.SMTP call and the asserted call will not occur. The test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_email_with_empty_message": "```plaintext\n[Explanation]\nThe function checks if message_data is falsy and returns early only if client_socket.recv(...) returns an empty bytes object. In the test, client_socket.recv was set up to return a MagicMock whose decode() returns '', so message_data is truthy and the function continues to call input() (for sender and recipient) and getpass.getpass. The test did not patch builtins.input, so calling input() will block or raise EOFError during the test, causing it to fail. Therefore the code will not pass the test.\n\n[Result]\n```plaintext\nFAIL\n```", "test_smtp_server_connection_error": "```plaintext\n[Explanation]\nThe test patches smtplib.SMTP to raise SMTPConnectError, but task_func wraps the SMTP usage in a try/except that catches Exception and prints an error instead of re-raising, so the SMTPConnectError will not propagate to the caller. Additionally, the test did not patch builtins.input while the function calls input() twice (which would cause EOFError in a non-interactive test), but the primary reason the test assertion expecting SMTPConnectError fails is that the code swallows the exception.\n\n[Result]\nFAIL\n```", "test_socket_closes_after_operation": "```plaintext\n[Explanation]\nThe test expects client_socket.close() to be called once. The provided task_func calls client_socket.recv(...) but never calls client_socket.close(), so the assertion will fail. (Additionally, the function uses input() which the test does not mock, but the primary reason for failure is the missing socket.close() call.)\n\n[Result]\n```plaintext\nFAIL\n```", "test_successful_email_dispatch": "```plaintext\n[Explanation]\nThe test expects the function to send a success message back to the client socket and close it (asserting client_socket.send(...) and client_socket.close()). The provided code only prints the result to stdout and never calls client_socket.send or client_socket.close. Additionally, the test's mocking of getpass/input does not match the code's use of input() (inputs are not mocked), but the primary failing issue is the missing send/close behavior. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n"}}
{"id": "BigCodeBench/326", "raw_responses": {"test_no_bat_files": "```plaintext\n[Explanation]\nThe test patches glob.glob to return an empty list. task_func calls glob.glob, gets [], iterates none, and returns []. The subprocess patch is unused (code calls subprocess.run, not Popen), but that doesn't affect this test. No errors occur, so the returned value matches the expected [].\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_single_bat_file_success": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen, but the code calls subprocess.run (not Popen), so the mock is never used. As a result the actual subprocess.run will be executed (or an exception caught), producing an exit code different from the mocked 0. Therefore the function will not return the expected [('file1.bat', 0)].\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_single_bat_file_failure": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen and glob.glob. glob.glob is used so it returns ['file1.bat']. The code calls subprocess.run (not subprocess.Popen directly). subprocess.run internally uses Popen, but because Popen is mocked it returns a MagicMock. subprocess.run uses \"with Popen(...) as p\", so the object used (mock_process.__enter__.return_value) is not the mock_process whose wait.return_value was set to 1. As a result, subprocess.run will produce a CompletedProcess whose returncode is a MagicMock (or otherwise not the integer 1), so the function returns a value that does not equal [('file1.bat', 1)].\n\n[Result]\nFAIL\n```", "test_multiple_bat_files_mixed_results": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen to control process behavior, but the implementation calls subprocess.run (not Popen). Because the test's mocks won't apply, the code will not produce the mocked return codes and the output will differ from the expected list. Therefore the test will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_exception_handling": "```plaintext\n[Explanation]\nThe test patches subprocess.Popen to raise an exception, expecting task_func to call Popen and handle the exception. However, the code calls subprocess.run (not Popen), so the mocked exception is never triggered. subprocess.run will execute (or return a numeric exit code), resulting in a tuple like ('file1.bat', <int>) rather than ('file1.bat', None). Therefore the test's expected result does not match the actual behavior.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/199", "raw_responses": {"test_valid_input": "```plaintext\n[Explanation]\nThe function validates inputs, converts the given UTC time to each city's local time (pytz gives the expected DST abbreviations for the specified date), and builds a DataFrame with columns in the correct order. The UTC datetime in the test uses tzinfo=pytz.UTC so the tz check passes. With seed=42 the random calls produce the sequence of weather indices that yield ['Sunny','Sunny','Rainy','Cloudy','Cloudy'], matching the expected list. The DataFrame length and columns match the assertions.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_datetime_type": "```plaintext\n[Explanation]\nThe test calls task_func with a string for utc_datetime. The function immediately checks isinstance(utc_datetime, datetime) and raises ValueError when it's not a datetime object. That matches the test's expected exception.\n\n[Result]\nPASS\n```", "test_missing_timezone_for_custom_city": "```plaintext\n[Explanation]\nThe test passes a custom cities list including 'Paris' but supplies a timezones dict missing 'Paris'. The function checks for each city and raises ValueError if a city's timezone is not defined. All prior validations (utc datetime with pytz.UTC, types) pass, so the function will raise ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_custom_cities_and_timezones": "```plaintext\n[Explanation]\nThe function validates inputs, converts the provided UTC datetime (which in the test is datetime(..., tzinfo=pytz.UTC)) to each city's timezone, and returns a DataFrame containing a 'City' column with the requested cities. The test only checks that the set of cities in the report matches the provided custom_cities ['New York', 'London']. Given the provided timezones mapping includes both cities and no validation fails, the DataFrame will contain those cities. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_reproducibility_with_seed": "```plaintext\n[Explanation]\nThe function seeds Python's global random generator (set_seed(seed)) and then uses randint from the same module, so calling task_func twice with the same seed produces the same sequence of random choices. All other operations (timezone conversion, string formatting, DataFrame construction) are deterministic given the same inputs. The utc_datetime provided has tzinfo=pytz.UTC so the function's validation passes. Therefore the two DataFrames will be identical and assert_frame_equal will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/678", "raw_responses": {"test_basic_operation": "```plaintext\n[Explanation]\nThe function reads all .json files in the given directory, converts them to DataFrames, adds a 'source' column, moves each file into a \"processed\" subfolder, and concatenates the DataFrames. The test creates two JSON files each with 2 records, so the returned DataFrame should have 4 rows. Potential concerns (ordering, case of 'source') do not affect the length check. The test helper uses os/json/shutil without explicit imports, but those modules are already imported at the top of the provided code, so they are available in the same execution environment. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_directory": "```plaintext\n[Explanation]\nThe function returns an empty pandas DataFrame when no JSON files are present. The test creates an empty directory and calls task_func, so json_files is empty, dataframes remains empty, and result_df = pd.DataFrame() which has .empty == True. The top-level imports (os, json, shutil, pandas) in the provided code make those names available for the test operations. Therefore the assertion will succeed.\n\n[Result]\nPASS\n```", "test_non_json_files": "```plaintext\n[Explanation]\nThe function lists only files ending with '.json' and checks os.path.isfile, so 'b.txt' is ignored. It reads 'a.json' into a DataFrame (one row), moves the file to processed/, and returns a DataFrame of length 1. No errors occur, so the test's assertion len(df) == 1 will hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_single_file": "```plaintext\n[Explanation]\nThe function reads .json files in the given directory, converts the JSON list to a pandas DataFrame, appends a 'source' column, moves the file to a processed subdirectory, and returns the concatenated DataFrame. The test creates one JSON file containing a single object, so task_func will produce a DataFrame with one row. All used modules are imported at top-level, and file operations succeed, so the assertion len(df) == 1 will hold.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_with_empty_json_file": "```plaintext\n[Explanation]\nThe function reads 'a.json' which contains an empty list. pd.DataFrame([]) produces an empty DataFrame; adding the 'source' column to it preserves zero rows. Concatenating yields an empty DataFrame, so df.empty is True and no errors are raised. The test's cleanup also succeeds.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/879", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function runs correctly (input checks, contingency table, chi-square with SciPy). The test draws two independent random binary columns with np.random.seed(12); under the null the chi-square p-value is uniform(0,1), so only a 20% chance to fall in [0.4,0.6]. For this fixed seed the p-value is not within 0.5 \u00b1 0.1, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe input DataFrame has Var1 counts A:25 X/25 Y and B:25 X/25 Y, so the contingency table cells are all 25. All checks (non-empty, columns present, object/categorical dtype, >=2 categories, expected frequencies >=5) pass. chi2_contingency yields chi2=0 and p-value=1. The test asserts p\u22481 within 0.1, so the function will pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function validates inputs, builds a 4x4 contingency table from two random categorical columns of size 200, and computes the chi-square p-value. With 200 draws from 4 categories each, it's virtually certain (probability ~1) that every category appears and that expected frequencies exceed 5, so no ValueError is raised. chi2_contingency returns a p-value in [0,1], satisfying the test assertion.\n\n[Result]\n```plaintext\nPASS\n```", "test_edge_case_empty_dataframe": "```plaintext\n[Explanation]\nThe function immediately checks if data.empty and raises ValueError(\"Data cannot be empty\"). A DataFrame created with only column names and no rows has data.empty == True, so the function will raise ValueError as the test expects.\n\n[Result]\nPASS\n```", "test_edge_case_non_categorical": "```plaintext\n[Explanation]\nThe function checks that each column is either categorical or object dtype. For numeric float columns (from np.random.rand), both pd.api.types.is_categorical_dtype and is_object_dtype return False, so the function raises a TypeError as the test expects.\n\n[Result]\nPASS\n```", "test_edge_case_single_category": "```plaintext\n[Explanation]\nThe input has a single category in each column. The function checks for multiple categories via:\nif len(contingency_table.index) < 2 or len(contingency_table.columns) < 2:\nand raises ValueError(\"Each column must have multiple categories\") before any other processing. Thus task_func will raise ValueError for the provided test, matching the test's expectation.\n\n[Result]\nPASS\n```", "test_edge_case_large_categories_small_counts": "```plaintext\n[Explanation]\nThe test constructs two categorical columns with 20 random draws from 10 categories, producing a contingency table with many low counts. The function computes expected frequencies via chi2_contingency and raises ValueError if any expected < 5. Given the small sample relative to number of categories, the expected cell counts will almost certainly be below 5, so the function will raise ValueError as the test asserts.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_col_not_in_df": "```plaintext\n[Explanation]\nThe function first checks whether the given column names exist in data.columns and raises ValueError(\"Columns must be in the data\") if either is missing. The test calls task_func(data, 'a', 'Var2') where 'a' is not in the DataFrame columns, so a ValueError is raised as expected by the test.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/985", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function correctly parses the provided JSON, validates entries, creates a DataFrame, writes the CSV (index=False) to the given temporary directory, and returns the file path and DataFrame. For the test input the DataFrame and CSV will match (same columns, row order, and dtypes), so all assertions including pd.testing.assert_frame_equal will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\njson.loads('{}') returns an empty dict. The function checks \"if not data: raise ValueError('JSON data is empty')\", so calling task_func('{}', output_dir) raises ValueError as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nAll three test calls raise ValueError as expected:\n- First and second inputs lack the required \"Countries\" key, triggering the explicit ValueError.\n- Third input is malformed JSON; json.loads raises JSONDecodeError which the function catches and re-raises as ValueError.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function parses the provided JSON, validates it, and calls os.makedirs(output_dir, exist_ok=True) before writing the CSV. Given the test uses a non-existing subdirectory and valid JSON, the directory will be created and no exceptions raised.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe JSON in the test is a valid Python string (the apostrophe is escaped for the Python literal, not as a JSON escape), and json.loads will parse the country names with non-ASCII characters correctly. Populations are integers, validations pass, the DataFrame is created, the CSV is written to the temporary directory, and the returned DataFrame contains the country names checked by the test.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_6": "```plaintext\n[Explanation]\nThe input JSON has an empty \"Countries\" dict. The function builds validated_data as an empty list and then pd.DataFrame(validated_data) produces an empty DataFrame with no \"Country\" or \"Population\" columns. The test asserts those columns exist, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_7": "```plaintext\n[Explanation]\nEach test call expects a ValueError. The function parses JSON and immediately raises ValueError when any population is not an int/float (strings or null become non-numeric), or when negative. All three test JSONs include a string population (\"1000000\") or null, causing the function to raise ValueError on each call (the third also contains -1 but a prior non-numeric triggers the same exception). Therefore the test's assertions will be satisfied.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nThe input JSON is valid and contains the \"Countries\" key with a string country name and a non-negative integer population (0). The function parses and validates the data, builds a DataFrame with \"Country\" and \"Population\" columns, writes the CSV to the provided temporary directory, and returns the file path and DataFrame. All test assertions (DataFrame structure, file existence, presence of the country, and population value 0) will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function correctly loads the JSON, validates entries, floors float populations using math.floor, creates a DataFrame with 'Country' and 'Population' columns, writes the CSV to the temporary output directory, and returns the path and DataFrame. For the test JSON with float populations, both values are floored to 1234567 and 98765 respectively, the CSV is written, and DataFrame checks succeed.\n\n[Result]\n```plaintext\nPASS\n```"}}
{"id": "BigCodeBench/1053", "raw_responses": {"test_display_plot": "```plaintext\n[Explanation]\nThe test patches pandas.read_csv to return a DataFrame with a 'Text' column. task_func reads that DataFrame, combines the text, vectorizes it, builds a bar plot, and returns the Axes object because no save_path is given. The returned Axes is not None, satisfying the test's assertion. The patch target and imports align (pandas is already imported in the provided code), so no NameError occurs. Therefore the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_save_plot": "```plaintext\n[Explanation]\nThe patched pandas.read_csv returns a DataFrame with text, the function concatenates text, vectorizes and creates a plot, and since save_path is provided it saves 'test_output.png', closes the plot, and returns None. The test checks for None and that the file exists \u2014 both conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_empty_file": "```plaintext\n[Explanation]\nThe provided test code references pd.DataFrame but never imports pandas as pd, so running the test will raise a NameError in the test before task_func is meaningfully exercised. Although task_func would return None for an empty DataFrame (CountVectorizer raises an error that's caught, leading to None), the malformed test itself causes the test run to error, so the code will not pass the given test.\n\n[Result]\nFAIL\n```", "test_invalid_file_path": "```plaintext\n[Explanation]\npandas.read_csv is patched to raise FileNotFoundError. task_func catches FileNotFoundError and re-raises a FileNotFoundError, so the call will raise FileNotFoundError as the test expects.\n\n[Result]\nPASS\n```", "test_large_data_set": "```plaintext\n[Explanation]\nThe test patches pandas.read_csv to return a DataFrame with 1000 unique words in the 'Text' column. task_func reads that DataFrame, combines the text, vectorizes it, builds a bar plot and returns the Axes object when save_path is not provided. No errors should occur (CountVectorizer handles these tokens, stop_words don't remove them, and the function returns ax). Therefore the returned value is not None and the assertion passes.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/92", "raw_responses": {"test_cluster_centers": "```plaintext\n[Explanation]\nThe function creates a PathCollection for the centroids with label='Centroids' via ax.scatter(centroids[:, 0], centroids[:, 1], ..., label='Centroids'). The test looks for PathCollection children with that label and checks that get_offsets() has length 3. KMeans with n_clusters=3 produces 3 centroids, so the centroid PathCollection exists and has 3 offsets. The returned ax is correct and pandas is available from the module import. Therefore the test conditions are satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_single_cluster_error": "```plaintext\n[Explanation]\nThe function checks \"if not isinstance(n_clusters, int) or n_clusters <= 1: raise ValueError(...)\". Calling task_func(self.data, 1) triggers this check and raises ValueError before any clustering, so the test expecting a ValueError will pass.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_valid_input": "```plaintext\n[Explanation]\nThe function validates inputs, runs KMeans on the 100x2 DataFrame, and returns the cluster labels (length 100) and the Axes. There are no issues that would cause an exception for this test (data is a pandas DataFrame, n_clusters=3 is valid, sklearn/matplotlib usage is standard). The test only asserts len(labels) == 100, which will be True.\n\n[Result]\nPASS\n```", "test_invalid_data_type": "```plaintext\n[Explanation]\nThe function explicitly checks \"if not isinstance(data, pd.DataFrame): raise ValueError(...)\" before any other operations. Calling task_func([[1, 2], [3, 4]], 3) passes a list, so it triggers the ValueError as the test expects. No other errors occur earlier or interfere.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_invalid_cluster_number": "```plaintext\n[Explanation]\nThe function checks n_clusters with \"if not isinstance(n_clusters, int) or n_clusters <= 1: raise ValueError\". Passing -1 (an int <= 1) triggers that ValueError. The DataFrame setup uses the pandas import from the module, so no NameError occurs. Therefore the test expecting a ValueError will succeed.\n\n[Result]\nPASS\n```", "test_return_type": "```plaintext\n[Explanation]\nThe provided module imports pandas as pd and matplotlib.pyplot as plt at top-level, so the test's use of pd and plt (in the same execution context) is valid. task_func returns a matplotlib Axes object (ax) after performing KMeans clustering and plotting; isinstance(ax, plt.Axes) will be True. No other errors are expected for the given inputs.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_return_labels": "```plaintext\n[Explanation]\nThe function performs KMeans on the provided pandas DataFrame and returns labels. The test seeds the RNG, creates a 100x2 DataFrame, calls task_func(self.data, 3), and asserts there are 3 unique labels. The code validates inputs, runs sklearn.cluster.KMeans with n_clusters=3, and returns the predicted labels. KMeans on random continuous data will produce 3 clusters (unique labels 0,1,2). The pandas import at top-level of the submitted code makes pd available when the code is executed before the tests, so the test's use of pd.DataFrame will work in the usual harness. No errors or edge cases are expected for this test.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
{"id": "BigCodeBench/208", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function sets the NumPy seed to the given value (seed=0) and generates the same sequence as the test (the test also set the seed to 0). It computes the descriptive statistics using NumPy percentiles and std (population), returning numeric floats. The test uses assertAlmostEqual with 5 decimal places, which matches the returned float precision. Creating the Matplotlib figure is unlikely to fail in the test environment. Therefore the computed statistics will match the expected values and the test should pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function generates 'elements' steps, computes a cumulative sum of length 50, and plots that array. Calling task_func(50, seed=42) produces a Line2D whose y-data is the random_walk array of length 50, so len(y_data) == 50. No other errors are expected for this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function raises ValueError for elements <= 0, so task_func(0) triggers the expected exception. For task_func(1) it generates one step, computes the cumulative sum (a single-value array), plots it with matplotlib, and returns the Axes. ax.lines[0].get_ydata() will have length 1. No other errors are produced by the code for this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function validates input, generates a random walk, returns a dict with keys including 'mean' and 'std', and returns the Axes object from plt.subplots(). For elements=10 it will produce a dict and an instance of matplotlib.axes.Axes, so all assertions in the test_case_4 should pass.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function generates a random walk of length 'elements' and plots it. For elements=100, np.cumsum produces an array of length 100, ax.plot(random_walk) creates a line with 100 y-values, and the title is set to 'Random Walk'. There are no input validation issues for this test, so both assertions will hold.\n\n[Result]\n```plaintext\nPASS\n```\n```"}}
{"id": "BigCodeBench/341", "raw_responses": {"test_numeric_data": "```plaintext\n[Explanation]\nThe function will create a matplotlib Figure with two axes for the numeric column. For the numeric test:\n- pd.api.types.is_numeric_dtype returns True for the int64 Series.\n- sns.histplot(data, kde=True, ax=ax1) will create bar patches on ax1, so ax1.patch(es) length > 0.\n- sns.boxplot(y=data, ax=ax2) will add Line2D objects for whiskers/median to ax2.lines, so len(ax2.lines) > 0.\n- The function returns a matplotlib.figure.Figure and plt was imported at module level in the provided code, so plt.close() in the test refers to that same name.\n\nAll asserted conditions in the test are satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_categorical_data": "```plaintext\n[Explanation]\nThe test calls task_func with a categorical column and then asserts:\n- fig.axes[0].patches has items (bars) \u2014 the count plot should satisfy this.\n- fig.axes[1].collections has items \u2014 the test expects a scatter/strip-like plot (which uses collections).\n\nThe implementation creates a horizontal bar chart for the second subplot (ax2.barh), which adds patch objects, not collections. Therefore fig.axes[1].collections will be empty and the assertion will fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_mixed_data": "```plaintext\n[Explanation]\nThe function identifies the 'numeric' column as numeric, creates two subplots, draws a histogram with seaborn (which produces bar patches) and a boxplot (which produces Line2D objects). It returns a matplotlib Figure with two axes. Therefore the assertions that fig is a Figure, len(fig.axes)==2, fig.axes[0].patches > 0, and fig.axes[1].lines > 0 should all be satisfied.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_column": "```plaintext\n[Explanation]\nThe function correctly raises a ValueError when the column is missing. However the test calls plt.close() after the assertRaises block but never imports plt in the test file (only imported matplotlib), so plt is undefined and a NameError occurs during the test, causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```\n```", "test_empty_dataframe": "```plaintext\n[Explanation]\nThe test constructs an empty DataFrame (df.empty is True). task_func checks df.empty and raises ValueError(\"Input df cannot be empty\"), which matches the test's assertRaises. The subsequent plt.close() refers to the module-level plt imported by the provided code, so it is defined. Therefore the test will pass.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/477", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function returns a DataFrame with shape (100,3), columns in order ['x','y','category'], x and y in [0,1), and ax from plt.subplots, so those tests pass. The only potential issue is ensuring all five categories appear; with seed=42 and 100 draws from 5 categories the sample produced by np.random.choice does include all {'A','B','C','D','E'} (the probability of missing any is negligibly small), so the set equality check will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function produces a DataFrame of shape (50, 3) with columns in the order ['x','y','category'], x and y drawn from np.random.rand in [0,1), and returns a matplotlib Axes. With seed=42 and CATEGORIES=['X','Y'] the deterministic np.random.choice call (size=50) yields both 'X' and 'Y' in the sample, so set(df['category']) == {'X','Y'} holds. All other assertions match the implementation.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nFor each tested N (5, 10, 50, 200) the function generates x and y arrays of length N and a categories array of length N, then constructs a DataFrame with three columns ('x','y','category'), so df.shape == (N, 3). The plotting code does not affect the DataFrame creation and should not raise errors in the given test. Therefore the test asserting the DataFrame shape will pass.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function samples categories with replacement when N (default 100) >= len(CATEGORIES). Although that does not guarantee all categories in general, with the fixed seed (42) and N=100 the sampled arrays will include every category for the provided lists (lengths 2, 3, and 1). Thus each df['category'] contains exactly the set C in each loop iteration, so the assertions pass.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function resets NumPy's global seed at the start of each call and then generates x, y, and category arrays with NumPy functions, so repeated calls with the same seed produce identical DataFrames and a different seed produces different values. The test compares DataFrames only (not the plot), so deterministic behavior ensures df1 == df2 and df1 != df3. No other side effects prevent the assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_6": "```plaintext\n[Explanation]\nFor N=0 and CATEGORIES=[], np.random.rand(0) yields empty arrays for x and y. The condition N >= len(CATEGORIES) (0 >= 0) holds, so np.random.choice(CATEGORIES, size=0, replace=True) returns an empty array (allowed), producing an empty 'category' column. Creating the DataFrame results in shape (0, 3) and list(df['category']) == []. The subsequent plotting calls (scatter on empty data and colorbar) do not raise errors for empty inputs. Thus the assertions in the test will be satisfied.\n\n[Result]\nPASS\n```", "test_case_7": "```plaintext\n[Explanation]\nFor N=3 and 4 categories, the function takes the else branch (N < len(CATEGORIES)) and uses np.random.choice with replace=False, producing 3 distinct categories. It returns a DataFrame of length 3 with 3 unique category values. The plotting code does not affect these assertions.\n\n[Result]\nPASS\n```", "test_case_8": "```plaintext\n[Explanation]\nFor N=50 and CATEGORIES=['X'], the code goes into the branch for N >= len(CATEGORIES) and uses np.random.choice with replace=True to generate 50 category entries; with only 'X' available, all entries will be 'X'. The DataFrame's 'category' column will therefore be all 'X', so the assertion (df['category'] == 'X').all() will be True. No other errors expected.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_9": "```plaintext\n[Explanation]\nThe function samples 50 values from the 3 given categories with replacement using a fixed seed (42). While sampling with replacement doesn't mathematically guarantee every category appears, with N=50 and 3 categories the probability a category is missing is negligibly small, and the fixed seed yields a deterministic draw that includes all three values {1,2,3}. The DataFrame's 'category' column will therefore contain all three categories and the assertSetEqual check passes.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/720", "raw_responses": {"test_return_value": "```plaintext\n[Explanation]\nThe function returns the path 'task_func_data/Output.txt' (constructed with os.path.join) and appends 10 rows to the existing file created in setUp. The test only asserts the returned string equals 'task_func_data/Output.txt', which will match on typical POSIX test environments where os.path.join yields that string. No exceptions are raised, so the test should pass.\n\n[Result]\nPASS\n```", "test_file_existence": "```plaintext\n[Explanation]\nThe test creates the directory and a non-empty Output.txt with a header. task_func appends 10 data rows and does not delete the file. It opens the file in append mode, checks file size (non-zero so no duplicate header), writes rows, and returns. The file remains on disk, so os.path.isfile(self.file_path) will be True.\n\n[Result]\nPASS\n```", "test_file_content": "```plaintext\n[Explanation]\nThe test expects the appended humidity value to be between 50 and 60, but the code generates humidity via random.uniform(40.0, 80.0). This does not guarantee the 50-60 range (only ~25% chance). Because the code does not ensure humidity \u2208 [50,60], the test can and typically will fail.\n\n[Result]\n```plaintext\nFAIL\n```\n", "test_data_appending": "```plaintext\n[Explanation]\nThe test sets up a file with only a header, then calls task_func() twice and expects the second call to add only one line. The provided task_func() appends 10 data rows on each call (writer.writerows(data) where data has 10 rows). After the first call the file will have 1(header)+10 rows; after the second call it will have +10 more rows, so the final count will be initial + 10, not initial + 1. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```", "test_headers_only_once": "```plaintext\n[Explanation]\nThe test's setUp creates the file with the header already present. task_func opens the same file in append mode and only writes the header if the file size is zero (os.stat(...).st_size == 0). Since the file is non-empty from setUp, task_func will not re-write the header on either call, so exactly one header row remains. No other errors affect this test.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/897", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function seeds Python's random module when provided, generates exactly 'rolls' choices from 1..6, counts frequencies into a length-6 numpy array whose sum equals rolls, and returns an Axes from plt.subplots() with the specified title and axis labels. All assertions in the test (length 6, sum 100, ax instance, title, xlabel, ylabel) are satisfied.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nCalling task_func(0, seed=2) produces an empty results list so frequencies are all zeros (np.array of six zeros). The function sets the title, xlabel, and ylabel on the Axes before returning. matplotlib.hist accepts empty data and still returns an Axes, so all assertions in the test will be satisfied.\n\n[Result]\n```plaintext\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function uses Python's random module with random.seed(seed) and random.choice over [1..6], so the 100000 deterministic rolls with seed=3 will produce the expected frequency array. The histogram is created and its title and axis labels are set exactly as asserted. There are no errors in counting or binning that would change those outputs.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_4": "```plaintext\n[Explanation]\nWith seed=4 and one roll, Python's random.choice from [1..6] deterministically selects 2, so the frequency array becomes [0,1,0,0,0,0]. The code sets the histogram title and axis labels exactly as the test expects. Therefore the test assertions match the code's behavior.\n\n[Result]\nPASS\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function seeds Python's random module when seed is provided, generates exactly `rolls` integer choices from 1..6, counts frequencies into a numpy array (whose sum equals `rolls`), and sets the axis title and labels to the exact strings checked by the test. Creating the histogram with plt.subplots() and not showing it does not affect the returned Axes. Therefore the assertions on sum(outcomes) and the axis title/x/y labels will succeed.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/988", "raw_responses": {"test_file_is_file": "```plaintext\n[Explanation]\nThe test creates four files in a temporary directory and calls task_func with the 'is_file' predicate. The function lists directory entries and uses os.path.isfile on each item path; for the created files this returns True. The result includes each filename with a single 'is_file' key set to True, satisfying the test assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_file_is_not_dir": "```plaintext\n[Explanation]\nThe function checks the given temporary directory, lists the created files, and evaluates the single predicate 'is_dir'. All created entries are files (not directories), so os.path.isdir on each item_path returns False. The result contains each filename as a key with a single 'is_dir': False entry, satisfying the test assertions. No errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_dir_is_dir": "```plaintext\n[Explanation]\nThe function lists the temporary directory, evaluates the 'is_dir' predicate using os.path.isdir for each item, and returns a dict keyed by item names. The test creates two directories ('somedir', 'aDirectory123') and calls task_func with ['is_dir']; os.path.isdir will correctly report True for both, and the helper assertions about predicate presence and result length also hold. No errors are raised.\n\n[Result]\n```plaintext\nPASS\n```", "test_dir_is_not_file": "```plaintext\n[Explanation]\nThe test creates two directories in a temporary directory and calls task_func with the single predicate 'is_file'. The function correctly lists directory entries, checks os.path.isfile for each item, and returns a mapping keyed by item name. For directories, is_file is False, and the result dict entries contain exactly the requested predicate. Therefore the assertions in the test will succeed.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_has_special_char": "```plaintext\n[Explanation]\nThe function inspects filenames (without extensions for special-char checks) and will find '!' '@' and the '.' in 'f.ile' as special characters. The temporary files are created successfully and os.listdir returns their names, so result[fn][field] will be True for each. No errors or mismatches occur for this test.\n\n[Result]\nPASS\n```", "test_has_no_special_char": "```plaintext\n[Explanation]\nThe function checks filenames without extensions for special characters using the regex r'[^a-zA-Z0-9_]', and for the provided test names ('file_', '_file', 'file.txt', 'some_file.txt') that correctly yields False. The directory and files are created correctly by the test and all other logic (listing, result keys) matches the assertions.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_has_numbers": "```plaintext\n[Explanation]\nThe test creates four files ('123', '123.txt', 'text123', 't1e2x3t4') and calls task_func with predicate 'has_numbers'. The implementation uses re.search(r'\\d', item) on the filename (including extension), so all four filenames correctly return True. The function returns an entry per file with the predicate present, matching the test's assertions. No filesystem or permission issues occur in this temporary directory.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_multiple_predicates": "```plaintext\n[Explanation]\nThe test creates a single file \"test1!.txt\" in a temporary directory and calls task_func with all four valid predicates. The function lists the directory, checks is_file/is_dir correctly using os.path functions, detects special characters by inspecting the name without extension (detects '!'), and detects digits by searching the item name (finds '1' in \"test1!.txt\"). All predicates are present and the assertions match. No errors or edge cases affect this test.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_deduplicate_predicates": "```plaintext\n[Explanation]\nThe test creates one file named 'test_file' and calls task_func with predicates ['is_file','is_file']. The function deduplicates predicates (via set) to a single 'is_file', validates it, lists the directory, and returns a single entry for 'test_file' with an 'is_file' key. The checks for result length and presence of the predicate will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_empty_predicates": "```plaintext\n[Explanation]\nThe test calls task_func with an existing temporary directory and an empty predicates list. The function deduplicates and filters predicates, finds no valid predicates, and raises ValueError(\"No valid predicates provided.\"), which matches the test's expected exception.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_invalid_predicates": "```plaintext\n[Explanation]\nThe test calls task_func with a real temporary directory and predicates ['foo', 'bar'].\nThe function filters predicates against {'is_file','is_dir','has_special_chars','has_numbers'}; none match, so it raises ValueError(\"No valid predicates provided.\").\nThe test expects a ValueError via assertRaises, so the behavior matches.\n\n[Result]\nPASS\n```", "test_nonexistent_directory_error": "```plaintext\n[Explanation]\nThe function immediately checks os.path.exists(dir_path) and raises FileNotFoundError if the path does not exist. The test calls task_func('nonexistent_dir', ['is_file']) which will not exist in the test environment, so the function will raise FileNotFoundError as expected.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/93", "raw_responses": {"test_transformed_data_shape": "```plaintext\n[Explanation]\nPCA on a 10x2 DataFrame with n_components=2 yields a (10,2) transformed array. The function wraps that in a DataFrame and returns it as the first element; the test unpacks and checks transformed_data.shape == (10, 2). No input validation or other code paths interfere, so the test should pass.\n\n[Result]\nPASS\n```", "test_invalid_n_components": "```plaintext\n[Explanation]\nThe function checks \"if not isinstance(n_components, int) or n_components <= 0: raise ValueError\". Calling task_func(self.data, 0) meets n_components <= 0, so a ValueError is raised as the test expects.\n\n[Result]\nPASS\n```", "test_invalid_n_components_type": "```plaintext\n[Explanation]\nThe function checks \"if not isinstance(n_components, int) or n_components <= 0\" and immediately raises ValueError for non-int values. Passing 'two' (a string) satisfies that condition, so ValueError is raised as the test expects.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_plot_axes": "```plaintext\n[Explanation]\nThe function creates a new Axes with plt.subplots and calls ax.scatter exactly once when n_components=2, which adds a single PathCollection to ax.collections. Given the test DataFrame with two numeric columns and PCA(n_components=2) running without error, len(ax.collections) will be 1, so the assertion passes.\n\n[Result]\nPASS\n```", "test_values": "```plaintext\n[Explanation]\nThe test constructs the DataFrame in setUp using np.random.rand without setting a seed, so the data passed into task_func is nondeterministic. The test then calls np.random.seed(42) immediately before running task_func, but that does not affect the already-created DataFrame. Therefore the PCA output from the actual input will not match the hard-coded expected values (the test only allows a global sign flip). As a result, the comparison will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/346", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe test creates script1.py that exits with code 0. task_func checks the file exists, builds the command [sys.executable, script_path], runs subprocess.run with check=True and returns result.returncode. Running a script that exits 0 will not raise and result.returncode is 0, matching the test assertion. No other errors or edge cases affect this specific test.\n\n[Result]\nPASS\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function uses subprocess.run(..., check=True), which raises subprocess.CalledProcessError when the subprocess exits with a non-zero code. The test calls task_func on script2 which exits with code 1 and expects return_code == 1, but task_func will instead raise CalledProcessError, causing the test to fail.\n\n[Result]\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe task_func builds a command using sys.executable and the script path, checks the script exists, runs subprocess.run(...) with check=True and returns the returncode. The test creates script3 that prints its args and exits with 0, writes it to disk, and calls task_func with wait=True and two args. subprocess.run will complete successfully and return 0, matching the test assertion.\n\n[Result]\nPASS\n```", "test_case_4": "```plaintext\n[Explanation]\nThe test calls task_func(..., False) for an existing script. The function checks the file exists, builds the command, and in the wait=False branch starts subprocess.Popen(cmd) and returns None. The test only asserts the return value is None, so it will pass. (The module-level imports include os, so os is available for the test setup.)\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe test runs task_func on a script that raises an exception at top level. task_func uses subprocess.run(..., check=True), so the child process exiting with a non-zero status will cause subprocess.CalledProcessError to be raised, which task_func re-raises. The test expects this exception. The test code uses names like os and subprocess, which are imported at the top of the provided module, so no NameError occurs. Therefore the behavior matches the test's expectation.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/567", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function parses the string into integers, builds bins from 0.5 to 5.5 for input '1-2-3-4-5', and plots a histogram. The axis title and labels are set as required. Matplotlib's default tick locator on the x-range [0.5, 5.5] yields major ticks at 1\u20135 (as floats), which compare equal to the integers in the test. Therefore the assertions should succeed.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_2": "```plaintext\n[Explanation]\nThe function correctly parses the data, creates a histogram, and sets the title and axis labels, so those assertions pass. However, with data all equal to 5 the computed bins are [4.5, 5.5], so matplotlib will set x-axis ticks like [4.5, 5.0, 5.5] (not a single tick [5]). Therefore ax.get_xticks() will not equal [5], causing the test to fail.\n\n[Result]\n```plaintext\nFAIL\n```", "test_case_3": "```plaintext\n[Explanation]\nThe function correctly parses the single value and creates bins [6.5, 7.5], sets labels and title. However, matplotlib's automatic x-ticks for that x-range are not guaranteed to be exactly [7]; typically they will be something like [6.5, 7.0, 7.5] (or otherwise not a single tick at 7). The test asserts the tick list equals [7], so it will fail.\n\n[Result]\nFAIL\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function creates a histogram and sets title and axis labels as expected, so those assertions would pass. However, it does not set the x-axis tick locations; matplotlib will choose default tick positions (not the exact data values [1,2,4,8,10]). The test asserts the xticks equal those data values, so that assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_5": "```plaintext\n[Explanation]\nThe function creates a histogram but does not set the x-axis tick locations. Matplotlib's default tick locator will not necessarily produce ticks exactly [1, 50, 100, 150], so the test asserting those exact xticks will fail.\n\n[Result]\nFAIL\n```"}}
{"id": "BigCodeBench/906", "raw_responses": {"test_case_1": "```plaintext\n[Explanation]\nThe function checks filename.endswith('_processed'), but the test file is named 'file2_processed.txt' (the processed marker is before the extension), so the condition is false and no files are added to the archive. The test expects 'file2_processed.txt' to be present, so the assertion will fail.\n\n[Result]\nFAIL\n```", "test_case_2": "```plaintext\n[Explanation]\nThe function creates a zip at target_dir and only adds files whose names end with '_processed'. The test creates two files 'file1.txt' and 'file3.txt' (no '_processed' suffix), so no files are added and the archive is empty. The test asserts len(archive.namelist()) == 0, which will be true.\n\n[Result]\nPASS\n```", "test_case_3": "```plaintext\n[Explanation]\nThe source directory is empty in setUp, so the function creates an empty zip at the target path and writes no entries. archive.namelist() will be empty, so len(...) == 0 holds. No errors are raised.\n\n[Result]\n```plaintext\nPASS\n```\n```", "test_case_4": "```plaintext\n[Explanation]\nThe function always returns os.path.join(target_dir, archive_name), so the returned path will contain the custom archive name. The test only asserts that the custom archive name is a substring of the returned path; it does not check archive contents or matching filename pattern. Therefore the test will pass.\n\n[Result]\n```plaintext\nPASS\n```\n", "test_case_5": "```plaintext\n[Explanation]\nThe function creates the target directory, constructs archive_path using the given target_dir and default 'archive.zip', creates (empty) zip file, and returns archive_path. In the test both source and target are empty directories, so the loop adds nothing but the archive is still created and the returned path matches the expected path. No errors occur.\n\n[Result]\nPASS\n```"}}
{"id": "BigCodeBench/1057", "raw_responses": {"test_default_input": "```plaintext\n[Explanation]\nThe function uses default_animals (10 items) and default_foods (7 items) when called with no arguments, builds a DataFrame of size (10,7), and only shuffles rows/columns using numpy.random.permutation which does not change the shape. The test only asserts the DataFrame shape is (10,7), so the code meets the test expectation.\n\n[Result]\nPASS\n```", "test_custom_input": "```plaintext\n[Explanation]\nThe function builds a 3x4 DataFrame from the provided animals and foods, then shuffles rows and columns using numpy.random.permutation. Shuffling does not change the DataFrame shape. Although the test sets random.seed (Python's RNG), numpy's RNG is independent \u2014 that affects reproducibility but not shape. Therefore result.shape will remain (3, 4) and the assertion passes.\n\n[Result]\nPASS\n```", "test_empty_input": "```plaintext\n[Explanation]\nThe test passes empty lists for animals and foods, expecting an empty DataFrame shape (0, 0). However, the function treats empty lists (len == 0) as missing and replaces them with default lists, producing a DataFrame of shape (10, 7) instead. Therefore the assertion will fail.\n\n[Result]\nFAIL\n```", "test_single_input": "```plaintext\n[Explanation]\nWith animals=['Dog'] and foods=['Meat'] the function builds a 1x1 DataFrame containing \"Dog:Meat\". The subsequent numpy-based shuffles on rows and columns leave a single-element DataFrame unchanged. The test's use of random.seed(3) (Python's random) does not affect numpy.random, but that is irrelevant here since permutation of length 1 is deterministic. Both asserted conditions (shape (1,1) and presence of \"Dog:Meat\") are satisfied.\n\n[Result]\nPASS\n```", "test_partial_default": "```plaintext\n[Explanation]\n- animals is provided with length 3; foods is None so default_foods (length 7) is used.\n- The function builds a 3x7 DataFrame and only shuffles rows/columns, which doesn't change shape.\n- No errors or exceptions occur for this input, so the shape equals (3, 7) as asserted.\n\n[Result]\n```plaintext\nPASS\n```\n"}}
